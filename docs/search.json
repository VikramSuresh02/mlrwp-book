[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning in Non-Life Reserving",
    "section": "",
    "text": "Preface\nThe General Insurance Machine Learning in Reserving working party (MLRWP) is a group of over 70 volunteers, bringing together a range of data scientists, actuaries and academics from around the globe.\nWhen we started out in 2019, our premise was to find out why, whilst machine learning techniques are widespread in pricing, they are not being adopted ‘on the ground’ in reserving (certainly in the UK). Since then we have been working to help GI reserving actuaries develop data science skills, and are looking at ways that machine learning can be incorporated into reserving practice.\nWe have a website and regularly publish on our blog.\nIn this book, we have gathered together much of the material from our blog."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Our opening statement from August 2020\n\n\n\nIt’s probably fair to say that we are living in the era of big data and machine learning.\n\n\nThis was true in 2020 when Sarah MacDonnell (chair of the MLRWP) launched our website and blog in her first post and in the years since, advances in neural networks, and the recent explosion in large language models have put AI and ML firmly front and centre.\nIn the actuarial world machine learning (ML) has certainly made inroads into personal lines pricing - tight margins and high competitiveness create a large incentive to extract as much value and insight from data as possible.\nHowever, there has been less use of machine learning in reserving, possibly due to less obvious or immediate competitive advantages. Additionally practical challenges in implementing ML, from actually using the models, to finding the time in a busy reservint team to develop the necessary skills have limited use of ML.\nBut while reserving may not make extensive use of ML yet, there is recognition of its potential and a lot of interest. Ceratinly, we have observed high engagement with our material at the various conferences we have presented at.\nML techniques applied to richer, more detailed and broader datasets may allow us to gain insights we have not been able to conceive of before; be it as new segmentations, operational improvements, early warning systems, cost efficiencies, or revelations into claim life cycles, settlement patterns or consumer behaviour.\nThere may also be operational efficiencies to be gained; freeing time for targeted deep dives, allowing more frequent updates, or benefitting those with very large numbers of diverse classes.\nOver the last four years we have published material in a number of areas. We gather much of this material together in this book, organised by workstream:\n\nFoundations: to provide useful educational resources, including sharing of code\nData: to collate and promote sources of data that are available to help further research\nLiterature Review: to review and promote relevant papers (and help us bring together the best ideas that are out there)\nResearch: to undertake our own research projects\nPractical Considerations: to understand practical issues facing reserving actuaries implementing ML in their work\nSurvey: to understand what is currently being done on the ground, and identify any barriers"
  },
  {
    "objectID": "Foundations/foundations.html",
    "href": "Foundations/foundations.html",
    "title": "Foundations",
    "section": "",
    "text": "The Foundations Workstream aims to provide a path to gaining competency in common statistical and machine learning techniques by:\n\ncreating a roadmap of methods to learn\ngathering together relevant learning materials and,\ndeveloping notebooks in R and Python with example code, where the methods are applied to reserving data sets.\n\nOur workstream page contains a list of useful resources as well as links to our posts, which are reproduced here."
  },
  {
    "objectID": "Foundations/01_getting_started.html#programming-language-choice---r-or-python",
    "href": "Foundations/01_getting_started.html#programming-language-choice---r-or-python",
    "title": "2  Getting started",
    "section": "2.1 Programming language choice - R or Python?",
    "text": "2.1 Programming language choice - R or Python?\nFirst bit of advice is don’t get bogged down in the language wars! Both have advantages and disadvantages. If you’re getting started with ML, then you want to use the language that you will find easiest to quickly learn so that you can focus on the techniques rather than your code syntax and deciphering cryptic error messages.\nIf your workplace uses one language rather than the other then it will likely make sense to select that language. However, if you have no prior experience of either language and your reason for learning one of them is to gain a practical understanding of data science then the following guidance may be helpful.\n\nIf your academic background is more statistics than computer science then R may better suit your background than Python.\nIf your aim is to learn machine learning then either language is well suited and both have lots of free learning resources\nIf your aim is to learn leading edge deep neural network techniques, Python is the community’s preferred language and has many more learning resources than R\n\nManagement of dependencies (i.e. the particular versions of the language and packages used) in Python can be quite important. Particularly in cutting edge applications like neural networks, it may be necessary to use particular versions of packages to ensure that code runs correctly. But even simpler applications, like graphing can fall prey to this, where, for example, plotting code will run for one version of matplotlib but not for a more recent version. For this reason, newcomers to Python may need to get to grips with package management via environments sooner rather than later. Although the same issues can arise in R, anecdotally they appear less often, perhaps because the R core team emphasises backwards compatibility. Therefore, code examples in R are more likely to run without modification. Ultimately, if you use either language for practical applications, you should consider dependency\nNo matter what language you select, if you start using applications in practice you should consider managing your dependencies appropriately to ensure that code from the past continues to work in future."
  },
  {
    "objectID": "Foundations/01_getting_started.html#getting-started-with-ml",
    "href": "Foundations/01_getting_started.html#getting-started-with-ml",
    "title": "2  Getting started",
    "section": "2.2 Getting started with ML",
    "text": "2.2 Getting started with ML\n\n2.2.1 Data\nYou may have heard the saying that machine learning is at least 80% data, 20% modelling and that is generally true. The data frequently are the distinguishing factor between good models and great models.\nAs actuaries, we already have a sound basis in handling data - we are trained to be sceptical of data and to question unusual patterns. Many of us have experience of collecting and cleaning data from different sources for reserving and pricing jobs and we understand the importance of checks and reconciliations. Therefore, the main learning curves for actuaries in relation to data are likely to involve:\n\nsourcing external data, e.g. via web-scraping. This also includes learning to access SQL (or similar) data bases.\ncleaning and processing data in your language of choice (pandas in python may help here; data.table or tidyverse in R)\n\n\n\n2.2.2 Methods\nFor those new to ML, our advice is to approach learning techniques from a familiar direction. As actuaries most of us should have some familiarity with Generalised Linear Models (GLMs) if we have studied General Insurance Pricing, so this suggests a starting point. The steps are then:\n\ngain familiarity with using GLMs to apply traditional triangular reserving techniques. There are many papers and ready made R packages to help (e.g. glmReserve() in the R ChainLadder package).\napply regularised GLMs (e.g. apply lasso or ridge regression or mixture of both) to fit something that looks like a GLM but in a machine learning way - in these methods, the machine selects features for inclusion in the model.\nMove onto tree based methods like decision trees and random forests and from there into more advanced ML techniques such as Gradient Boosting Machines (GBMs). Note that GBMs include XGBoost, which is very popular among data scientists.\nAt this point you could then move to learning about neural networks. While these are likely to be very useful in the future, they are the least accessible both in terms of the actual methods - you need a good grasp of deep neural networks to understand the techniques most likely to be useful (such as recurrent neural networks - see, e.g., Deep Triangle) and also in terms of the data and hardware needed to get good results. You often need lots of data and high end computer equipment (or cloud based virtual machines) to train these models."
  },
  {
    "objectID": "Foundations/02_intro_to_r.html#why-use-r",
    "href": "Foundations/02_intro_to_r.html#why-use-r",
    "title": "3  Introduction to R",
    "section": "3.1 Why use R?",
    "text": "3.1 Why use R?\nThere are a number of benefits to using R. These are:\n\nFree: There is no license fee associated with using R (although beware that there may be license restrictions with using R packages).\nWidely used: R has a large user base, with data scientists, data miners, statisticians and actuaries using the tool around the world. That means that it only takes a quick search on the internet to get help with any problem that you might be experiencing with coding in R.\nAn R-ready workforce: R is now a part of the UK Institute and Faculty of Actuaries (IFoA) exam syllabus. Increasingly, more and more university students are also learning R as part of the undergraduate course in statistics and actuarial science.\nR is fast: R is usually faster at computational problems compared to traditional tools like Excel. For example, R can generate a million random standard Normal samples in less than a second. This is because of the way R is structured and the way it works. R can even be faster than server-based languages like SQL.\nPackages: There are many freely available reusable R code libraries available online called packages. These packages have been built by other R users and perform a variety of calculations or processes which means that you do not have to start from scratch all the time. The crowd-sourced sharing of packages helps accelerate development time in R. We will cover some of the more useful packages for actuaries in our future blog posts."
  },
  {
    "objectID": "Foundations/02_intro_to_r.html#how-do-i-get-started",
    "href": "Foundations/02_intro_to_r.html#how-do-i-get-started",
    "title": "3  Introduction to R",
    "section": "3.2 How do I get started?",
    "text": "3.2 How do I get started?\nR is available to download from the Comprehensive R Archive Network (CRAN) at https://cran.r-project.org/. CRAN is a network of servers around the world that contains copies of the R application and the various packages which is continuously being updated and maintained. To get started, you will first need to download R from the CRAN homepage.\n\nSelect the appropriate download for your operating system and follow the usual instructions to compile. You will usually want to download the most recent version, but occasionally you may need to download older versions if, for example, a package you need to use has not yet been updated to run on the latest version of R.\nThe instructions for each platform will be on the website as you click the version of R that you need. For this blog post, we will focus on the Windows version of R as it is the more common platform used by actuaries.\nFor first time users, you will need to install base R. It is important to note that base R is a console application i.e. the main interaction is through text. Most users will want to use an Integrated Development Environment (IDE) or a user interface. So, after downloading R, you should consider installing an IDE such as RStudio. You should install R first, then RStudio.\nRStudio provides a graphical user interface (GUI) that provides a more visual interaction when using R. You can download a free desktop version from the RStudio website: https://rstudio.com/. There are also paid options for support and for running and maintaining a server.\nOnce you have both R and RStudio installed, you are ready to begin coding."
  },
  {
    "objectID": "Foundations/02_intro_to_r.html#hello-world",
    "href": "Foundations/02_intro_to_r.html#hello-world",
    "title": "3  Introduction to R",
    "section": "3.3 Hello World!",
    "text": "3.3 Hello World!\nNow that you have the minimum software required installed, you can start coding. Getting started can be quite daunting as there are various schools of thought on how to use R and which packages. Base R is what comes built into R. There are also a growing series of data manipulation packages commonly known as tidyverse which consists of packages developed and maintained by RStudio. Beyond that there are other data manipulation packages such as data.table. Each have their advantages and disadvantages depending on the precise operation that you would like to perform\nThere are several free resources available for learning R which you can find on the internet. We list a few of them here:\n\nR for Data Science\nR-bloggers - an aggregator of online articles about R\nRweekly.org - a weekly curated list of updates from the R community\n\nFor more advanced users, useful reference guides are:\n\nAdvanced R, first edition more advanced techniques using mainly baseR\nAdvanced R, second edition the updated book, which incorporates some of the tidyverse ideas.\n\nAdditionally, you could also sign up to online courses from Coursera, Stanford, CodeAcademy, etc. that teach R with the added benefit of certification at the end."
  },
  {
    "objectID": "Foundations/02_intro_to_r.html#conclusion",
    "href": "Foundations/02_intro_to_r.html#conclusion",
    "title": "3  Introduction to R",
    "section": "3.4 Conclusion",
    "text": "3.4 Conclusion\nThere are, of course, other free statistical tools available out there. We have chosen to focus on R as it is one of the more prevalent programming languages being used in the actuarial space. R brings with it many benefits which you may find useful in the work that you do. Have a go and try it out!"
  },
  {
    "objectID": "Foundations/03_top_ten_r_packages.html#general",
    "href": "Foundations/03_top_ten_r_packages.html#general",
    "title": "4  My Top 10 R Packages for Data Analysis",
    "section": "4.1 General",
    "text": "4.1 General\n\n1. Tidyverse\nNo discussion of top R packages would be complete without the tidyverse. In a way, this is cheating because there are multiple packages included in this - data analysis with dplyr, visualisation with ggplot2, some basic modelling functionality, and comes with a fairly comprehensive book that provides an excellent introduction to usage.\nIf you were getting started with R, it’s hard to go wrong with the tidyverse toolkit. And if you are just getting started, check out our recent Insights - Starting the Data Analytics Journey - Data Collection video audio presentation. That and more can be found on our knowledge bank page.\n\nlibrary(tidyverse)\nggplot(mtcars, aes(mpg, disp, color=cyl)) + geom_point()"
  },
  {
    "objectID": "Foundations/03_top_ten_r_packages.html#data",
    "href": "Foundations/03_top_ten_r_packages.html#data",
    "title": "4  My Top 10 R Packages for Data Analysis",
    "section": "4.2 Data",
    "text": "4.2 Data\n\n2. Need for speed? dtplyr\nThere has been a perception that R is slow, but with packages like data.table, R has the fastest data extraction and transformation package in the West. However, the dplyr syntax may more familiar for those who use SQL heavily, and personally I find it more intuitive. So, dtplyr provides the best of both worlds.\n\nlibrary(dtplyr) \nlibrary(data.table)\nmtcars  |&gt;  \n  lazy_dt()  |&gt; \n  group_by(cyl)  |&gt; \n  summarise(total.count = n())  |&gt; \n  as.data.table()\n\n   cyl total.count\n1:   4          11\n2:   6           7\n3:   8          14\n\n\n\n\n3. Out of Memory? disk.frame\nOne major limitation of r data frames and Python’s pandas is that they are in memory datasets - consequently, medium sized datasets that SAS can easily handle will max out your work laptop’s measly 4GB RAM. The ideal solution would be to do those transformations on the data warehouse server, which would reduce data transfer and also should, in theory, have more capacity. If it runs with SQL, dplyr probably has a backend through dbplyr. Alternatively, with cloud computing, it is possible to rent computers with up to 3,904 GB of RAM.\nBut for those with a habit of exploding the data warehouse or those with cloud solutions being blocked by IT policy, disk.frame is an exciting new alternative. It does require some additional planning with respect to data chunks, but maintains a familiar syntax - check out the examples on the page.\nThe package stores data on disk, and so is only limited by disk space rather than memory…\n\n\n4. Parking it with parquet and Arrow\nRunning low on disk space once, I asked my senior actuarial analyst to do some benchmarking of different data storage formats: the “Parquet” format beat out sqlite, hdf5 and plain CSV - the latter by a wide margin. That experience is also likely not unique as well, considering this article where the author squashes a 500GB dataset to a mere fifth of its original size.\nIf you were working with a heavy workload with a need for distributed cluster computing, then sparklyr could be a good full stack solution, with integrations for Spark-SQL, and machine learning models xgboost, tensorflow and h2o.\nBut often you just want to write a file to disk, and all you need for that is Apache Arrow.\n\nlibrary(arrow)\nwrite_parquet(mtcars, \"test.parquet\") # Done!"
  },
  {
    "objectID": "Foundations/03_top_ten_r_packages.html#modelling",
    "href": "Foundations/03_top_ten_r_packages.html#modelling",
    "title": "4  My Top 10 R Packages for Data Analysis",
    "section": "4.3 Modelling",
    "text": "4.3 Modelling\n\n5. Trees: xgboost\nYou may have seen earlier videos from Zeming Yu on Lightgbm, myself on XGBoost and of course Minh Phan on CatBoost. Perhaps you’ve heard me extolling the virtues of h2o.ai for beginners and prototyping as well.\nLightGBM has become my favourite now in Python. It is incredibly fast, and although it has the limitation that it can only do leaf-wise models - unlike XGBoost which has the flexibility to use traditional depth-wise growth models as well - but a lower memory usage allows you to be greedier in putting large datasets into the model.\nSince this article was first published, installation of LightGBM has become much easier in R.\n\ninstall.packages(\"lightgbm\", repos = \"https://cran.r-project.org\")\n\nWith either package it is fairly straightforward to build a model - here we use model.matrix to convert categorical variables, then model with xgboost.\n\nlibrary(xgboost)\nlibrary(Matrix)\n# Road fatalities data - as previously seen in the YAP-YDAWG course\ndeaths &lt;- read.csv(\"https://raw.githubusercontent.com/ActuariesInstitute/YAP-YDAWG-R-Workshop/master/bitre_ardd_fatalities_dec_2018.csv\")\n\n# Explain age of the fatality based on speed limit, road user and crash type\nmodel_matrix = model.matrix(Age ~ Speed.Limit + Road.User + Crash.Type, data=deaths)[,-1]\nbst &lt;- xgboost(data = model_matrix, label = deaths$Age, nrounds=10, objective=\"reg:squarederror\")\n\n[1] train-rmse:34.509572 \n[2] train-rmse:28.322605 \n[3] train-rmse:24.726972 \n[4] train-rmse:22.752562 \n[5] train-rmse:21.717064 \n[6] train-rmse:21.189414 \n[7] train-rmse:20.914969 \n[8] train-rmse:20.779622 \n[9] train-rmse:20.712977 \n[10]    train-rmse:20.680121 \n\nxgb.importance(feature_names = colnames(model_matrix), model = bst) |&gt; \n  xgb.plot.importance()\n\n\n\n\nGenerally, sparse.model.matrix is more memory efficient than model.matrix, especially with large categorical levels. However, the code for the model explanation with the DALEX package (featured later) is made more complex. See this comment by the package author for an example of how to use sparse.model.matrix with xgboost and DALEX.\n\n\n6. Nets: keras\nNeural network models are generally better done in Python rather than R, since Facebook’s Pytorch and Google’s Tensorflow are built with it in mind. However in writing Analytics Snippet: Multitasking Risk Pricing Using Deep Learning I found Rstudio’s keras interface to be pretty easy to pick up.\nWhile most example usage and online tutorials will be in Python, they translate reasonably well to their R counterparts. The Rstudio team were also incredibly responsive when I filed a bug report and had it fixed within a day.\nFor another example of keras usage, the Swiss “Actuarial Data Science” Tutorial includes another example with paper and code.\nsince the time of writing, torch has become available in R and does not require a Python installation so is another alternative.\n\n\n7. Multimodel: MLR\nWorking with multiple models - say a linear model and a GBM - and being able to calibrate hyperparameters, compare results, benchmark and blending models can be tricky. This video on Applied Predictive Modeling by the author of the caret package explains a little more on what’s involved.\nIf you want to get up and running quickly, and are okay to work with just GLM, GBM and dense neural networks and prefer an all-in-one solution, h2o.ai works well. It does all those models, has good feature importance plots, and ensembles it for you with autoML too, as explained in this video by Jun Chen from the 2018 Weapons of Mass Deduction video competition. Ensembling h2o models got me second place in the 2015 Actuaries Institute Kaggle competition, so I can attest to its usefulness.\nmlr comes in for something more in-depth, with detailed feature importance, partial dependence plots, cross validation and ensembling techniques. It integrates with over 100 models by default and it is not too hard to write your own.\nThere is a handy cheat sheet.\nUpdate for 2020: the successor package mlr3 has matured significantly. It is now available on CRAN and the documentation has developed to become quite comprehensive. It has its own cheat sheets which can be found here. Like the original mlr package, it has many useful features for better model fitting. New users would generally benefit from using mlr3 for new projects today."
  },
  {
    "objectID": "Foundations/03_top_ten_r_packages.html#visualisation-and-presentation",
    "href": "Foundations/03_top_ten_r_packages.html#visualisation-and-presentation",
    "title": "4  My Top 10 R Packages for Data Analysis",
    "section": "4.4 Visualisation and Presentation",
    "text": "4.4 Visualisation and Presentation\n\n8. Too technical for Tableau (or too poor)? flexdashboard\nTo action insights from modelling analysis generally involves some kind of report or presentation. Rarely you may want to serve R model predictions directly - in which case OpenCPU may get your attention - but generally it is a distillation of the analysis that is needed to justify business change recommendations to stakeholders.\nFlexdashboard offers a template for creating dashboards from Rstudio with the click of a button. This extends R Markdown to use Markdown headings and code to signpost the panels of your dashboard.\nInteractivity similar to Excel slicers or VBA-enabled dropdowns can be added to R Markdown documents using Shiny. To do so, add ‘runtime: shiny’ to the header section of the R Markdown document. This is great for live or daily dashboards. It is also possible to produce static dashboards using only Flexdashboard and distribute over email for reporting with a monthly cadence.\nPreviously with the YAP-YDAWG R Workshop video presentation, we included an example of flexdashboard usage as a take-home exercise. Take a look at the code repository under “09_advanced_viz_ii.Rmd”!\n\n\n9. HTML Charts: plotly\nDifferent language, same package. Plot.ly is a great package for web charts in both Python and R. The documentation steers towards the paid server-hosted options but using for charting functionality offline is free even for commercial purposes. The interface is clean, and charts embeds well in RMarkdown documents.\nCheck out an older example using plotly with Analytics Snippet: In the Library\nOne notable downside is the hefty file size which may not be great for email. If that is an issue I would consider the R interface for Altair - it is a bit of a loop to go from R to Python to Javascript but the vega-lite javascript library it is based on is fantastic - user friendly interface, and what I use for my personal blog so that it loads fast on mobile. Leaflet is also great for maps.\n\n\n10. Explain it Like I’m Five: DALEX\nAlso featured in the YAP-YDAWG-R-Workshop, the DALEX package helps explain model prediction. Like mlr above, there is feature importance, actual vs model predictions, partial dependence plots:\n\nlibrary(DALEX)\nxgb_expl &lt;- explain(model = bst, data = model_matrix, y = deaths$Age)\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  xgb.Booster  (  default  )\n  -&gt; data              :  49734  rows  14  cols \n  -&gt; target variable   :  49734  values \n  -&gt; predict function  :  yhat.default will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package Model of class: xgb.Booster package unrecognized , ver. Unknown , task regression (  default  ) \n  -&gt; predicted values  :  numerical, min =  17.38693 , mean =  38.26188 , max =  64.52395  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -57.70045 , mean =  1.101735 , max =  72.88297  \n  A new explainer has been created!  \n\n# Variable splits type is either dependent on data by default, or \n# with uniform splits, shows an even split for plotting purposes\nresp &lt;- model_profile(xgb_expl, variables=\"Speed.Limit\", variable_splits_type = \"uniform\")\nplot(resp)\n\n\n\n\nYep, the data looks like it needs a bit of cleaning - check out the course materials! … but the key use of DALEX in addition to mlr is individual prediction explanations:\n\nbrk &lt;- predict_parts(xgb_expl, new_observation=model_matrix[1, ,drop=FALSE])\nplot(brk)"
  },
  {
    "objectID": "Foundations/03_top_ten_r_packages.html#concluding-thoughts",
    "href": "Foundations/03_top_ten_r_packages.html#concluding-thoughts",
    "title": "4  My Top 10 R Packages for Data Analysis",
    "section": "4.5 Concluding thoughts",
    "text": "4.5 Concluding thoughts\nWe have taken a journey with ten amazing packages covering the full data analysis cycle, from data preparation, with a few solutions for managing “medium” data, then to models - with crowd favourites for gradient boosting and neural network prediction, and finally to actioning business change - through dashboard and explanatory visualisations - and most of the runners up too…\nI would recommend exploring the resources in the many links as well, there is a lot of content that I have found to be quite informative. Did I miss any of your favourites? Let me know in the comments!"
  },
  {
    "objectID": "Foundations/04_a_tidyverse.html#a-short-introduction-to-how-rs-tidyverse-library-makes-data-processing-easy",
    "href": "Foundations/04_a_tidyverse.html#a-short-introduction-to-how-rs-tidyverse-library-makes-data-processing-easy",
    "title": "5  The tidyverse for actuaries",
    "section": "5.1 A short introduction to how R’s “tidyverse” library makes data processing easy",
    "text": "5.1 A short introduction to how R’s “tidyverse” library makes data processing easy\nThe tidyverse, developed by Hadley Wickham, is a collection of R packages designed to make every step of data analysis clear and easy to perform. Throughout this blog, I introduce the three packages from the tidyverse library that I have found most useful for my own actuarial work and explain how they can help overcome the shortcomings of more traditional methods such as Excel:\n\nreadr – for reading and writing data quickly\ntidyr – for cleaning data\ndplyr – for summarising and transforming data\n\nYou can download the full contents of the tidyverse in one go by installing the tidyverse “mega-package” directly. Alternatively you can download each package separately, as and when needed. For more information about using R, in general, our blog Introduction to R, provides a helpful introduction.\n\n5.1.1 readr\nDatasets with billions of data points are no longer uncommon. Just 20 years of a company’s premium data can easily exceed this. By specifying the number of rows and columns to be imported, readr’s read_csv function allows users to import large datasets (or subsets of the data) into R quickly. It is faster than opening the same files in Excel, particularly when your only purpose is to check the first few rows of content. This becomes especially practical when the number of rows of data exceed the number of rows in an Excel spreadsheet. Similarly, once you’ve used R to import and play around with your data, you can quickly write the file back out using the write_csv function.\n\n\n5.1.2 tidyr\ntidyr contains a collection of tools for transforming data into an easy-to-process format. To list a few, there are functions that can remove/replace NAs; separate out individual column entries; and expand/contract datasets into more manageable formats. Rather than resort to Excel, where these operations can often require hard coding or complex functions, tidyr provides a clear and reproducible way of transforming data.\nOne of my favourite tidyr functions is the complete function, which can be used to populate an incomplete table of triangular claims data with missing cohort and development period entries.\n\n\n5.1.3 dplyr\nOnce data has been imported and tidied, dplyr contains the functions to filter, group, merge and summarise data. The speed of operations is also less sensitive to dataset size than Excel – unlike Index Match, which will break down on a few thousand rows of data, dplyr’s equivalent left_join function can handle millions of rows in a matter of seconds.\nIf, for example, you have separate claims data files for individual lines of business, left_join provides a convenient way of aligning all datasets with their appropriate development periods and cohorts in one table.\nThe dplyr syntax sits at the heart of the tidyverse and is therefore a great first package to learn. The function names are deliberately interpretable making collaborative projects easy to follow. For those already familiar with dplyr, take a look at the blog post accompanying the most recent major update, which lists some useful new features.\nSince all tidyverse packages share the same design philosophy, each new package learned can naturally be incorporated into your existing models/processes.\nIt’s also possible to use dplyr with other backends for accessing code. In particular:\n\ndtplyr works with data.table so you have access to data.table’s speed while using dplyr syntax\ndbplyr translates your dplyr code to SQL for data stored in relational databases"
  },
  {
    "objectID": "Foundations/04_a_tidyverse.html#where-to-look-next",
    "href": "Foundations/04_a_tidyverse.html#where-to-look-next",
    "title": "5  The tidyverse for actuaries",
    "section": "5.2 Where to look next",
    "text": "5.2 Where to look next\nAll packages have their pros and cons, although the tidyverse is both fast and easy to read, there is a trade-off present, namely speed and dependencies. If processing speed is your number one priority and you want to limit your dependencies, then the data.table package may be of particular interest. The performances of popular data processing packages are benchmarked at https://h2oai.github.io/db-benchmark/, which can be a useful page to check before deciding what package to use for a certain piece of work.\nI encourage anyone wishing to learn more to read R for data science by Garrett Grolemund and Hadley Wickham, which is available for free online. It formally introduces the points above as well as other tidyverse tools for data science including the ggplot2 package for data visualisation, the stringr package for working with strings and the purrr package for more advanced functional programming. There is an active tidyverse community on Twitter and Stack Overflow with answers to practical problems other users have already faced and the packages themselves are constantly being developed and improved. I also recommend following Keith McNulty, who shares a lot of useful material on LinkedIn, Towards Data Science and his own blog as well as having a look at the tidyverse website itself."
  },
  {
    "objectID": "Foundations/04_a_tidyverse.html#final-take-away",
    "href": "Foundations/04_a_tidyverse.html#final-take-away",
    "title": "5  The tidyverse for actuaries",
    "section": "5.3 Final take away",
    "text": "5.3 Final take away\nWhether you work with big data or not, the tidyverse provides a great framework to write reproducible, easy-to-follow code for manipulating and summarising data. It will certainly help to:\n\navoid hard coding and misinterpreting complex excel functions\nimprove the interpretation and speed of existing R models\nconstruct new models/data processing systems in a harmonious framework"
  },
  {
    "objectID": "Foundations/04_b_datatable.html#what-is-data.table",
    "href": "Foundations/04_b_datatable.html#what-is-data.table",
    "title": "6  R’s data.table - a useful package for actuaries",
    "section": "6.1 What is data.table?",
    "text": "6.1 What is data.table?\ndata.table is a package for carrying out data manipulations in R of tabular data. This includes:\n\nadding and removing columns in a data set\nfiltering columns\nsorting data\njoining different data sources\nrolling joins\nsummarising data\n\nTabular structure includes columns of data where that data column is actually a list (unsurprisingly, called a list column). This greatly increases what you can do with your data. Essentially, if you need to do any type of data manipulation, you can probably do it with data.table."
  },
  {
    "objectID": "Foundations/04_b_datatable.html#why-use-data.table",
    "href": "Foundations/04_b_datatable.html#why-use-data.table",
    "title": "6  R’s data.table - a useful package for actuaries",
    "section": "6.2 Why use data.table?",
    "text": "6.2 Why use data.table?\nThere are a number of reasons for choosing data.table:\n\nIt is very fast and memory efficient, even for large data sets\nActively maintained and used by many people\nNo dependencies other than baseR\nFlexible\nConcise syntax\n\nOf course, there are other options for manipulating data. Popular choices include dplyr from the tidyverse suite, SQL software, or even just the tools in baseR. In this post we will focus on data.table."
  },
  {
    "objectID": "Foundations/04_b_datatable.html#now-for-the-details",
    "href": "Foundations/04_b_datatable.html#now-for-the-details",
    "title": "6  R’s data.table - a useful package for actuaries",
    "section": "6.3 Now for the details",
    "text": "6.3 Now for the details\n\n6.3.1 data.table is fast\ndata.table is much faster than dplyr or baseR for data manipulation tasks and can handle larger datasets. All development of data.table is done with speed in mind. It uses a number of tricks to produce better performance:\n\nAdding or removing columns from a data.table are done by reference or modifying in place, rather than by copying the entire table to a new location in memory.\nA data.table may have a key - once the key is created, extracting subgroups or joining tables by that key are extremely quick. Similarly, secondary indices allow fast access for other variables.\ndata.table supplies a number of optimised functions - e.g. fread() and fwrite() to read/write CSV files, fifelse(), fcoalesce(). fread() and fwrite() are so fast that there are many people who use the data.table package solely to access these functions.\n\nSome benchmarks for data manipulation are maintained at https://h2oai.github.io/db-benchmark/. Timings are given for datasets of different sizes - as data sets get larger, data.table really shines. Google data.table performance and you will find this message repeated in many places.\nMachine learning and large amounts of data often go hand-in-hand. So if you are doing your data manipulation in R, and have large amounts of data, you should strongly consider using data.table.\nSimilarly, if a lot of your work involves programming with large amounts of data, or where speed and memory optimisation is important, then data.table has a lot to offer.\n\n\n6.3.2 data.table is actively maintained\nIn the open source world, it is important to consider carefully the packages you are using before selecting a tool for repeat use.\n\nAre they actively maintained?\nAre bugs quickly fixed?\nAre new features regularly added?\nAre lots of people using the package to find the bugs / missing features?\n\ndata.table is a very popular package and is regularly maintained.\n\n\n6.3.3 data.table has no dependencies\nStrictly speaking, it has no dependencies other than baseR, with a policy to make the dependency on baseR as old as possible for as long as possible. For example, the current release of data.table (V1.31.1 as at October 2020) will still work with R v3.1.0 which was released in April 2014. This leads to a more stable product - code that you write now to manipulate data will most likely still work in 2 or 3 years time - and you won’t have to update 20 different packages before running that code either.\n\n\n6.3.4 data.table is flexible\nAs noted above, data.table contains a full suite of data manipulation tools. Furthermore, a data.table is also a data.frame so any data.frame code will work on a data.table. So you lose nothing, but gain a lot.\n\n\n6.3.5 Concise syntax\ndata.table syntax is very concise and minimalist. Whether this is a pro or con is subjective - this will appeal to some but the steep learning curve will be off-putting for others. Speaking for myself, verbose code such as dplyr or SQL or SAS make my head and fingers hurt(!) - terse data.table code is much more appealing to me. It’s fast to read and fast to type. However, the functions in dplyr are more transparent to newcomers.\nFor those new to data.table, there are plenty of online resources to draw on. In my experience, I’ve managed to find example code for many complex data manipulation jobs on StackOverflow; the difficult part has been coming up with the appropriate search phrase.\nAs an example of the syntax, the code below:\n\nextracts the subset of the iris data where Sepal.Length &lt; 6\ngroups the data by Species\ncalculates the number and the mean of the Sepal.Width and Petal.Length in each group and assigns column names to these summary statistics\n\n\n# setup\nlibrary(data.table)\ndata(iris)\nsetDT(iris)    # make iris a data.table\n\n# now do the data manipulation operations\niris[Sepal.Length &lt; 6.0, \n     .(num=.N, \n       mean_sepal_width = mean(Sepal.Width), \n       mean_petal_length = mean(Petal.Length)), \n     keyby=.(Species)]\n\n      Species num mean_sepal_width mean_petal_length\n1:     setosa  50         3.428000          1.462000\n2: versicolor  26         2.673077          3.969231\n3:  virginica   7         2.714286          4.971429"
  },
  {
    "objectID": "Foundations/04_b_datatable.html#conclusion",
    "href": "Foundations/04_b_datatable.html#conclusion",
    "title": "6  R’s data.table - a useful package for actuaries",
    "section": "6.4 Conclusion",
    "text": "6.4 Conclusion\nIf you or your team use R, then you should consider having data.table in your toolkit, particularly if:\n\nyou work with large data sets in R\nyou need fast, efficient code\nYou need to optimise your use of RAM\nyou are writing packages, software or repeatable tasks and want to minimise your dependencies for more robust and easier to maintain code.\nyou want shorter code"
  },
  {
    "objectID": "Foundations/04_b_datatable.html#resources-to-learn-data.table",
    "href": "Foundations/04_b_datatable.html#resources-to-learn-data.table",
    "title": "6  R’s data.table - a useful package for actuaries",
    "section": "6.5 Resources to learn data.table",
    "text": "6.5 Resources to learn data.table\nThe obvious place to start is with the package itself and its help documentation (links below), but there are many additional on-line resources to learn data.table.\n\nPackage home on github including news, updates, a brief guide and a cheat sheet.\nCRAN home with links to vignettes. Vignettes also available in R via browseVignettes(package=\"data.table\") after you have installed the package. There are currently 9 of these covering a wide range of topics.\nIf you are more comfortable with dplyr code and don’t mind the dependencies, then dtplyr provides a data.table backend to dplyr."
  },
  {
    "objectID": "Foundations/05_a_glms_r.html#introduction",
    "href": "Foundations/05_a_glms_r.html#introduction",
    "title": "7  Reserving with GLMs in R",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nThe first time I did a reserving job (back in 2001) I used GLMs. Coming from a statistical background and being new to the actuarial workplace at the time, this didn’t seem unusual to me. Since then, most of my reserving jobs have used GLMs - personally I find it a lot easier and less error-prone than working with excel templates. Also, once you know what you are doing, you can do everything with a GLM that you can do with an excel-based model, and then more.\nHowever, some people reading this article may be new to the idea of using GLMs in reserving. So I’m going to use an example where we start with a chain ladder model, fitted as a GLM and then explore the additional features that we can add using a GLM. All the R code will be shared here.\nThe material is mostly based on a 2016 CAS monograph Stochastic Loss Reserving Using Generalized Linear Models that I co-authored with Greg Taylor, and an accompanying personal blog post that works through replicating some of the modelling in the monograph. Take a look at these if you want to see more about this example.\nBefore we begin, let’s attach the R packages that we need, and turn off scientific notation."
  },
  {
    "objectID": "Foundations/05_a_glms_r.html#data",
    "href": "Foundations/05_a_glms_r.html#data",
    "title": "7  Reserving with GLMs in R",
    "section": "7.2 Data",
    "text": "7.2 Data\nThe data used here were sourced from the Meyers and Shi (2011) database, and are the workers compensation triangle of the New Jersey Manufacturers Group. They are displayed in Section 1.3 of the monograph. We’ve made a CSV file of the data (in long format) available here for convenience. If you want to load it in, then use the fread statement that points to the website address which is included in the comments below.\n\n# location of data on the website:\n# (Nov 2023 note: due to revamping of MLRWP website, link is broken. So have changed link here)\nmsdata &lt;- fread(\"./_glms_meyershi.csv\")\n\nsetDT(msdata)\n\n#print(msdata) for a simple look\n\n# printing the table in a nicer format\ndatatable(msdata) |&gt; \n  formatRound(c(\"cumulative\", \"incremental\"), digits = 0)\n\n\n\n\n\n\nSo we have four columns:\n\nacc_year: accident year, numbered from 1 to 10\ndev_year: development year, also numbered from 1 to 10\ncumulative: cumulative payments to date\nincremental: incremental payments for that accident year, development year combination.\n\nWe can also plot the data\n\np1 &lt;- ggplot(data=msdata, aes(x=dev_year, y=cumulative, colour=as.factor(acc_year))) +\n    geom_line(size=1) +\n    scale_color_viridis_d(begin=0.9, end=0) + \n    ggtitle(\"Cumulative\") + \n    theme_bw() + \n    theme(legend.position = \"none\", legend.title=element_blank(), legend.text=element_text(size=8))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\np2 &lt;- ggplot(data=msdata, aes(x=dev_year, y=incremental, colour=as.factor(acc_year))) +\n    geom_line(size=1) +\n    scale_color_viridis_d(begin=0.9, end=0) + \n    ggtitle(\"Incremental\") + \n    theme_bw() + \n    theme(legend.position = \"right\", legend.title=element_blank(), legend.text=element_text(size=8))\n\np1 + p2   # combine the plots using patchwork\n\n\n\n\nThe data look quite well behaved - each year seems to have a similar development pattern."
  },
  {
    "objectID": "Foundations/05_a_glms_r.html#chain-ladder-model",
    "href": "Foundations/05_a_glms_r.html#chain-ladder-model",
    "title": "7  Reserving with GLMs in R",
    "section": "7.3 Chain ladder model",
    "text": "7.3 Chain ladder model\n\n7.3.1 Fitting the model\nOur first model will be the familiar chain ladder (volume all) model. The monograph (and references therein) note that certain types of GLM gives exactly the same result as the chain ladder so I’m going to use that to get the chain ladder result.\nThe specific model I’m using that replicates the chain ladder result is the Over-dispersed Poisson (ODP) cross classified (cc) model (Sections 3.3.2 and 3.3.3 of the monograph).\nTo apply the model, we will use the glm function from the base R stats package. The cross-classified model requires separate levels for each of accident and development year so we first make a factor version of these variates. I’m also going to add a calendar year term (cal_year) for later use in model diagnostics.\nI use data.table for data manipulation. For those not familiar with it, := is an assignment operator and the syntax dt[, a := b] creates a new variable called a in the dt data.table (which is also a data.frame), and sets it equal to b. The comma at the start is there because the first part of a data.table command subsets the data and is left blank if there is no subsetting required.\n\nmsdata[, acc_year_factor := as.factor(acc_year)\n       ][, dev_year_factor := as.factor(dev_year)\n         ][, cal_year := acc_year + dev_year - 1]\n\nhead(msdata)\n\n   acc_year dev_year cumulative incremental acc_year_factor dev_year_factor\n1:        1        1      41821       41821               1               1\n2:        1        2      76550       34729               1               2\n3:        1        3      96697       20147               1               3\n4:        1        4     112662       15965               1               4\n5:        1        5     123947       11285               1               5\n6:        1        6     129871        5924               1               6\n   cal_year\n1:        1\n2:        2\n3:        3\n4:        4\n5:        5\n6:        6\n\n\nNow we fit the model and look at the results via summary.\n\nThe family is the quasipoisson - this is how we fit an ODP model with glm().\nThe link is log\nThe formula is simply incremental ~ 0 + acc_year_factor + dev_year_factor\n\nThe 0 tells glm() to fit a model without an intercept - which is how we fit the model in the monograph\n\n\n\nglm_fit1 &lt;- glm(data = msdata, \n    family = quasipoisson(link = \"log\"),\n    formula = \"incremental ~ 0 + acc_year_factor + dev_year_factor\")\n\n\nsummary(glm_fit1)\n\n\nCall:\nglm(formula = \"incremental ~ 0 + acc_year_factor + dev_year_factor\", \n    family = quasipoisson(link = \"log\"), data = msdata)\n\nCoefficients:\n                  Estimate Std. Error t value             Pr(&gt;|t|)    \nacc_year_factor1  10.65676    0.03164 336.794 &lt; 0.0000000000000002 ***\nacc_year_factor2  10.79533    0.02994 360.507 &lt; 0.0000000000000002 ***\nacc_year_factor3  10.89919    0.02887 377.465 &lt; 0.0000000000000002 ***\nacc_year_factor4  10.98904    0.02808 391.326 &lt; 0.0000000000000002 ***\nacc_year_factor5  11.03883    0.02783 396.654 &lt; 0.0000000000000002 ***\nacc_year_factor6  11.01590    0.02855 385.867 &lt; 0.0000000000000002 ***\nacc_year_factor7  11.00808    0.02945 373.734 &lt; 0.0000000000000002 ***\nacc_year_factor8  10.89050    0.03266 333.463 &lt; 0.0000000000000002 ***\nacc_year_factor9  10.83613    0.03669 295.348 &lt; 0.0000000000000002 ***\nacc_year_factor10 10.69108    0.05104 209.454 &lt; 0.0000000000000002 ***\ndev_year_factor2  -0.20466    0.02276  -8.993  0.00000000009767316 ***\ndev_year_factor3  -0.74741    0.02819 -26.512 &lt; 0.0000000000000002 ***\ndev_year_factor4  -1.01667    0.03284 -30.954 &lt; 0.0000000000000002 ***\ndev_year_factor5  -1.45160    0.04214 -34.446 &lt; 0.0000000000000002 ***\ndev_year_factor6  -1.83254    0.05471 -33.495 &lt; 0.0000000000000002 ***\ndev_year_factor7  -2.14026    0.07150 -29.933 &lt; 0.0000000000000002 ***\ndev_year_factor8  -2.34827    0.09312 -25.218 &lt; 0.0000000000000002 ***\ndev_year_factor9  -2.51317    0.12673 -19.831 &lt; 0.0000000000000002 ***\ndev_year_factor10 -2.66449    0.19930 -13.369  0.00000000000000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 114.5364)\n\n    Null deviance: 27479374.2  on 55  degrees of freedom\nResidual deviance:     4128.1  on 36  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n\n\nI’m now going to save a more data.table version of the coefficient table in the glm_fit1 object - this will be used later and having the coefficients available in a data.table makes things easier. I’ll call this coeff_table.\n\n# save the data for later use as a data.table\n# variable 1 = parameter name\n# variable 2 = parameter estimates\nglm_fit1$coeff_table &lt;- data.table(parameter = names(glm_fit1$coefficients), \n                                   coeff_glm_fit1 = glm_fit1$coefficients)\n\n\nhead(glm_fit1$coeff_table)\n\n          parameter coeff_glm_fit1\n1: acc_year_factor1       10.65676\n2: acc_year_factor2       10.79533\n3: acc_year_factor3       10.89919\n4: acc_year_factor4       10.98904\n5: acc_year_factor5       11.03883\n6: acc_year_factor6       11.01590\n\n\n\n\n7.3.2 Loss reserve\nNow we’ll have a look at the loss reserve. If you’ve done the chain ladder calculations, you should find this gives the same answer.\n\n# first make the lower triangle data set\nay &lt;- NULL\ndy &lt;- NULL\n\n\nfor(i in 2:10){\n    ay &lt;- c(ay, rep(i, times=(i-1)))\n    dy &lt;- c(dy, (10-i+2):10)\n}\n\n\nfutdata &lt;- data.table(acc_year = ay, dev_year = dy)\n\n# make factors\nfutdata[, cal_year := acc_year + dev_year\n        ][, acc_year_factor := as.factor(acc_year)\n          ][, dev_year_factor := as.factor(dev_year)]\n\n# make the prediction and sum by acc_year\nx &lt;- predict(glm_fit1, newdata = futdata, type=\"response\")\nfutdata[, incremental := x]\n\n\n# data.table syntax to get summary by accident year\nocl_year &lt;- futdata[,  lapply(.SD, sum), .SDcols=c(\"incremental\"), by=\"acc_year\"]\n\n# total ocl\nocl_total &lt;- futdata[, sum(incremental)]\n\n# print the acc year table with total\nocl_year[, acc_year := as.character(acc_year) ]  # to make a table with total row\nocl_year_print &lt;- rbind(ocl_year, data.table(acc_year=\"Total\", incremental=ocl_total))\nsetnames(ocl_year_print, \"incremental\", \"OCL\")  # rename column for printing\n\ndatatable(ocl_year_print) |&gt; \n  formatRound(\"OCL\", digits = 0)\n\n\n\n\n\n\n\n\n7.3.3 Model diagnostics\n\n7.3.3.1 Calculations\nSo far, we’ve fitted a model and have calculated the loss reserve. We can do all this with the chain ladder algorithm. Now we start looking at some of the extras that using a GLM gives us - for a start the statistical model structure means that we can calculate residuals with known properties, assuming the assumptions underlying the model are valid.\nIf the model assumptions are satisfied by the data, then the residuals should not have any remaining structure. Plots of residuals can therefore be useful to detect violations of model assumptions. Note that we need to be careful to use the right types of residuals - in many cases we want to use standardised deviance residuals:\n\nDeviance residuals because the more recognisable Pearson residuals (based on actual - fitted) are difficult to interpret for non-normal models.\nStandardised because the raw residuals are on different scales depending on the scale of the underlying values.\n\nSome more details may be found in Chapter 6 of the monograph and also in Chapter 5 of A Practitioner’s Introduction to Stochastic Reserving.\nHere we look at the following:\n\nResidual Scatterplots\n\nby linear predictor\nby accident, development and calendar years\nif the model assumptions are satisfied then the residuals should look homogeneous (or in layperson’s language, like a random cloud), centred around zero)\n\nHeat map of actual vs fitted laid out in triangular form\n\nIn this we get the actual/fitted ratio in each (acc, dev) cell (subject to lower and upper bounds of [0.5, 2]) and then plot the colour-coded triangle of the actual/fitted values\nheat maps are helpful to check for model fit and may help to identify missing interactions.\n\n\nWe have to prepare the data by adding the fitted values and residuals.\n\nBecause this model has a lot of parameters, there are two observations where the fitted is exactly equal to the actual – (acc_year=1, dev_year=10) and (acc_year=10, dev_year=1). This is because these observations have a unique parameter.\nThe deviance calculations below return NaN (not a number) for these points, but the residual should really be 0 so this adjustment is made manually.\nAlso add actual/fitted ratios and the log of these (restricted to the range [log(0.5), log(2)]) - these will be used for a heatmap later.\n\nThe restricted range is used to generate easier to read shadings in the heat-map, while the conversion to log means that the shading scales will be similar intensity for \\(x\\)% and \\(1/x\\) %\n\n\nTechnical note on residuals with glm()\n\nThe residuals in a glm object accessed with $residuals are residuals used in the model fitting algorithm.\nFor diagnostic purposes, we require the standardised deviance residuals.\n\nThese are the signed square roots of the contribution of the ith observation to the deviance, divided by hat matrix values.\nThe stats::rstandard() function may be used with glm objects to extract the standardised deviance residuals.\n\n\n\nmsdata[, residuals1 := rstandard(glm_fit1)\n       ][, fitted1 := glm_fit1$fitted.values\n         ][, linear_predictor1 := log(fitted1)\n           ][, AvsF1 := incremental / fitted1\n             ][, AvsF_restricted1 := log(pmax(0.5, pmin(2,AvsF1)))]\n\n# check for NaN residuals\nmsdata[is.nan(residuals1),]\n\n   acc_year dev_year cumulative incremental acc_year_factor dev_year_factor\n1:        1       10     144781        2958               1              10\n2:       10        1      43962       43962              10               1\n   cal_year residuals1 fitted1 linear_predictor1 AvsF1         AvsF_restricted1\n1:       10        NaN    2958          7.992269     1 0.0000000000000017763568\n2:       10        NaN   43962         10.691081     1 0.0000000000000008881784\n\n# these occur where we expect them so so replace with 0\n# the is.nan(residuals1) in an example of data.table subsetting - we only set the NaN resids to 0\nmsdata[is.nan(residuals1), residuals1 := 0]\n\nLook at first 10 rows\n\nhead(msdata, 10)  # look at first 10 rows\n\n    acc_year dev_year cumulative incremental acc_year_factor dev_year_factor\n 1:        1        1      41821       41821               1               1\n 2:        1        2      76550       34729               1               2\n 3:        1        3      96697       20147               1               3\n 4:        1        4     112662       15965               1               4\n 5:        1        5     123947       11285               1               5\n 6:        1        6     129871        5924               1               6\n 7:        1        7     134646        4775               1               7\n 8:        1        8     138388        3742               1               8\n 9:        1        9     141823        3435               1               9\n10:        1       10     144781        2958               1              10\n    cal_year  residuals1   fitted1 linear_predictor1     AvsF1\n 1:        1 -0.37704981 42478.725         10.656759 0.9845164\n 2:        2  0.06821815 34616.808         10.452095 1.0032410\n 3:        3  0.02211088 20117.514          9.909346 1.0014657\n 4:        4  0.50192703 15368.757          9.640092 1.0387958\n 5:        5  1.36344235  9948.355          9.205163 1.1343584\n 6:        6 -1.13119533  6796.876          8.824218 0.8715769\n 7:        7 -0.33754581  4996.553          8.516503 0.9556589\n 8:        8 -0.56680264  4058.159          8.308485 0.9220929\n 9:        9 -0.01379476  3441.253          8.143591 0.9981829\n10:       10  0.00000000  2958.000          7.992269 1.0000000\n            AvsF_restricted1\n 1: -0.015604749951504761490\n 2:  0.003235741327327290584\n 3:  0.001464610789023347613\n 4:  0.038062125103392241421\n 5:  0.126067171138105615924\n 6: -0.137451186377783390880\n 7: -0.045354245283896227336\n 8: -0.081109303248728628621\n 9: -0.001818723883632102989\n10:  0.000000000000001776357\n\n\n\n\n7.3.3.2 Plotting\nNow let’s look at the residual scatterplots. In the linear predictor scatterplot, the points are colour coded so that the lighter points belong to the earlier development years, and the darker points belong to the later ones.\n\np1 &lt;- ggplot(data=msdata, aes(x=linear_predictor1, y=residuals1, colour=dev_year)) +\n    geom_point(size=2) +\n    scale_colour_viridis(begin=0.9, end=0) +\n    theme_bw() + \n    theme(legend.position = \"none\") +\n    ggtitle(\"Linear predictor\")\n\n\np2 &lt;- ggplot(data=msdata, aes(x=acc_year, y=residuals1)) +\n    geom_point(size=2, colour=\"#2d708eff\") +\n    theme_bw() + \n    ggtitle(\"Accident year\")\n\np3 &lt;- ggplot(data=msdata, aes(x=dev_year, y=residuals1)) +\n    geom_point(size=2, colour=\"#2d708eff\") +\n    theme_bw() + \n    ggtitle(\"Development year\")\n\np4 &lt;- ggplot(data=msdata, aes(x=cal_year, y=residuals1)) +\n    geom_point(size=2, colour=\"#2d708eff\") +\n    theme_bw() + \n    ggtitle(\"Calendar year\")\n\n#p &lt;- plot_grid(p1, p2, p3, p4, nrow=2, rel_widths = c(1,1,1,1))\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\nThese results are quite good - bear in mind there are only a small number of points so plots must be interpreted in relation to this. In particular:\n\nThe residuals do not appear to fan out or fan in (once you take into account that later development years have small number of points)\nThey appear centred around 0\n\nNow construct and draw the heat map. Note that the colours are:\n\nblue (A/F = 50%)\nwhite (A/F = 100%)\nred (A/F = 200%)\n\nwith shading for in-between values\n\n# heatmap code\n# to get the correct shading I've plotted the log of the restricted A/F values\n\np_hm &lt;- ggplot(data=msdata, aes(x=dev_year, y=acc_year)) + \n    geom_tile(aes(fill = AvsF_restricted1))+scale_y_reverse()+\n    scale_fill_gradient2(name=\"AvF_min\", low=\"royalblue\", mid=\"white\", high=\"red\", midpoint=0, space=\"Lab\", na.value=\"grey50\", guide=\"colourbar\")+\n    labs(x=\"Development year\", y=\"Accident year\")+\n    theme(legend.position = \"none\")+\n    theme(axis.title.x = element_text(size=8), axis.text.x  = element_text(size=7))+\n    theme(axis.title.y = element_text(size=8), axis.text.y  = element_text(size=7))+\n    theme(panel.background = element_rect(fill = \"grey\", colour = \"grey\", size = 2, linetype = \"solid\"),\n          panel.grid = element_line(colour=\"grey\")) + \n    NULL\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\nprint(p_hm)\n\n\n\n\nIn a heat map for a reserving triangle, we look for a random scattering of red and blue points. This plot looks quite good (though we’ll revisit this shortly)."
  },
  {
    "objectID": "Foundations/05_a_glms_r.html#refining-the-model",
    "href": "Foundations/05_a_glms_r.html#refining-the-model",
    "title": "7  Reserving with GLMs in R",
    "section": "7.4 Refining the model",
    "text": "7.4 Refining the model\nWe could stop here - and just use the results from this model, which match those produced by the chain ladder. The diagnostics suggest that the model fits quite well. However, because this is a GLM, we have more options than just replicating the chain ladder.\nIn particular, can we:\n\nidentify simplifications to the model to make it more parsimonious (i.e. reduce the number of parameters)?\nidentify any areas of poorer fit that may suggest missing model terms including interactions?\n\n\n7.4.1 Simplifying the model\nFirst we consider if we can use a parametric shape for the accident and development year parameters. The end result should be something similar to the chain ladder approach but with far fewer parameters.\n\n7.4.1.1 Accident year\nFirst plot the accident year parameters.\n\n# extract the data\ndt_acc_year &lt;- glm_fit1$coeff_table[grepl(\"acc_year\", parameter),  \n                                    ][, acc_year := as.integer(gsub(\"acc_year_factor\", \"\", parameter))]\n\n\n# plot\nggplot(data=dt_acc_year, aes(x=acc_year, y=coeff_glm_fit1)) +\n    geom_line(size=2, colour=\"#440154ff\") +\n    geom_point(size=4, colour=\"#440154ff\") + \n    theme_bw() + \n    ggtitle(\"Accident year parameters\")\n\n\n\n\n\nNote that their shape closely resembles that of a parabola.\nThis suggests that we can replace the 10 accident year parameters by\n\nthe overall intercept\nan acc_year term\nan acc_year squared term\n\nSo refit the model on this basis.\n\nDrop the 0 from the glm_fit1 formula to allow the model to have an intercept\nReplace the acc_year_factor term with the parabola terms.\n\n\n\n# add an x^2 term (we already have the x - acc_year)\nmsdata[, acc_year_2 := acc_year^2]\n\nglm_fit2 &lt;- glm(data = msdata, \n    family = quasipoisson(link = \"log\"),\n    formula = \"incremental ~ acc_year + acc_year_2 + dev_year_factor\")\n\n\nsummary(glm_fit2)\n\n\nCall:\nglm(formula = \"incremental ~ acc_year + acc_year_2 + dev_year_factor\", \n    family = quasipoisson(link = \"log\"), data = msdata)\n\nCoefficients:\n                   Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       10.470978   0.034414 304.264 &lt; 0.0000000000000002 ***\nacc_year           0.200075   0.014219  14.071 &lt; 0.0000000000000002 ***\nacc_year_2        -0.017907   0.001356 -13.210 &lt; 0.0000000000000002 ***\ndev_year_factor2  -0.205555   0.021276  -9.661     0.00000000000243 ***\ndev_year_factor3  -0.750108   0.026492 -28.314 &lt; 0.0000000000000002 ***\ndev_year_factor4  -1.014806   0.030982 -32.755 &lt; 0.0000000000000002 ***\ndev_year_factor5  -1.451958   0.039797 -36.484 &lt; 0.0000000000000002 ***\ndev_year_factor6  -1.830488   0.051662 -35.432 &lt; 0.0000000000000002 ***\ndev_year_factor7  -2.142154   0.067504 -31.734 &lt; 0.0000000000000002 ***\ndev_year_factor8  -2.352674   0.087924 -26.758 &lt; 0.0000000000000002 ***\ndev_year_factor9  -2.513722   0.119637 -21.011 &lt; 0.0000000000000002 ***\ndev_year_factor10 -2.660878   0.187820 -14.167 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 102.5776)\n\n    Null deviance: 750824  on 54  degrees of freedom\nResidual deviance:   4427  on 43  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n\n\nWe see in the coefficient table part of the summary that the two acc_year terms are highly significant.\n\nNow extract the coefficients and compare the previous and current fits.\n\nRemember that the intercept must be included in these calculations.\nAgain, save the coefficient table as a data.table in the glm_fit2 object for later use.\n\n\n# extract the coefficient table\nglm_fit2$coeff_table &lt;- data.table(parameter = names(glm_fit2$coefficients), \n                                   coeff_glm_fit2 = glm_fit2$coefficients)\n\n#print(glm_fit2$coeff_table)  # easy print\n\ndatatable(glm_fit2$coeff_table) |&gt; \n  formatRound(\"coeff_glm_fit2\", digits = 4)\n\n\n\n\n\n\nNow compare the past and current parameter estimates for accident year.\n\n# pull out the acc year coefficinents only\ndt_acc_year[, coeff_glm_fit2 := glm_fit2$coeff_table[parameter == \"acc_year\", coeff_glm_fit2]*acc_year + \n                glm_fit2$coeff_table[parameter == \"acc_year_2\", coeff_glm_fit2]*acc_year^2 + \n                glm_fit2$coeff_table[parameter == \"(Intercept)\", coeff_glm_fit2]]\n\n# make long for ggplot\ndt_acc_year_plot &lt;- melt(dt_acc_year, \n                         id.vars = \"acc_year\", \n                         measure.vars = c(\"coeff_glm_fit1\", \"coeff_glm_fit2\"), \n                         variable.name=\"model\", \n                         value = \"estimate\")\n\n# remove the coeff_ from the model names\ndt_acc_year_plot[, model := gsub(\"coeff_\", \"\", model, fixed=TRUE)]\n\nggplot(data=dt_acc_year_plot, aes(x=acc_year, y=estimate, colour=model)) +\n    geom_line(size=2) +\n    geom_point(size=4) +\n    scale_colour_viridis_d(begin=0, end=0.5) + \n    theme_bw() + \n    ggtitle(\"Accident year parameters\")\n\n\n\n\nThis looks very good - the fit is very similar, but we have 7 fewer parameters.\n\n\n7.4.1.2 Development year\n\nNow we do the same thing for development year\nNote that the glm_fit2 model (and the glm_fit1 model too) do not have a parameter for dev_year = 1 as this is the base level.\n\nThis means that the parameter is really 0, so we must remember to include this.\n\n\n\n# extract the data\ndt_dev_year &lt;- glm_fit2$coeff_table[grepl(\"dev_year\", parameter),  \n                                    ][, dev_year := as.integer(gsub(\"dev_year_factor\", \"\", parameter))][]   # known data.table printing bug\n\n# add year 1\ndt_dev_year &lt;- rbind(dt_dev_year, data.table(parameter=\"dev_year_factor1\", coeff_glm_fit2=0, dev_year=1))\nsetorder(dt_dev_year, dev_year)\n\n\n# plot\nggplot(data=dt_dev_year, aes(x=dev_year, y=coeff_glm_fit2)) +\n    geom_line(size=2, colour=\"#440154ff\") +\n    geom_point(size=4, colour=\"#440154ff\") +\n    theme_bw() +\n    ggtitle(\"Development year parameters\")\n\n\n\n\n\nLooking at this plot, it appears that a straight line would fit quite well\nThis fit would be improved by allowing the straight line to bend (have a knot) at dev_year = 7\n\nSo let’s try this below\nnote we actually fit dev_year - 1 rather than dev_year\n\nthis means that the parameter estimate at dev_year = 1 is 0, just as it is in the glm_fit2 model, so it makes the results comparable\nif we fit dev_year, then the parameter estimate at dev_year=1 would be non-zero, so the two fits would be shifted relative to each other and we would need to adjust for that.\n\n\n\n\n# add dev-1 and dev-7 terms\nmsdata[, dev_year_m1 := dev_year - 1]\nmsdata[, dev_year_ge_7 := pmax(dev_year-7.5, 0)]\n\n# fit the model\nglm_fit3 &lt;- glm(data = msdata, \n    family = quasipoisson(link = \"log\"),\n    formula = \"incremental ~ acc_year + acc_year_2 + dev_year_m1 + dev_year_ge_7\")\n\n# extract and save the coefficient table\nglm_fit3$coeff_table &lt;- data.table(parameter = names(glm_fit3$coefficients), \n                                   coeff_glm_fit3 = glm_fit3$coefficients)\n\n# display a summary of the model\nsummary(glm_fit3)\n\n\nCall:\nglm(formula = \"incremental ~ acc_year + acc_year_2 + dev_year_m1 + dev_year_ge_7\", \n    family = quasipoisson(link = \"log\"), data = msdata)\n\nCoefficients:\n               Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)   10.509475   0.052096 201.734 &lt; 0.0000000000000002 ***\nacc_year       0.204224   0.021608   9.451     0.00000000000104 ***\nacc_year_2    -0.018295   0.002058  -8.891     0.00000000000719 ***\ndev_year_m1   -0.364073   0.008845 -41.160 &lt; 0.0000000000000002 ***\ndev_year_ge_7  0.238860   0.088426   2.701              0.00941 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 242.0614)\n\n    Null deviance: 750824  on 54  degrees of freedom\nResidual deviance:  11879  on 50  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nAssuming the fit is satisfactory, our original model with 19 parameters has now been simplified to 5 parameters - much more parsimonious and robust.\nLet’s check the fit by dev_year to see.\n\n\n# get the dev_year fit under the new model and add to the data.table containing the factor level parameters\np1 &lt;- glm_fit3$coeff_table[parameter == \"dev_year_m1\", coeff_glm_fit3]\np2 &lt;- glm_fit3$coeff_table[parameter == \"dev_year_ge_7\", coeff_glm_fit3]\ndt_dev_year[, coeff_glm_fit3 := p1*(dev_year-1) + p2*pmax(0, dev_year-7.5) ]\n\n\n# make long for ggplot\ndt_dev_year_plot &lt;- melt(dt_dev_year, id.vars = \"dev_year\", measure.vars = c(\"coeff_glm_fit2\", \"coeff_glm_fit3\"), variable.name=\"model\", value = \"estimate\")\n\n# remove the coeff_ from the model names\ndt_dev_year_plot[, model := gsub(\"coeff_\", \"\", model, fixed=TRUE)]\n\n\nggplot(data=dt_dev_year_plot, aes(x=dev_year, y=estimate, colour=model)) +\n    geom_line(size=2) +\n    geom_point(size=4) +\n    scale_colour_viridis_d(begin=0, end=0.5) +\n    theme_bw() +\n    ggtitle(\"Development year parameters\")\n\n\n\n\n\nThis looks good.\nHowever dev_year = 2 is a bit underfit in the latest model, so we can add something to improve this fit (a term at dev_year=2)\nSo refit and replot.\n\n\nmsdata[, dev_year_eq_2 := as.integer(dev_year == 2)]\n\nglm_fit4 &lt;- glm(data = msdata, \n    family = quasipoisson(link = \"log\"),\n    formula = \"incremental ~ acc_year + acc_year_2 + dev_year_m1 + dev_year_ge_7 + dev_year_eq_2\")\n\n\nglm_fit4$coeff_table &lt;- data.table(parameter = names(glm_fit4$coefficients), coeff_glm_fit4 = glm_fit4$coefficients)\n\n\np1 &lt;- glm_fit4$coeff_table[parameter == \"dev_year_m1\", coeff_glm_fit4]\np2 &lt;- glm_fit4$coeff_table[parameter == \"dev_year_ge_7\", coeff_glm_fit4]\np3 &lt;- glm_fit4$coeff_table[parameter == \"dev_year_eq_2\", coeff_glm_fit4]\ndt_dev_year[, coeff_glm_fit4 := p1*(dev_year-1) + p2*pmax(0, dev_year-7.5) + p3*(dev_year == 2) ]\n\n\n# make long for ggplot\ndt_dev_year_plot &lt;- melt(dt_dev_year, id.vars = \"dev_year\", measure.vars = c(\"coeff_glm_fit2\", \"coeff_glm_fit4\"), variable.name=\"model\", value = \"estimate\")\n\n# remove the coeff_ from the model names\ndt_dev_year_plot[, model := gsub(\"coeff_\", \"\", model, fixed=TRUE)]\n\n\nggplot(data=dt_dev_year_plot, aes(x=dev_year, y=estimate, colour=model)) +\n    geom_line(size=2) +\n    geom_point(size=4) +\n    scale_colour_viridis_d(begin=0, end=0.5) +\n    theme_bw() +\n    ggtitle(\"Development year parameters\")\n\n\n\n\nLooks good! Fitting dev_year=2 better has also improved the tail fitting (dev_year&gt;7).\n\n\n\n7.4.2 Identifying missing structure\n\nThe second part of the model refining process involves checking for missing structure.\nLet’s have a better look at the heat map, as it stands after the model simplification process\n\n\nmsdata[, residuals4 := rstandard(glm_fit4)\n       ][, fitted4 := glm_fit4$fitted.values\n         ][, linear_predictor4 := log(fitted4)\n           ][, AvsF4 := incremental / fitted4\n             ][, AvsF_restricted4 := log(pmax(0.5, pmin(2,AvsF4)))]\n\n\np_hm &lt;- ggplot(data=msdata, aes(x=dev_year, y=acc_year)) + \n    geom_tile(aes(fill = AvsF_restricted4))+scale_y_reverse()+\n    scale_fill_gradient2(name=\"AvF_min\", low=\"royalblue\", mid=\"white\", high=\"red\", midpoint=0, space=\"Lab\", na.value=\"grey50\", guide=\"colourbar\")+\n    labs(x=\"Development year\", y=\"Accident year\")+\n    theme(legend.position = \"none\")+\n    theme(axis.title.x = element_text(size=8), axis.text.x  = element_text(size=7))+\n    theme(axis.title.y = element_text(size=8), axis.text.y  = element_text(size=7))+\n    theme(panel.background = element_rect(fill = \"grey\", colour = \"grey\", size = 2, linetype = \"solid\"),\n          panel.grid = element_line(colour=\"grey\")) + \n    NULL\n\nprint(p_hm)\n\n\n\n\nLet’s add some annotations to highlight some structure\n\np_hm + \n    annotate(geom=\"rect\", xmin= 0.5, xmax=1.5, ymin=0.5, ymax=6.5, colour=\"darkblue\", alpha=0.1, size=1.5) +\n    annotate(geom=\"rect\", xmin= 0.5, xmax=1.5, ymin=6.5, ymax=10.5, colour=\"darkred\", alpha=0.1, size=1.5) +\n    annotate(geom=\"rect\", xmin= 1.5, xmax=2.5, ymin=0.5, ymax=6.5, colour=\"darkred\", alpha=0.1, size=1.5) +\n    annotate(geom=\"rect\", xmin= 1.5, xmax=2.5, ymin=6.5, ymax=9.5, colour=\"darkblue\", alpha=0.1, size=1.5) +\n    annotate(geom=\"segment\", x=3, xend=3, y=1, yend=8, arrow=arrow(), colour=\"darkblue\", size=2) +\n    annotate(geom=\"rect\", xmin= 3.5, xmax=4.5, ymin=0.5, ymax=7.5, colour=\"darkred\", alpha=0.1, size=1.5) \n\n\n\n\nWe see:\n\ndevelopment year 1, a distinct area of blue in the earlier accident years (A &lt; F), followed by red (A &gt; F)\ndevelopment year 2, a distinct area of red in the earlier accident years (A &gt; F), followed by blue (A &lt; F)\ndevelopment year 3, a possible progression from red to blue with increasing accident year (F increasing relative to A)\ndevelopment year 4, nearly all red (A &gt; F)\n\nThis suggests the payment pattern has altered and can be accommodated by (mostly) interaction terms within the GLM. Consider adding the following terms:\n\n(development year = 1) * (accident year is between 1 and 6)\n(development year = 2) * (accident year is between 1 and 6)\n(development year = 3) * (accident year linear trend)\n(development year = 4)\n\nSo, let’s refit the model with terms to capture these and have a look at the heat map again\n\n# add the new terms\nmsdata[, dev_year_eq_1 := as.integer(dev_year == 1)]\nmsdata[, dev_year_eq_3 := as.integer(dev_year == 3)]\nmsdata[, dev_year_eq_4 := as.integer(dev_year == 4)]\nmsdata[, acc_year_1_6 := as.integer(acc_year &gt;= 1 & acc_year &lt;= 6)]\n\n\nglm_fit5 &lt;- glm(data = msdata, \n    family = quasipoisson(link = \"log\"),\n    formula = \"incremental ~ acc_year + acc_year_2 + dev_year_m1 + dev_year_ge_7 + dev_year_eq_2 + dev_year_eq_4 +\n    dev_year_eq_1:acc_year_1_6 +  dev_year_eq_2:acc_year_1_6 + dev_year_eq_3:acc_year \")\n\n\nsummary(glm_fit5)\n\n\nCall:\nglm(formula = \"incremental ~ acc_year + acc_year_2 + dev_year_m1 + dev_year_ge_7 + dev_year_eq_2 + dev_year_eq_4 +\\n\\tdev_year_eq_1:acc_year_1_6 +  dev_year_eq_2:acc_year_1_6 + dev_year_eq_3:acc_year \", \n    family = quasipoisson(link = \"log\"), data = msdata)\n\nCoefficients:\n                            Estimate Std. Error t value             Pr(&gt;|t|)\n(Intercept)                10.490384   0.032004 327.787 &lt; 0.0000000000000002\nacc_year                    0.206624   0.010871  19.007 &lt; 0.0000000000000002\nacc_year_2                 -0.018333   0.001140 -16.078 &lt; 0.0000000000000002\ndev_year_m1                -0.368487   0.006911 -53.317 &lt; 0.0000000000000002\ndev_year_ge_7               0.271988   0.044265   6.145           0.00000019\ndev_year_eq_2               0.037485   0.024691   1.518              0.13596\ndev_year_eq_4               0.052800   0.023821   2.217              0.03175\ndev_year_eq_1:acc_year_1_6 -0.067134   0.028071  -2.392              0.02102\ndev_year_eq_2:acc_year_1_6  0.127316   0.031042   4.101              0.00017\nacc_year:dev_year_eq_3     -0.011262   0.003866  -2.913              0.00556\n                              \n(Intercept)                ***\nacc_year                   ***\nacc_year_2                 ***\ndev_year_m1                ***\ndev_year_ge_7              ***\ndev_year_eq_2                 \ndev_year_eq_4              *  \ndev_year_eq_1:acc_year_1_6 *  \ndev_year_eq_2:acc_year_1_6 ***\nacc_year:dev_year_eq_3     ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 53.93331)\n\n    Null deviance: 750824.1  on 54  degrees of freedom\nResidual deviance:   2426.9  on 45  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n\n# save for later use\nglm_fit5$coeff_table &lt;- data.table(parameter = names(glm_fit5$coefficients), coeff_glm_fit5 = glm_fit5$coefficients)\n\nThis model should match that displayed in Table 7-5 of the monograph - and indeed it does (some very minor differences in parameter values - the model in the monograph was fitted in SAS).\nLook at the updated heat map again with the annotations - has the model resolved the identified issues?\n\n# attach fitteds and residuals\nmsdata[, residuals5 := rstandard(glm_fit5)\n       ][, fitted5 := glm_fit5$fitted.values\n         ][, linear_predictor5 := log(fitted5)\n           ][, AvsF5 := incremental / fitted5\n             ][, AvsF_restricted5 := log(pmax(0.5, pmin(2,AvsF5)))]\n\n\n\np_hm &lt;- ggplot(data=msdata, aes(x=dev_year, y=acc_year)) + \n    geom_tile(aes(fill = AvsF_restricted5))+scale_y_reverse()+\n    scale_fill_gradient2(name=\"AvF_min\", low=\"royalblue\", mid=\"white\", high=\"red\", midpoint=0, space=\"Lab\", na.value=\"grey50\", guide=\"colourbar\")+\n    labs(x=\"Development year\", y=\"Accident year\")+\n    theme(legend.position = \"none\")+\n    theme(axis.title.x = element_text(size=8), axis.text.x  = element_text(size=7))+\n    theme(axis.title.y = element_text(size=8), axis.text.y  = element_text(size=7))+\n    theme(panel.background = element_rect(fill = \"grey\", colour = \"grey\", size = 2, linetype = \"solid\"),\n          panel.grid = element_line(colour=\"grey\")) + \n    annotate(geom=\"rect\", xmin= 0.5, xmax=1.5, ymin=0.5, ymax=6.5, colour=\"darkblue\", alpha=0.1, size=1.5) +\n    annotate(geom=\"rect\", xmin= 0.5, xmax=1.5, ymin=6.5, ymax=10.5, colour=\"darkred\", alpha=0.1, size=1.5) +\n    annotate(geom=\"rect\", xmin= 1.5, xmax=2.5, ymin=0.5, ymax=6.5, colour=\"darkred\", alpha=0.1, size=1.5) +\n    annotate(geom=\"rect\", xmin= 1.5, xmax=2.5, ymin=6.5, ymax=9.5, colour=\"darkblue\", alpha=0.1, size=1.5) +\n    annotate(geom=\"segment\", x=3, xend=3, y=1, yend=8, arrow=arrow(), colour=\"darkblue\", size=2) +\n    annotate(geom=\"rect\", xmin= 3.5, xmax=4.5, ymin=0.5, ymax=7.5, colour=\"darkred\", alpha=0.1, size=1.5) \n\n\nprint(p_hm)\n\n\n\n\nThis looks much better.\nWe should also look at the residual plots again\n\np1 &lt;- ggplot(data=msdata, aes(x=linear_predictor5, y=residuals5, colour=dev_year)) +\n    geom_point(size=2) +\n    scale_colour_viridis(begin=0.9, end=0) +\n    theme_bw() + \n    theme(legend.position = \"none\") +\n    ggtitle(\"Linear predictor\")\n\n\np2 &lt;- ggplot(data=msdata, aes(x=acc_year, y=residuals5)) +\n    geom_point(size=2, colour=\"#2d708eff\") +\n    theme_bw() + \n    ggtitle(\"Accident year\")\n\np3 &lt;- ggplot(data=msdata, aes(x=dev_year, y=residuals5)) +\n    geom_point(size=2, colour=\"#2d708eff\") +\n    theme_bw() + \n    ggtitle(\"Development year\")\n\np4 &lt;- ggplot(data=msdata, aes(x=cal_year, y=residuals5)) +\n    geom_point(size=2, colour=\"#2d708eff\") +\n    theme_bw() + \n    ggtitle(\"Calendar year\")\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\nThese residuals do look better than those from the chain ladder model.\n\n\n7.4.3 Loss reserve\nNow that we have a model, let’s produce the estimate of the outstanding claims by accident year and in total.\n\nTake the lower triangle data [futdata] created above\nAdd on the new variates we created\nScore the model on this data\nSummarise the results\n\nCreate the data and score using predict(). Store the predicted values in the incremental column.\n\n# add all model variates\nfutdata[, acc_year_2 := acc_year^2\n        ][, dev_year_m1 := dev_year - 1\n          ][, dev_year_ge_7 := pmax(0, dev_year - 7.5)\n            ][, dev_year_eq_1 := as.integer(dev_year == 1)\n              ][, dev_year_eq_2 := as.integer(dev_year == 2)\n                ][, dev_year_eq_3 := as.integer(dev_year == 3)\n                  ][, dev_year_eq_4 := as.integer(dev_year == 4)\n                    ][, acc_year_1_6 := as.integer(acc_year&gt;=1 & acc_year &lt;=6)]\n\n\nx &lt;- predict(glm_fit5, newdata = futdata, type=\"response\")\nfutdata[, incremental := x]\n\nhead(futdata)\n\n   acc_year dev_year cal_year acc_year_factor dev_year_factor incremental\n1:        2       10       12               2              10    3618.769\n2:        3        9       12               3               9    4470.907\n3:        3       10       13               3              10    4059.635\n4:        4        8       12               4               8    5324.841\n5:        4        9       13               4               9    4835.016\n6:        4       10       14               4              10    4390.250\n   acc_year_2 dev_year_m1 dev_year_ge_7 dev_year_eq_1 dev_year_eq_2\n1:          4           9           2.5             0             0\n2:          9           8           1.5             0             0\n3:          9           9           2.5             0             0\n4:         16           7           0.5             0             0\n5:         16           8           1.5             0             0\n6:         16           9           2.5             0             0\n   dev_year_eq_3 dev_year_eq_4 acc_year_1_6\n1:             0             0            1\n2:             0             0            1\n3:             0             0            1\n4:             0             0            1\n5:             0             0            1\n6:             0             0            1\n\n\nGet reserves by accident year and in total\n\nocl_year &lt;- futdata[,  lapply(.SD, sum), .SDcols=c(\"incremental\"), by=\"acc_year\"]\nocl_total &lt;- ocl_year[, sum(incremental)]\n\ndatatable(ocl_year) |&gt; \n  formatRound(\"incremental\", digits = 0)\n\n\n\n\n\n\nThe total reserve is 370,493."
  },
  {
    "objectID": "Foundations/05_a_glms_r.html#reviewing-this-example",
    "href": "Foundations/05_a_glms_r.html#reviewing-this-example",
    "title": "7  Reserving with GLMs in R",
    "section": "7.5 Reviewing this example",
    "text": "7.5 Reviewing this example\nLooking back over this example, what we have done is started with a chain ladder model and then shown how we can use a GLM to fit a more parsimonious model (i.e. fewer parameters). It may then be possible to reconcile the shape of the parametric fit by accident year to underlying experience in the book - here we saw higher payments in the middle accident years. Is this due to higher claims experience or higher premium volumes? Does this give us an insight that allows us to better extrapolate into the future when setting reserves?\nWe have also used model diagnostics to identify areas of misfit and then used GLM interactions to capture these changes."
  },
  {
    "objectID": "Foundations/05_a_glms_r.html#practical-use-of-glms-in-traditional-reserving",
    "href": "Foundations/05_a_glms_r.html#practical-use-of-glms-in-traditional-reserving",
    "title": "7  Reserving with GLMs in R",
    "section": "7.6 Practical use of GLMs in traditional reserving",
    "text": "7.6 Practical use of GLMs in traditional reserving\n\n7.6.1 Modelling\nThe ideas in this simple example extend to more complex traditional scenarios. By traditional I mean that the data you have available to you are classified by accident (or underwriting), development and calendar periods only.\nFirst decide what you are going to model. Here we had a single model of incremental payments. However you could fit a Payments Per Claim Finalised (PPCF) model which consists of 3 submodels - numbers of claims incurred by accident period, number of claims finalised by (accident, development period) and payments per claim finalised by (accident, development period). Each of these could then be fitted by a GLM.\nFor whatever you’re modelling, you then pick the two triangle directions that you think are most critical for that experience. You can’t include all 3 at the start since they are correlated.\nSo, for PPCF submodels:\n\nfor number of claims incurred models, accident and development period effects are likely to be where you start.\nnumbers of claims finalised will usually depend on development period (type of claim) and calendar period (to take account of changes in claim settlement processes)\nfor claim size models, you will probably want development and calendar period effects. For these models you could use operational time instead of development period to avoid changes in the timing of claims finalisations impacting your model.\n\nThen fit the models by starting with the modelled effects as factors and use methods such as those outlined above to reduce the number of parameters by using parametric shapes. Look for missing structure and consider adding interactions or (carefully) adding limited functions of the third triangular direction. Take advantage of GLM tools to refine your model. Use what you know about the portfolio to inform your model - if you know that there was a period of rapid claims inflation, then include that in your model.\n\n\n7.6.2 Setting reserves\nIt is possible to overlay judgement onto a GLM’s predictions. At the end of the day, the predictions are just based on a mathematical formula. So, taking claims inflation as an example, if you’ve been seeing 5% p.a. over the last 3 years, but you think this is going to moderate going forward, then you can adjust the projections by removing the 5% p.a. rate into the future and replacing it with, say, 2% p.a. Once you get familiar with using GLMs, you might find it easier to incorporate judgement - the GLM can capture more granularity about past experience which in turn may make it easier to work out how things might change in future and how to numerically include these changes.\n\n\n7.6.3 Additional References\nThe references below have further examples of fitting GLMs in this way, and show how to capture quite complex experience. Although both use individual data, the methodology can be used in a similar manner for aggregate data.\n\nLoss Reserving with GLMs: A Case Study\nIndividual Claim modelling of CTP data\nPredictive modeling applications in actuarial science, Frees and Derig, 2004 - in particular see Chapter 18 in Volume 1 and Chapter 3 in Volume 2.\n\nPlease feel free to add references to other useful material in the comments."
  },
  {
    "objectID": "Foundations/06_lasso.html#introduction-to-the-lasso",
    "href": "Foundations/06_lasso.html#introduction-to-the-lasso",
    "title": "8  Self-assembling claim reserving models using the LASSO",
    "section": "8.1 Introduction to the LASSO",
    "text": "8.1 Introduction to the LASSO\nThe LASSO (Least Absolute Shrinkage and Selection Operator) is a form of regularised regression. This means that the regression is subject to a penalty for the addition of each additional term in the predictor. The purpose of this is to prevent over-fitting of the model to data.\n\n8.1.1 From GLMs …\nThe loss function associated with a GLM regression is the deviance, defined as\n\\[D(y; X, \\hat{\\beta}) = -\\sum_{i=1}^N \\ell (y_i; X, \\hat{\\beta})  + \\text{other terms}\\]\nwhere:\n\n\\(y\\) is the N-vector of observations \\(y_i\\)\n\\(X\\) is the regression design matrix\n\\(\\beta\\) is the p-vector of coefficients \\(\\beta_j\\) in the linear predictor\n\\(\\hat{\\beta}\\) is the regression estimate of \\(\\beta\\)\n\\(\\ell (y_i; X, \\hat{\\beta})\\) is the log-likelihood of observation \\(y_i\\)\nthe other terms are of no great interest here.\n\nParameter estimation of a GLM can be performed by minimisation of the deviance with respect to \\(\\hat{\\beta}\\), which is equivalent to maximisation of the likelihood.\n\n\n8.1.2 … to the LASSO\nThe loss function for the LASSO version of this GLM is:\n\\[\\mathcal{L} (y; X, \\hat{\\beta}) = D(y; X, \\hat{\\beta}) + \\lambda ||\\hat{\\beta}||_1\\]\nwhere:\n\n\\(\\lambda ||\\hat{\\beta}||_1\\) is the parameter penalty or regularisation\n\\(||\\hat{\\beta}||_1 = \\sum_{j=1}^p | \\hat{\\beta}_j|\\) is the \\(L_1\\) norm of \\(\\hat{\\beta}\\) (which is the absolute value)\n\\(\\lambda \\geq 0\\) is a tuning parameter controlling the size of the penalty and therefore the amount of the regularisation.\n\nLASSO parameter estimation is effected by minimisation of \\(\\mathcal{L} (y; X, \\hat{\\beta})\\) with respect to \\(\\hat{\\beta}\\). The quantity \\(\\lambda ||\\hat{\\beta}||_1\\) adds a penalty for each non-zero coefficient.\n\n\n8.1.3 Model selection\nThe non-differentiability of the penalty function tends to cause the occurrence of corner solutions in the minimisation, so that some coefficients \\(\\hat{\\beta}_j\\) are forced to zero.\nThe strength of this effect increases with \\(\\lambda\\). In the limit \\(\\lambda=0\\), the LASSO is the same as the GLM; in the limit \\(\\lambda \\rightarrow \\infty\\), the LASSO model becomes null, i.e. \\(\\hat{\\beta} = 0\\) (other than the intercept if this has not been regularised). Thus, as \\(\\lambda\\) increases from 0 to \\(\\infty\\), the LASSO model shifts from the relatively complex GLM to increasingly simpler representations of the data.\nIn practical use, models are fitted for a number of different possible values of \\(\\lambda\\) (a path of values, from high to low). Cross-validation may then be used to select a particular \\(\\lambda\\), and therefore model - one popular choice is the model which minimises the selected error measure over the different folds. Another popular alternative is to select the model with an average error across the folds that is one standard deviation away from the minimum. In this case, it is always the simpler model (i.e. higher \\(\\lambda\\) value) that is selected with the aim of further reducing the risk of overfitting.\nThere is a Bayesian version of the LASSO (further details in the paper, and references therein), in which it is viewed as a hierarchical model, consisting of the original GLM with randomised parameters, each subject to the Laplace prior density:\n\\[ \\pi(\\beta_j) = \\frac{1}{2} \\lambda \\exp( -\\lambda | \\beta_j |), \\space\\space\\space\\space -\\infty &lt; \\beta_j &lt; \\infty .\\]\nThe LASSO is dealt with in more detail in Hastie T, Tibshirani R and Friedman J. (2009). The Elements of Statistical Learning: Data Mining, Inference and Prediction. Springer, New York USA."
  },
  {
    "objectID": "Foundations/06_lasso.html#using-the-lasso-for-reserving",
    "href": "Foundations/06_lasso.html#using-the-lasso-for-reserving",
    "title": "8  Self-assembling claim reserving models using the LASSO",
    "section": "8.2 Using the LASSO for reserving",
    "text": "8.2 Using the LASSO for reserving\nIn GIRO 2018 (and presented again at TIGI 2019), we, along with our co-author Hugh Miller, describe how the LASSO could be used to carry out reserving for a traditional triangle. The motivation behind this work was to develop an automatic method to construct a general insurance claims reserving model for data sets with complex features where simple approaches such as the chain ladder fail. In the past we have advocated the use of GLM models for such data sets, but the construction of a GLM is a time-consuming process, even for a skilled analyst. Our aim was to develop a procedure that produced a model similar to a GLM, but using machine learning techniques.\nWe used the LASSO rather than other types of regularised regression, for the following reasons:\n\nThe ability of the LASSO to select parameters (by setting some parameter estimates to zero)\nThe LASSO leads to lower error variance in the fitted values (at the cost of some additional bias).\n\nThis approach has performed well - as may be seen from the figures towards the end of this article, which show that the fitted curves can track the underlying specification quite closely, even in the presence of significant noise, or difficult to detect interactions. A paper describing the method is available on SSRN. This contains much more technical detail around the choice, use and implementation of the LASSO for the reserving problem.\nFor the rest of this post, we will work through this method in a tutorial example. This is based on an existing worked example so if you’ve seen that one, then you may already be familiar with a lot of the content below."
  },
  {
    "objectID": "Foundations/06_lasso.html#tutorial-on-how-to-use-our-lasso-technique-for-reserving",
    "href": "Foundations/06_lasso.html#tutorial-on-how-to-use-our-lasso-technique-for-reserving",
    "title": "8  Self-assembling claim reserving models using the LASSO",
    "section": "8.3 Tutorial on how to use our LASSO technique for reserving",
    "text": "8.3 Tutorial on how to use our LASSO technique for reserving\nIn our paper we investigate the use of the LASSO using four synthetic (i.e. simulated) data sets as well as a real data set. This worked example will use the third simulated data set. The specifications for this data set are given in Section 4.2.1 of the paper. The main points to note are that this data set:\n\nis a 40x40 triangle, with\naccident and development period effects\ncalendar period effects (i.e. superimposed inflation)\na step-interaction between accident and development period for accident periods greater than 16 and development periods greater than 20 (note this affects only 10 cells of the triangle)\n\nThese effects are represented graphically below."
  },
  {
    "objectID": "Foundations/06_lasso.html#initial-setup",
    "href": "Foundations/06_lasso.html#initial-setup",
    "title": "8  Self-assembling claim reserving models using the LASSO",
    "section": "8.4 Initial Setup",
    "text": "8.4 Initial Setup\nFirst we must open R and load any libraries required. These include:\n\ndata.table for manipulating data\nglmnet to fit the LASSO model\nggplot2 and patchwork for graphing\n\nIf you would prefer all data manipulation in base R, then refer to the original version of this tutorial.\n\nlibrary(data.table)\nlibrary(glmnet)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(DT)\n\n\n# other setup\noptions(\"scipen\"=99) # turn off scientific notation\n\n# colour palette for some plots\n# IFoA colours\n# primary colours\ndblue &lt;- \"#113458\"\nmblue &lt;- \"#4096b8\"\ngold &lt;- \"#d9ab16\"\nlgrey &lt;- \"#dcddd9\"\ndgrey &lt;- \"#3f4548\"\nblack &lt;- \"#3F4548\"\n#secondary colours\nred &lt;- \"#d01e45\"\npurple &lt;- \"#8f4693\"\norange &lt;- \"#ee741d\"\nfuscia &lt;- \"#e9458c\"\nviolet &lt;- \"#8076cf\""
  },
  {
    "objectID": "Foundations/06_lasso.html#generating-the-synthetic-data-set",
    "href": "Foundations/06_lasso.html#generating-the-synthetic-data-set",
    "title": "8  Self-assembling claim reserving models using the LASSO",
    "section": "8.5 Generating the synthetic data set",
    "text": "8.5 Generating the synthetic data set\nThe functions defined below will generate data sets similar to those used in our paper. Here we’ll use simulated data set 3, but we’ve included the code to generate the other data sets so that you can experiment with others if you wish.\nThe utility function LinearSpline() will be widely used in this example - it takes a vector var and produces a spline piece between start and stop - flat (and 0) up to start, thereafter increasing to stop and then levelling out at this point. This is used both in the data generation and in the generation of basis functions for the LASSO.\n\nLinearSpline &lt;- function(var, start, stop){\n    pmin(stop - start, pmax(0, var - start))\n}\n\n\nCreateSyntheticData&lt;-function(whichsim, numperiods)\n{\n\n    # whichsim determins which data set to simulate\n    # numperiods determines the size of the triangle\n      \n    # create the acc/dev/cal parameters\n    kk &lt;- rep(1:numperiods, each = numperiods) #AQ\n    jj &lt;- rep(1:numperiods, times= numperiods) #DQ\n    tt &lt;- kk+jj-1 # PQ\n\n    # set alpha/beta/gamma - hard-code up the sim values\n    if (whichsim == 1){\n        alpha &lt;- log(100000)+0.1*LinearSpline(kk,1,15)+0.2*LinearSpline(kk,15,20) - 0.05*LinearSpline(kk,30,40) \n        beta  &lt;- (16/3 - 1)*log(jj)- (1/3)*jj\n        gamma &lt;- 0\n        mu &lt;- exp( alpha + beta + gamma)  \n    }\n    else if (whichsim == 2){\n        alpha &lt;- log(100000)+0.1*LinearSpline(kk,1,15)+0.2*LinearSpline(kk,15,20) - 0.05*LinearSpline(kk,30,40) \n        beta  &lt;- (16/3 - 1)*log(jj)- (1/3)*jj  # a is 16/3, b is 1/3 \n        gamma &lt;- gammafunc(tt)\n        mu &lt;- exp( alpha + beta + gamma)  \n    }\n    else if (whichsim == 3){\n        alpha &lt;- log(100000)+0.1*LinearSpline(kk,1,15)+0.2*LinearSpline(kk,15,20) - 0.05*LinearSpline(kk,30,40) \n        beta  &lt;- (16/3 - 1)*log(jj)- (1/3)*jj  # a is 16/3, b is 1/3 \n        gamma &lt;- gammafunc(tt)\n        mu &lt;- exp( alpha + beta + gamma + 0.3*beta*ifelse(kk&gt;16 & jj&gt;20,1,0))  \n    }\n    else if (whichsim == 4){\n        alpha &lt;- log(100000)+0.1*LinearSpline(kk,1,15)+0.2*LinearSpline(kk,15,20) - 0.05*LinearSpline(kk,30,40) \n        beta  &lt;- (16/3 - 1)*log(jj)- (1/3)*jj  # a is 16/3, b is 1/3 \n        gamma &lt;- gammafunc(tt)\n        mu &lt;- exp( alpha + beta + gamma*((numperiods-1)-LinearSpline(jj,1,numperiods))/(numperiods-1) )  # need to check\n    }\n    \n    varbase &lt;- (0.3 * mu[  kk==1 & jj ==16] )^2 # can scale variance up and down here\n    CC  &lt;-  varbase / mu[  kk==1 & jj ==16]\n    \n    vars   &lt;- CC*mu\n    tausq  &lt;- log (vars / (mu^2) + 1)\n    \n    pmts &lt;- exp( rnorm( numperiods^2, mean = log(mu)-0.5*tausq , sd = sqrt(tausq)  ) )\n    \n    # indicator for past/future = traint/test\n    train_ind&lt;-(tt&lt;=numperiods)\n    \n    ### data.table for output\n    full&lt;-data.table(pmts, acc=as.integer(kk), dev=as.integer(jj), cal=as.integer(tt), mu, train_ind )\n    full\n}\n\n\n#---------------------------\n# function to generate calendar period effects used in CreateSyntheticData()\n# written as a seperate function for convenience\n\ngammafunc &lt;- function(t){\n    gg &lt;- \n        ifelse( t&lt;=12, gg &lt;- 0.0075*LinearSpline(t,1,12),\n                ifelse(t&lt;=24,  gg &lt;- 0.0075*LinearSpline(12,1,12) + 0.001* (t-12)*(t-11)/2,\n                       ifelse(t&lt;=32, gg &lt;- 0.0075*LinearSpline(12,1,12) + 0.001* (24-12)*(24-11)/2,\n                           ifelse(t&lt;=40, gg &lt;- 0.0075*LinearSpline(12,1,12) + 0.001* (24-12)*(24-11)/2 + 0.002*(t-32)*(t-31)/2,\n                               0.0075*LinearSpline(12,1,12) + 0.001* (24-12)*(24-11)/2 + 0.002*(40-32)*(40-31)/2\n                           ))))\n    gg  \n}\n\nThe code below generates the data set as specified in our paper. If you want to use our data set, use a seed of 130. Otherwise, use different seeds to produce different data sets. Or change use_data_set to 1, 2 or 4 to use data sets simulated in a similar way to those in our paper.\n\nuse_data_set &lt;- 3       # which data set to use\nuse_data_set_seed &lt;- 130  # seed to generate data\nnum_periods &lt;- 40   # size of data set\n\nset.seed(use_data_set_seed)\n\ndat &lt;- CreateSyntheticData(whichsim=use_data_set, numperiods=num_periods)\n\nLet’s have a look at the data:\n\nacc / dev / cal = accident / development / calendar quarter\npmts = simulated payments\nmu = underlying mean\ntrain_ind: TRUE for past values and FALSE for future.\n\n\n# head(dat,10) gives the same information, the rest is just formatting\n\nhead(dat, 10)  |&gt; \n  datatable() |&gt; \n  formatRound(c(\"pmts\", \"mu\"), digits = 0)\n\n\n\n\n\n\nIt may also be helpful to visualise the data, using shading to indicate the size of the payments (specifically, log(payments)).\n\n# get limits for use with raster plots\ndata_raster_limits &lt;- c(floor(dat[, min(log(pmts))]), ceiling(dat[, max(log(pmts))]))\n\nggplot(dat, aes(dev, acc)) +\n  geom_raster(aes(fill = log(pmts)))+\n  geom_line(aes(x=num_periods+1-acc, y=acc), colour=dgrey, size=2)+\n  scale_y_reverse()+\n  scale_fill_viridis_c(begin=1, end=0, limits=data_raster_limits)+\n  theme_classic()+\n  labs(x=\"Development quarter\", y=\"Accident quarter\", title=\"Log(payments)\")+\n  NULL\n\n\n\n\n\n8.5.1 Specifying the LASSO model\nThe LASSO model requires data (which we have), basis functions or regressors and model settings for the LASSO procedure. In our example we have 3 regressors - accident, development and calendar periods. If we use these as is, then all our model can potentially contain are a linear effect related to each of these. That won’t come anywhere close to modelling the data. Therefore, a key part of our paper was how to expand these 3 regressors into a flexible set of basis functions, capable of capturing a variety of shapes in the model experience.\n\n8.5.1.1 Set of basis functions\nIn the paper we proposed creating a basis function set consisting of ramp functions and step (or heaviside) functions, as shown in the plot below:\n\nRamps start flat at 0, then have a knot (turning point) after which they increase linearly\nStep or heaviside functions start at 0, then step up to 1 and a specific point and remain there.\n\n\n\n\n\n\nMore specifically we use the ramp functions for main effects and the heaviside functions for interactions.\nSince the procedure is intended to be automatic, the default basis function set includes all possible main effects ramps and all possible two-way interaction step functions.\nFor the particular data set here, this means that our basis functions include:\n\n3 x 39 ramp functions, e.g. \\[\\max(\\text{acc} - k, 0), \\;\\; k=1, 2, ..., 39\\]\n\nand similar for dev and cal.\n\n3 x 39 x 39 interactions \\[I(\\text{acc} \\geq k)*I(\\text{acc} \\geq j), \\;\\; k, j = 2,3,..., 40\\]\n\nsimilar for (acc, cal) and (dev, cal)\n\nThis leads to 4680(!) basis functions.\n\nSince there are really only 2 regressors (as in, once you know two of accident, development and calendar, you know the third), there are a lot of correlated variables. For real problems (as in the real data example in the paper), where you have prior knowledge about what effects are likely to be in the data we would recommend more discernment in the selection of which of the effects to include. For example, if you expect only development and calendar effects, then do not include the accident period basis functions.\n\n\n8.5.1.2 Scaling\nReferring back to the introduction of this article, the loss function to be minimised for the LASSO is\n\\[\\mathcal{L} (y; X, \\hat{\\beta}) = D(y; X, \\hat{\\beta}) + \\lambda ||\\hat{\\beta}||_1\\]\nWe noted that \\(\\lambda\\) controlled the size of the penalty attaching to each parameter, and therefore each basis function. However, as the size of the parameters also features in this equation, it is important that the basis functions be on similar scales. Otherwise whether a parameter is included or not could depend on whether the corresponding basis function has large values (and therefore smaller \\(\\hat{\\beta}\\)) or small values (and therefore larger \\(\\hat{\\beta}\\)), which is clearly undesirable.\nOnce you’re aware of this issue, it should come as no surprise that scaling parameters is something that is common in machine learning work like this one. Indeed, the glmnet package which we will be using comes with an in-built scaling tool. However this scaling will scale each variable independently. This isn’t what we want since the basis functions are not independent - we have over 4000 of these, but really we only have 3 fundamental regressors so we probably want all acc functions to be scaled similarly (and the same is true for dev and cal).\nOur paper has some more discussion around scaling. In a nutshell, we calculate a scaling factor for each of the 3 fundamental regressors and then apply that to all basis functions generated from that regressor. The scaling we use is \\(\\rho\\) scaling which is calculated as\n\\[\\sum_{i=1}^n {\\frac{(x_i-\\bar{x})^2}{n}}\\]\nwhere\n\n\\(\\bar{x}\\) is the mean of the regressor \\(x\\) and\n\\(n\\) is the number of elements in the regressor.\n\nAnother nuance to note is that in the real world, the future data will not be available. So our fundamental regressors contain only past data and the scaling factors should therefore be calculated using past data only.\nIn other implementations of the LASSO, it is often common to centre the variables as well. We don’t do that here as we want to preserve interactions having a zero level.\n\n\n8.5.1.3 Code\nNow we’ll go through the code used to generate the set of basis functions.\nWe first write a function to do the \\(\\rho\\)-scaling. The input is a vector and the output is a scaling factor. Having this as a function makes sense because we need to call it three times (once for each of acc, dev and cal).\n\n# function to calculate scaling factors for the basis functions\n# scaling is discussed in the paper\nGetScaling &lt;- function(vec) {\n  fn &lt;- length(vec)\n  fm &lt;- mean(vec)\n  fc &lt;- vec - fm\n  rho_factor &lt;- ((sum(fc^2))/fn)^0.5\n}\n\nWe next write some code to calculate the main effects ramp functions. This is also a function, again because we need to run it three times.\nInputs to this function are the fundamental regressor and its name and scaling and the number of periods, which in turn tell the function what ramps to generate (you might think that is not needed because we can calculate it from the input vector, but it is important if we are working with vectors that contain future time periods - vectors of future cal will contain calendar periods not present in the past data and we don’t want to generate ramp functions for these values).\nThe output is a matrix with named columns - e.g. L_1_999_acc, L_2_999_acc, …, L_39_999_acc where each column is the scaled ramp vector.\n\n# function to create the ramps for a particular primary vector\nGetRamps &lt;- function(vec, vecname, np, scaling){\n  \n  # vec = fundamental regressor\n  # vecname = name of regressor\n  # np = number of periods\n  # scaling = scaling factor to use\n  \n  # pre-allocate the matrix to hold the results for speed/efficiency\n  n &lt;- length(vec)\n  nramps &lt;- (np-1)\n  \n  mat &lt;- matrix(data=NA, nrow=n, ncol=nramps)\n  cnames &lt;- vector(mode=\"character\", length=nramps)\n  \n\n  col_indx &lt;- 0\n\n  for (i in 1:(np-1)){\n    col_indx &lt;- col_indx + 1\n\n    mat[, col_indx] &lt;- LinearSpline(vec, i, 999) / scaling\n    cnames[col_indx] &lt;- paste0(\"L_\", i, \"_999_\", vecname)\n  }\n  \n  colnames(mat) &lt;- cnames\n  \n  return(mat)\n}\n\nWe also need to calculate the interactions which the function does below for a given pair of input vectors, e.g. accident and development periods. The inputs are similar to those for the GetRamps() function, except that we have two sets, one for each input fundamental regressor. The output is also a column-named matrix where names are, e.g., I_acc_ge_5xI_dev_ge_14.\n\n# create the step (heaviside) function interactions\nGetInts &lt;- function(vec1, vec2, vecname1, vecname2, np, scaling1, scaling2) {\n\n  # vec1 = fundamental regressor 1\n  # vec2 = fundamental regressor 2\n  # vecname1 = name of regressor 1\n  # vecname2 = name of regressor 2\n  # np = number of periods\n  # scaling1 = scaling factor to use for regressor 1\n  # scaling2 = scaling factor to use for regressor 2\n    \n      \n  # pre-allocate the matrix to hold the results for speed/efficiency\n  n &lt;- length(vec1)\n  nints &lt;- (np-1)*(np-1)\n  \n  mat &lt;- matrix(data=NA_real_, nrow=n, ncol=nints)\n  cnames &lt;- vector(mode=\"character\", length=nints)\n  \n\n  col_indx &lt;- 0\n\n  for (i in 2:np){\n    \n    ivec &lt;- LinearSpline(vec1, i-1, i) / scaling1\n    iname &lt;- paste0(\"I_\", vecname1, \"_ge_\", i)\n    \n    if (length(ivec[is.na(ivec)]&gt;0)) print(paste(\"NAs in ivec for\", i))\n    \n    for (j in 2:np){\n      col_indx &lt;- col_indx + 1  \n      mat[, col_indx] &lt;- ivec * LinearSpline(vec2, j-1, j) / scaling2\n      cnames[col_indx] &lt;- paste0(iname, \"xI_\", vecname2, \"_ge_\", j)\n      \n      jvec &lt;- LinearSpline(vec2, j-1, j) / scaling2\n      if (length(jvec[is.na(jvec)]&gt;0)) print(paste(\"NAs in jvec for\", j))\n\n    }\n  }\n  \n  colnames(mat) &lt;- cnames\n  \n  return(mat)\n\n  \n}\n\nFinally we put all these functions together and run the code to:\n\ncalculate the scaling factors\ngenerate the ramp functions\ngenerate the interactions\nput everything together into a single matrix, varset\n\nWe create varset as a matrix and not a data.frame or a data.table since this is the form required by glmnet.\n\n\nNote that we calculate the basis functions for all the data, both past and future. However we only use past data when calculating the scaling factors - dat[train_ind == TRUE,] is the data.table way to select the past rows in the data set.\n\n# get the scaling values\nrho_factor_list &lt;- vector(mode=\"list\", length=3)\nnames(rho_factor_list) &lt;- c(\"acc\", \"dev\", \"cal\")\n\nfor (v in c(\"acc\", \"dev\", \"cal\")){\n  # NB: only calculating scaling using past data    \n  rho_factor_list[[v]] &lt;- GetScaling(dat[train_ind == TRUE, get(v)])\n}\n\n\n# main effects - matrix of values of Ramp functions\n\nmain_effects_acc &lt;- GetRamps(vec = dat[, acc], vecname = \"acc\", np = num_periods, scaling = rho_factor_list[[\"acc\"]])\nmain_effects_dev &lt;- GetRamps(vec = dat[, dev], vecname = \"dev\", np = num_periods, scaling = rho_factor_list[[\"dev\"]])\nmain_effects_cal &lt;- GetRamps(vec = dat[, cal], vecname = \"cal\", np = num_periods, scaling = rho_factor_list[[\"cal\"]])\n                             \nmain_effects &lt;- cbind(main_effects_acc, main_effects_dev, main_effects_cal)\n\n\n# interaction effects\nint_effects &lt;- cbind(\n    GetInts(vec1=dat[, acc], vecname1=\"acc\", scaling1=rho_factor_list[[\"acc\"]], np=num_periods, \n            vec2=dat[, dev], vecname2=\"dev\", scaling2=rho_factor_list[[\"dev\"]]),\n\n    GetInts(vec1=dat[, dev], vecname1=\"dev\", scaling1=rho_factor_list[[\"dev\"]], np=num_periods, \n            vec2=dat[, cal], vecname2=\"cal\", scaling2=rho_factor_list[[\"cal\"]]),\n    \n    GetInts(vec1=dat[, acc], vecname1=\"acc\", scaling1=rho_factor_list[[\"acc\"]], np=num_periods, \n            vec2=dat[, cal], vecname2=\"cal\", scaling2=rho_factor_list[[\"cal\"]])\n)\n\n\nvarset &lt;- cbind(main_effects, int_effects)\n\nHave a look at a subset of the data\n\nvarset[1:10, 39:44]\n\n      L_39_999_acc L_1_999_dev L_2_999_dev L_3_999_dev L_4_999_dev L_5_999_dev\n [1,]            0   0.0000000   0.0000000   0.0000000   0.0000000   0.0000000\n [2,]            0   0.1048285   0.0000000   0.0000000   0.0000000   0.0000000\n [3,]            0   0.2096570   0.1048285   0.0000000   0.0000000   0.0000000\n [4,]            0   0.3144855   0.2096570   0.1048285   0.0000000   0.0000000\n [5,]            0   0.4193139   0.3144855   0.2096570   0.1048285   0.0000000\n [6,]            0   0.5241424   0.4193139   0.3144855   0.2096570   0.1048285\n [7,]            0   0.6289709   0.5241424   0.4193139   0.3144855   0.2096570\n [8,]            0   0.7337994   0.6289709   0.5241424   0.4193139   0.3144855\n [9,]            0   0.8386279   0.7337994   0.6289709   0.5241424   0.4193139\n[10,]            0   0.9434564   0.8386279   0.7337994   0.6289709   0.5241424\n\n\nWe can also look at the scaling factors used\n\nrho_factor_list\n\n$acc\n[1] 9.539392\n\n$dev\n[1] 9.539392\n\n$cal\n[1] 9.539392\n\n\nThe factors are all equal in this case. In hindsight this makes sense - we have the complete 40 x 40 past triangle, and all regressors are numbered from 1, so all three regressors are permutations of the same numbers.\n\nSome of the interactions will be constant over the past data set - i.e. those that involve combinations of parameters that are only possible in the future such as I_acc_ge35xI_dev_ge30. We don’t have to remove these but we can do if we want.\nThis next block of code does this.\n\n# drop any constant columns over the training data set\n# do this by identifying the constant columns and dropping them\n\n# get the past data subset only using TRUE/FALSE from train_ind in dat\nvarset_train &lt;- varset[dat$train_ind, ]\n\n# identify constant columns as those with max=min\nrm_cols &lt;- varset_train[, apply(varset_train, MARGIN=2, function(x) max(x, na.rm = TRUE) == min(x, na.rm = TRUE))]\n\n# drop these constant columns\nvarset &lt;- varset[, !(colnames(varset) %in% colnames(rm_cols))]\n\n\n\n\n\n8.5.2 Fitting a LASSO\nThe glmnet package has two functions for fitting a LASSO: glmnet() and cv.glmnet().\n\nglmnet() fits a regularised regression model for various values of the penalty parameter, \\(\\lambda\\).\ncv.glmnet() adds a cross-validation (“CV”) layer on top of this so helps us to select the particular value of \\(\\lambda\\) to use - \\(\\lambda\\) values that have lower cross-validation error values are preferred.\n\nWe’ll use the latter function here.\nIn addition to the data and basis functions, cv.glmnet() has a number of parameters. These include:\n\nfamily - the response type to use. We’ll use the Poisson distribution here, but note that recent versions of the glmnet package have widened the options available for this parameter. At the time we did this work, the Poisson (with a log link) was the most suitable option.\nnlambda and lambda.min.ratio - control the vector of lambda values calculated by cv.glmnet()\npmax - the maximum number of variables ever to be non-zero in a model\ndfmax - maximum number of variables in a model\nalpha - set this to 1 for the LASSO\nthresh - convergence parameter. Default is 1E-7\nmaxit - maximum number of interations\nnfolds - number of cross-validation folds\nstandardize - whether to standardise the x variables (we set this to FALSE since we have done our own standardisation).\n\nThere’s also a parallel argument which will use parallel processing to speed things up. This is worth using if you have a large data set and access to a multi-core machine.\nBelow we fit the LASSO via cv.glmnet() with our selections for the parameters above.\n\nmy_pmax &lt;- num_periods^2   # max number of variables ever to be nonzero\nmy_dfmax &lt;- num_periods*10  #max number of vars in the model\n\ntime1 &lt;- Sys.time()\ncv_fit &lt;- cv.glmnet(x = varset[dat$train_ind,], \n                  y = dat[train_ind==TRUE, pmts], \n                  family = \"poisson\", \n                  nlambda = 200,  # default is 100 but often not enough for this type of problem\n                  nfolds = 8,\n                  thresh = 1e-08, # default is 1e-7 but we found we needed to reduce it from time to time\n                  lambda.min.ratio = 0, # allow very small values in lambda\n                  dfmax = my_dfmax, \n                  pmax = my_pmax, \n                  alpha = 1, \n                  standardize = FALSE, \n                  maxit = 200000)  # convergence can be slow so increase max number of iterations\nprint(\"time taken for cross validation fit: \")\n## [1] \"time taken for cross validation fit: \"\nSys.time() - time1\n## Time difference of 43.85213 secs\n\ncv.glmnet objects have a plot method associated with them which plots the CV fit:\n\nplot(cv_fit)\n\n\n\n\nEach point shows the average error across all folds, the bars show the standard deviation. The dashed lines represent the minimum CV error model (smaller \\(\\lambda\\), called lambda.min) and one a standard deviation away (lambda.1se). These are selections commonly used by modellers. The numbers across the upper y-axis are the number of non-zero parameters in the model for that value of \\(\\lambda\\).\nOccasionally, the minimum CV value is the final point in the lambda sequence. In this case you should rerun cv.glmnet() with a longer lambda vector - one easy way to do this is to extract the lambda vector from the cv.glmnet object, and then extend it by adding values at the end, e.g. using a decay factor. Then rerun the cross-validation fit with this longer vector (specify its use via the lambda function argument) and, all going well, the minimum CV error lambda will now be a true minimum, rather than the last evaluated point. The original article does something like this, if you would like to see some more details around this.\n\n\n8.5.3 Analysing the model\n\n8.5.3.1 Model coefficients\nWe used the model corresponding to the minimum CV error in the paper (lambda.min in the cv.glmnet results object, cv_fit). First let’s look at the coefficients in this model.\n\n# all coefficients, including those that are 0\ncoefs_min &lt;- predict(cv_fit, type = \"coefficients\", s = cv_fit$lambda.min)  # NB this is a data.frame not a vector\ncoefnames &lt;- c(\"Intercept\", colnames(varset))  # don't forget the intercept  # coeff names - this is a vector\n\n# get indicators for non-zero ones\nind_nz_min&lt;-which(!(coefs_min == 0))\n\n# make a data.table for easy viewing\nnzcoefs_min &lt;- data.table(Parameter = coefnames[ind_nz_min], Coefficient = coefs_min[ind_nz_min,])\n\n# print the table\ndatatable(nzcoefs_min) |&gt; \n  formatRound(\"Coefficient\", digits = 4)\n\n\n\n\n\n\nNote the interactions at the end. Are these detecting the interaction that we know is in this simulated data set? We will find out below.\n\n\n8.5.3.2 Tracking plots\nWe plot the model results in a number of different ways in the paper. Below we replicate some of these plots.\nFirst, we add the fitted values to the data.table.\n\ndat[, fitted := as.vector(predict(cv_fit, \n                                  newx = varset, \n                                  s = cv_fit$lambda.min, \n                                  type=\"response\"))]\n\nThe function below produces the tracking graphs shown in the paper - plot values for all levels of one fundamental predictor holding a second predictor at a fixed value. To help with interpreting the results, it also shades the past part of the data in grey.\n\nGraphModelVals&lt;-function(dat, primary_predictor, secondary_predictor, secondary_predictor_val, \n                         xaxis_label, yaxis_label, var_names, log_values = TRUE, \n                         include_st=FALSE, include_legend=FALSE, font_size=6){\n\n  # dat = input data. Must be in wide format\n  # primary_predictor = plot values for this predictor\n  # secondary_predictor = hold this predictor fixed\n  # secondary_predictor_val = value to hold secondary_predictor fixed at\n  # xaxis_label = label for x axis\n  # yaxis_label = label for y axis\n  # var_names = names of actual / fitted variables in the input data to plot.     \n  #    var_names must be list with names like this: list(actual=\"pmts\", mean=\"mu\", fitted=\"fitted\")\n  # log_values = plot log(values) - default = TRUE\n  # include_st = include a subtitle in the plot saying the grey rectangle is past data, default=FALSE    \n  # include_legend = include the legend in the plot, default=FALSE    \n  # font_size = size of font, default = 6\n  # (these last 3 variables + default values might seem odd - this function is used in a later article\n  #    and the defaults are sensible in that context)    \n\n  \n  # extract data we want to use\n  use_dat &lt;- dat[get(secondary_predictor) == secondary_predictor_val, ]\n  \n  # turn into long format (tidy format in this case) using melt.data.table since that works better with ggplot\n  dat_long &lt;- melt(dat[get(secondary_predictor) == secondary_predictor_val, ],\n                   measure.vars = unlist(var_names),\n                   id.vars = primary_predictor)\n  \n  # make the names nicer - colnames and labels\n  setnames(dat_long, primary_predictor, \"predictor\")\n  \n  dat_long[variable == var_names$actual, variable := \"Simulated\"\n           ][variable == var_names$mean, variable := \"Underlying\"\n             ][variable == var_names$fitted, variable := \"Fitted\"]\n  \n  # get the levels of the variables right so that they are plotted in the right order\n  dat_long[, variable := factor(variable, levels=c(\"Fitted\", \"Simulated\", \"Underlying\"))]\n  \n  \n  if (log_values) dat_long[, value := log(value)]\n  \n  # figure out past data rectangle coordinates\n  xmin1 &lt;- use_dat[train_ind == TRUE, min(get(primary_predictor))]\n  xmax1 &lt;- use_dat[train_ind == TRUE, max(get(primary_predictor))]\n  \n  ymin1 &lt;- dat_long[, min(value)]*0.95\n  ymax1 &lt;- dat_long[, max(value)]*1.05\n  \n  \n  # draw the tracking plots\n  g &lt;- ggplot(data=dat_long, aes(x=predictor, y=value, group=variable))+\n    geom_line(aes(linetype=variable, colour=variable, size=variable, alpha=variable))+\n    geom_line(aes(linetype=variable, colour=variable))+\n    scale_colour_manual(name=\"\", values=c(red, dgrey, dgrey))+\n    scale_linetype_manual(name=\"\", values=c(\"solid\", \"solid\", \"dotted\"))+\n    scale_size_manual(name=\"\", values=c(2,1,1))+\n    scale_alpha_manual(name=\"\", values=c(0.8, 0.5, 0.5))+\n    theme_classic()+\n    annotate(geom=\"rect\", xmin=xmin1, xmax=xmax1, ymin=ymin1, ymax=ymax1, alpha=0.1)+\n    labs(x=xaxis_label, y=yaxis_label, title=paste(xaxis_label, \"tracking for\", secondary_predictor, \"=\", secondary_predictor_val)) +\n    theme(axis.title = element_text(size = font_size), axis.text = element_text(size = font_size-1))\n         \n  if(include_st==TRUE) g &lt;- g + labs(subtitle=\"Past data in grey rectangle\") + theme(plot.subtitle = element_text (size = font_size))\n\n  g &lt;- if(include_legend==TRUE) g + theme(legend.position=\"bottom\") else g + theme(legend.position = \"none\")\n  \n  \n\n  # return the results  \n  invisible(list(data=dat_long, graph=g))\n}\n\nNow let’s look at development quarter when accident quarter is 20. Remember the step-interaction starts at dev=21 - which we see in the graph.\n\ndev_graph_list &lt;- GraphModelVals(dat, \n                                 primary_predictor = \"dev\", \n                                 secondary_predictor = \"acc\", \n                                 secondary_predictor_val = 20, \n                                 xaxis_label = \"Development quarter\",\n                                 yaxis_label = \"Log(Payments)\",\n                                 var_names = list(actual=\"pmts\", mean=\"mu\", fitted=\"fitted\"),\n                                 include_st = TRUE,\n                                 include_legend = TRUE,\n                                 font_size = 10)\n\ndev_graph_list$graph\n\n\n\n\nSimilarly we can look at accident quarter tracking when development quarter is 24 and again see the interaction.\n\nacc_graph_list &lt;- GraphModelVals(dat, \n                                 primary_predictor = \"acc\", \n                                 secondary_predictor = \"dev\", \n                                 secondary_predictor_val = 24, \n                                 xaxis_label = \"Accident quarter\",\n                                 yaxis_label = \"Log(Payments)\",\n                                 var_names = list(actual=\"pmts\", mean=\"mu\", fitted=\"fitted\"),\n                                 include_st = TRUE,\n                                 include_legend = TRUE,\n                                 font_size = 10)\nacc_graph_list$graph\n\n\n\n\nYou can explore other combinations of primary_predictor, secondary_predictor and secondary_predictor_val to see how the models track the past and future experience in other parts of the triangle\n\n\n8.5.3.3 Heat map\nPlotting colour-coded actual/fitted values can be a useful way of looking at the model fit. The function below plots these values (subject to lower and upper bounds of 25% and 400%) and shades them with blue values being less than 100% and red values being greater than 100%. The darker the shading, the further from 100%.\nThe function below will draw the heatmap.\n\n# heat maps\n\nGraphHeatMap &lt;- function(dat, x=\"dev\", y=\"acc\", actual, fitted, lims=c(0.25, 4),\n                         xlab=\"Development quarter\", ylab=\"Accident Quarter\"){\n  \n  # copy data to avoid modifying original\n  localdat &lt;- copy(dat)\n  \n  # get fails if there is a variable with the same name so make local copies\n  local_x &lt;- x\n  local_y &lt;- y\n  local_actual &lt;- actual\n  local_fitted &lt;- fitted\n  \n  # make restricted Avs F for heatmap and set up past/future split line\n  np &lt;- max(localdat[[y]])\n  \n  localdat[, .avsf := get(local_actual) / get(local_fitted)\n           ][, .avsf_restrict_log := log(pmax(min(lims), pmin(max(lims), .avsf)))\n             ][, .past_line := np + 1 - get(local_y)]\n  \n  \n  g &lt;- ggplot(data=localdat, aes_string(x=local_x, y=local_y)) +\n    geom_tile(aes(fill = .avsf_restrict_log))+scale_y_reverse()+\n    theme_classic()+\n    scale_fill_gradient2(name=\"AvF_min\", low=mblue, mid=\"white\", high=red, midpoint=0, \n                         space=\"Lab\", na.value=lgrey, guide=\"colourbar\")+\n    labs(x=xlab, y=ylab)+\n    geom_line(aes_string(x=\".past_line\", y=local_y), colour=dgrey, size=2)+\n    theme(strip.text = element_text(size=8,colour=dgrey), \n          strip.background = element_rect(colour=\"white\", fill=\"white\"))+\n    theme(axis.title.x = element_text(size=10), axis.text.x  = element_text(size=10))+\n    theme(axis.title.y = element_text(size=10), axis.text.y  = element_text(size=10))+\n    theme(element_line(size=0.25, colour=dgrey))+\n    theme(legend.position=\"none\", )+  \n    NULL    \n  \n\n  invisible(list(data=localdat, graph=g))\n  \n\n}\n\n\ng &lt;- GraphHeatMap(dat, x=\"dev\", y=\"acc\", actual=\"pmts\", fitted=\"fitted\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\ng$graph\n\n\n\n\nYou should, of course, carry out a full model validation exercise on any model prior to use. These plots may be helpful tools in this.\n\n\n\n8.5.4 Claims reserves\nFinally, let’s have a look at the claim reserve estimates and compare them to those from an 8-period chain ladder.\nThe code below uses data.table functionality to sum up the reserves by accident period and overall.\n\nos_acc &lt;- dat[train_ind == FALSE, \n              .(LASSO = sum(fitted), simulated = sum(pmts), underlying = sum(mu)), \n              keyby=.(acc)]\n\nos &lt;- os_acc[, .(LASSO = sum(LASSO), \n                 simulated = sum(simulated), \n                 underlying = sum(underlying))]\n\n\n8.5.4.1 Chainladder reserves\nIn the paper we compared some of our results to the 8-period Chainladder reserve. This is calculated below.\n\n# get cumulative payments to make it easier to calculate CL factors\ndat[, cumpmts := cumsum(pmts), by=.(acc)][train_ind==FALSE, cumpmts := NA]\n\n# 8-period average\ncl_fac &lt;- numeric(num_periods-1) # hold CL factors\n\nfor (j in 1:num_periods-1){\n  \n  cl_fac[j] &lt;- \n      dat[train_ind==TRUE & dev == (j+1) & acc &gt; (num_periods-8-j) & acc &lt;= (num_periods-j), sum(cumpmts)] /\n      dat[train_ind==TRUE & dev == (j) & acc &gt; (num_periods-8-j) & acc &lt;= (num_periods-j), sum(cumpmts)]\n}\n\n# accumulate the CL factors\ncl_cum &lt;- cumprod(rev(cl_fac))\n\n# leading diagonal for projection\nleading_diagonal &lt;- dat[train_ind==TRUE & cal == num_periods & acc &gt; 1, cumpmts]\n\n# CL amounts now\ncl_os &lt;- cl_cum * leading_diagonal - leading_diagonal\n\nWe will attach these results to the data.tables holding the results from the LASSO.\n\nos_acc[, Chainladder := cl_os]\n\nos[, Chainladder := sum(cl_os)]\n\n\n\n8.5.4.2 Results by accident period\nHere’s a plot of the results (similar to that in the paper). Note that the y-axis is restricted for readability so that the actual chain ladder value (488 Bn) for accident period 40 does not display on the graph.\n\n# make a long [tidy format] version of the data for use with ggplot2\nos_acc_long &lt;- melt(os_acc, id.vars = \"acc\")\n\n# divide by Bn to make numbers more readable\nos_acc_long[, value := value/1e9]\n\nos_plot &lt;-\n    ggplot(data=os_acc_long, aes(x=acc, y=value, colour=variable, \n                                 linetype=variable, size=variable, alpha=variable))+\n    geom_line()+\n    scale_linetype_manual(name=\"\", values=c(\"solid\", \"dashed\", \"dotted\", \"solid\" ))+\n    scale_colour_manual(name=\"\", values=c(red, dgrey, dgrey, mblue ))+\n    scale_size_manual(name=\"\", values=c(2, 1, 1, 1.5))+\n    scale_alpha_manual(name=\"\", values=c(0.8, 0.5, 0.5, 0.8))+\n    coord_cartesian(ylim=c(0, 40))+\n    theme_classic()+\n    theme(legend.position=\"bottom\", legend.title=element_blank())+\n    labs(x=\"Accident quarter\", y=\"Amount (B)\")+\n    annotate(geom=\"text\", x=37, y=40, label=\"488B-&gt;\")+\n    ggtitle(\"Outstanding amounts\")\n    \nos_plot\n\n\n\n\nYou can see from the graph that - despite the presence of an interaction affecting only a small number of cells, the LASSO model detects and responds appropriately to this change. By contrast, the chainladder model does not perform so well.\nThe table below displays these results also.\n\ncols &lt;- setdiff(names(os_acc), \"acc\")\n\nos_acc[, lapply(.SD, function(x) x/1e9)][, acc := acc*1e9]  |&gt; \n  datatable() |&gt; \n  formatRound(cols, digits = 1)\n\n\n\n\n\n\n\n\n8.5.4.3 Total reserves\nThe overall reserve values (in units of B) are\n\nos[, lapply(.SD, function(x) x/1e9)] |&gt; \n  datatable() |&gt; \n  formatRound(cols, digits = 1)"
  },
  {
    "objectID": "Foundations/06_lasso.html#commentary",
    "href": "Foundations/06_lasso.html#commentary",
    "title": "8  Self-assembling claim reserving models using the LASSO",
    "section": "8.6 Commentary",
    "text": "8.6 Commentary\nIn practice, no actuary would fit the 8-period chainladder (or any other estimating period) without significant review and likely modification based on actuarial judgement. So in an example like this, the gains from a method like the LASSO will not be as large as suggested by the results above. Furthermore, the large step change in this data set would likely result from something like a legislative change that the actuary should know about and be able to estimate its effect and then incorporate this into any projection.\nNonetheless, there is benefit from the raw or naive model projection being closer to what we might want to project and in that sense, the LASSO model clearly outperforms the chainladder."
  },
  {
    "objectID": "Foundations/06_lasso.html#conclusion",
    "href": "Foundations/06_lasso.html#conclusion",
    "title": "8  Self-assembling claim reserving models using the LASSO",
    "section": "8.7 Conclusion",
    "text": "8.7 Conclusion\nThe aim of this article has been to demonstrate the fitting of a claims reserving model using a LASSO approach in R and to produce some of the model diagnostic and results output that a user might wish to examine. If you would like to experiment some more, you could try modifying the synthetic data set code to produce other types of simulated data (such as those in our paper), or try it out on a real data example."
  },
  {
    "objectID": "Foundations/07_mlr3example.html#introduction",
    "href": "Foundations/07_mlr3example.html#introduction",
    "title": "9  ML modelling on triangles - a worked example",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nIn this chapter we look at applying different machine learning (ML) models to a data set. Our goal is to illustrate a work flow for:\n\nsetting up a data set for ML\napplying different ML models to this data\ntuning the ML hyper-parameters\ncomparing and contrasting performance for the past fitted values and the future predictions.\n\nA secondary goal is to demonstrate the utility of a multi-model machine learning framework which enables the user to easily “plug and play” different machine learning models within the same framework. We use the mlr3 set of packages in R here - this and other similar packages in R (e.g. caret and tidymodels) and tools in scikit-learn in python may be useful in creating a smoother work flow.\nA notebook version of this article is also available which may be helpful if you want to experiment with this code.\n\n9.1.1 Who is this article aimed at?\nThis article is aimed at those who know a little about some of the standard machine learning models, but who may not have done much hands-on work with an analysis. It will also be useful for those who have done some experimentation, but maybe not on a reserving data set. Finally, it may also be of interest to R users who have never used a multi-model framework before and who would like to see how the mlr3 ecosystem works."
  },
  {
    "objectID": "Foundations/07_mlr3example.html#pre-requisites-to-this-article",
    "href": "Foundations/07_mlr3example.html#pre-requisites-to-this-article",
    "title": "9  ML modelling on triangles - a worked example",
    "section": "9.2 Pre-requisites to this article",
    "text": "9.2 Pre-requisites to this article\nWe’ve tried to make this article accessible to people new to ML, and to make this article stand-alone. Having some knowledge about basic machine learning techniques like decision trees, random forests and XGBoost (gradient boosting) will help. Furthermore, we also fit the Chainladder model as a GLM (sometimes referred to as a Stochastic Chainladder) and fit a particular type of LASSO model. We’ve included limited details on these models here - our previous chapters have more details on these:\n\nReserving with GLMs - see Chapter 7\nSelf-assembling claim reserving models using the LASSO - see Chapter 8\n\n\n9.2.1 The data set\nOur example deals with using ML to set reserves for a single aggregate 40x40 triangle. We’ve selected the data set because:\n\nIt’s small, so the code will run relatively quickly on your machine - there’s nothing worse than running through a worked example and having to wait hours for results!\nA lot of reserving is still done using traditional accident (or underwriting) + development aggregated triangles so it is relevant to the real world.\n\nWe use a simulated data set. There are several reasons for this:\n\nWe know the future results so can examine how different reserve predictions perform.\nWe can control how the future experience emerges. In particular, we can ensure there are no future systemic changes (e.g. from legislation or court precedent) that would impact future payments. Changes like this can make it difficult when examining performance of reserve prediction methods on real data - it can be hard to separate poor performance of the model from a good model where the future experience departs markedly from that used to build the model.\nWe can share the data set (and the code to generate it) with you.\n\nThe data set used is simulated data set 3 from the paper Self-assembling insurance claim models using regularized regression and machine learning. This is a 40x40 triangle of incremental quarterly payments over 10 years. Variables on the data set are accident, development and calendar quarters only. A copy of the data set is available here.\n\n\n9.2.2 Machine learning and aggregate triangles\nTypically, ML and big data go hand-in-hand. By big data we mean lots of observations and lots of features (covariates / independent variables / regressors). Aggregate triangles are small in both senses - small numbers of observations and limited features (often just the 3 time features of accident/underwriting period, development period and calendar period).\nThis is not to say that it is invalid to use machine learning for aggregate triangles - many people have demonstrated good results from machine learning for such data - there are many papers on this topic. But it’s also true that an experienced actuary using a Chainladder model (or other traditional) method, overlaid with judgement, will often get a similar answer to the best ML method, even for a complex triangle. However ML may be a more efficient way of getting to an answer. In the worked example below, we use a data set which deviates significantly from Chainladder assumptions. Therefore the unadjusted Chainladder projection is quite poor and would need a significant investment of time to yield a better model. In contrast, the better performing ML models we looked at have a better first estimate, so may not require as much intervention to produce a final estimate, thereby leading to a time-saving.\n\n\n9.2.3 Don’t rank the ML models used based on this example\nThe purpose of this article is to demonstrate a workflow for fitting ML models in R. While we’ve done some work to improve model fitting, there are a lot more things we would consider if we were looking for the best models (e.g. feature engineering, train/test splits, performance measures). So, although we do look at the relative performance of the different models at the end, you should not make any conclusions about the relative performance of the different ML methods based on this work.\nYou may find that by adjusting some hyper-parameters, or by revising the training/test data set split, you’re able to find better models. We’ll be discussing this point a bit further at the end of the article.\nA priori, given the form of the data (simulated using a regression style structure) and that the LASSO model we used is based on previous work which did aim to establish a framework to fit good models to triangular data using the LASSO, we expected that the LASSO model would perform the best and (spoiler alert!) it did.\n\n\n9.2.4 mlr3 package\nOne problem with applying many different types of ML models is that they all generally have different input specifications for the data, and different output types. This can make it a bit tedious to try several models.\nThere are a number of ML aggregator or multi-model packages that do a lot of the data manipulation behind the scenes. These packages take in data in a specified form and allow users to call the different ML methods. They abstract away any data manipulation required to put the data into the form required by the method. The benefit of these is apparent - set up the data in the aggregator and then switch models in and out at will.\nOne of our previous chapters covers multi-model packages.\nSome ML aggregators in R include\n\ncaret\ntidymodels - a successor to caret.\nmlr3 is the successor to the popular but no longer actively developed mlr.\n\nThe Python package scikit-learn also provides a lot of useful machine learning tools.\nIf you’re looking to use an aggregator package then it’s probably best to read up on a few of them, then select the one that best works for you. We picked mlr3 first because one of us has used mlr in the past and second we are part of the Machine Learning in Reserving working party, so we had to give mlr3 a try!\nmlr3 uses the R6 object orientated classes - R6 is similar to object orientated programming in other languages such as Python or C++, but is quite different to R’s S3 and S4 object orientated approaches. So, depending on your programming experience, the syntax may take a bit of getting used to.\nAs a rough rule of thumb, when working with R6 classes we:\n\nFirst set up an instance of the class. Note that the class can contain both methods (like functions) and data.\nWe then run methods for the class object. These may update the data in the object.\nWe can view and use the data in the class object. Sometimes we can edit the data directory, othertimes, the data is read only and can only be modified using a method (function) available in the class object.\n\nIf you want to learn more there is plenty of documentation out there:\n\nThe mlr3 book\nCheatsheets\nGallery of examples\n\nWhen using mlr3 code, then you can get inline help in R by using the help() method in a class object. This will bring you to help pages. E.g. lrn(\"regr.rpart\")$help() with get you help on decision tree learners and the learner object in general.\nIn our worked example below, we aim to explain each step clearly, so if you prefer to use another package (or language) or work separately with the individual ML packages, you should be able to translate what we are doing into code that is more familiar to you.\n\n\n9.2.5 ML methods used\nWe’ve looked at the following methods:\n\nDecision trees\nRandom forests\nXGBoost\nLASSO\n\nWe also use a GLM which is set up to replicate the Chainladder estimates (refer to this post for more details).\nAn obvious omission from here are neural networks / deep learning models. We’ve omitted these for a couple of reasons:\n\nWe’re using mlr3 here. While there is a mlr3keras package, the developers state (as of May 2021) that mlr3keras is in very early stages, and currently under development. Functionality is therefore experimental and we do not guarantee correctness, safety or stability.\nThe setup for mlr3keras is more difficult as it requires having an instance of Python running on your system. R’s reticulate package then interfaces between the two. Sometimes this is straightforward (some recent improvements have made this a lot easier), but other times it can be frustrating.\n\nHowever, we are actively looking at using deep learning models and will be revisiting them in the near future."
  },
  {
    "objectID": "Foundations/07_mlr3example.html#outline-of-the-workflow",
    "href": "Foundations/07_mlr3example.html#outline-of-the-workflow",
    "title": "9  ML modelling on triangles - a worked example",
    "section": "9.3 Outline of the workflow",
    "text": "9.3 Outline of the workflow\nThe workflow we have set up consists of the following:\n\nPrepare the data for use in the models, including train and test partitions (past data) and hold-out or validation partitions (future data).\nFit each of the models selected. For each of these we:\n\nSelect hyper-parameters (these control the model-fitting process) for tuning\nTune the hyper-parameters using the train and test partitions and select the set of values that yield the best results\nCalculate predicted values on the holdout (future) data.\n\nRun diagnostics on the predicted values for each model and look at the reserve estimates.\n\nWe’ll now work through each of the stages below."
  },
  {
    "objectID": "Foundations/07_mlr3example.html#setup",
    "href": "Foundations/07_mlr3example.html#setup",
    "title": "9  ML modelling on triangles - a worked example",
    "section": "9.4 Setup",
    "text": "9.4 Setup\nThe first step is to load all the libraries required for this work (and install them first if needed). We’ve included details of our session which includes versions of the packages we used at the end of this article. If you are having problems running our code, check that your packages (the mlr3 ones in particular) are the same as ours.\nNote that mlr3 is actually a collection of a number of different packages. If you want to do a specific task, it’s worth seeing if there is an mlr3 package out there for it. There are a number of core mlr3 packages which are hosted on CRAN. We’ve attached them separately here to make it clear which ones we are using but it is also possible to install and attach the most commonly used ones via the mlr3verse wrapper package.\nSome of the accompanying packages are on github only. Of these, we’re just going to use one here - mlr3extralearners - which contains an extended set of machine learning models (the CRAN package mlr3learners includes the most common ones). Install this using the remotes package - the code is given below in the comments. If you’re using renv to manage your packages, then use that instead.\nYou also need to have the following packages installed - but you do not need to attach them as a library:\n\nglmnet (for the LASSO model)\nrpart (decision tree)\nranger (random forest)\nxgboost\nviridis (colour schemes used in some of the plots).\n\nFinally you need to install mlr3measures but you must not attach it (i.e. use library(mlr3measures)) as this will lead to code errors.\nThe code below checks for these packages and will give you a warning message if you are missing any of them. You can then install them (with the exception of mlr3extralearners) with install.packages() or, for RStudio users, via the Packages tab.\n\nreqd_packages &lt;- c(\"data.table\",\n                   \"DT\",\n                   \"ggplot2\",\n                   \"glmnet\",\n                   \"mlr3verse\",\n                   \"mlr3extralearners\",\n                   \"patchwork\",\n                   \"ranger\",\n                   \"rpart\",\n                   \"rpart.plot\",\n                   \"viridis\",\n                   \"xgboost\")\n\n\nfor (p in reqd_packages){\n  check_p &lt;- requireNamespace(p, quietly = TRUE)\n  if (!check_p) warning(paste(\"Install the package\", p, \"before continuing\\n\"))\n}\n\nmessage(\"Package checking complete\\n\")\n\nPackage checking complete\n\n# To install packages via code:\n# install.packages(&lt;package name&gt;) \n\n# To install mlr3extralearners (which is not on CRAN so must be installed on github)\n# 1. Ensure you have the remotes package installed\n# 2. if note using renv then install using\n#     remotes::install_github(\"mlr-org/mlr3extralearners\")\n#\n# if this fails (e.g. if you are using renv to manage packages then it might), you may need to use\n# remotes::install_github(\"mlr-org/mlr3extralearners\", INSTALL_opts = c(\"--no-multiarch\") )\n# see https://github.com/rstudio/renv/issues/162\n#\n# If using renv, then use this:\n# renv::install(\"mlr-org/mlr3extralearners\")\n\nNow that we’ve checked the required packages are there, we can get started with the example by attaching the libraries we require.\n\n\n# machine learning libraries\nlibrary(mlr3verse)   # base mlr3 package\nlibrary(mlr3extralearners) # needed for glms\n\n# data manipulation\nlibrary(data.table)\n\n# graphing\nlibrary(ggplot2)  # plotting\nlibrary(patchwork) # easy way to combine ggplots\nlibrary(rpart.plot)   # to draw decision trees created by rpart\n\n# making nice tables\nlibrary(DT)\n\n# stop R from showing big numbers in scientific notation\noptions(\"scipen\"=99)  \n\n\n# colours for plots - not all are used\n# We will also use viridis for some plots\ndblue  &lt;- \"#113458\"\nmblue  &lt;- \"#4096b8\"\ngold   &lt;- \"#d9ab16\"\nlgrey  &lt;- \"#dcddd9\"\ndgrey  &lt;- \"#3f4548\"\nblack  &lt;- \"#3F4548\"\nred    &lt;- \"#d01e45\"\npurple &lt;- \"#8f4693\"\norange &lt;- \"#ee741d\"\nfuscia &lt;- \"#e9458c\"\nviolet &lt;- \"#8076cf\""
  },
  {
    "objectID": "Foundations/07_mlr3example.html#data-preparation",
    "href": "Foundations/07_mlr3example.html#data-preparation",
    "title": "9  ML modelling on triangles - a worked example",
    "section": "9.5 Data preparation",
    "text": "9.5 Data preparation\nWe’re using a simulated data triangle with all values (past and future).\n\n\n\n\n\nThe data set we use is the simulated data set 3 from this paper. It is available in CSV form here.\nThis is a 40x40 triangle of payments:\n\nThat vary by accident quarter\nThat vary by development quarter\nHave varying rates of superimposed inflation in payment size by calendar quarter (including no inflation)\nHave a step-up in payment size for accident quarters &gt; 16 and development quarters &gt; 20.\n\nThe first two effects are captured by a Chainladder model but the last two effects depart from Chainladder assumptions.\n\n9.5.1 data.table package\nWe’ll be using the data.table package for manipulating the data. There’s an introduction to data.table on our blog if you are unfamiliar with it. If you see := in the R code, then that’s data.table assignment.\n\n\n9.5.2 Load the data\nFirst, load in the data and have a look at it.\n\ndat &lt;- fread(\"./_lasso_simdata3.csv\")\n\nhead(dat)\n##          pmts acc dev cal          mu train_ind\n## 1:   242671.2   1   1   1    71653.13      TRUE\n## 2:   164001.3   1   2   2  1042775.62      TRUE\n## 3:  3224477.8   1   3   3  4362599.77      TRUE\n## 4:  3682530.8   1   4   4 10955670.09      TRUE\n## 5: 10149368.6   1   5   5 20800545.12      TRUE\n## 6: 28578274.7   1   6   6 33089166.75      TRUE\ntail(dat)\n##         pmts acc dev cal        mu train_ind\n## 1: 125261750  40  35  74 109039367     FALSE\n## 2:  62657370  40  36  75  82853302     FALSE\n## 3:  63467681  40  37  76  62682720     FALSE\n## 4:  26041979  40  38  77  47227843     FALSE\n## 5:  33947274  40  39  78  35444881     FALSE\n## 6:  37258687  40  40  79  26503298     FALSE\n\n# create the num_periods variable - number of acc/dev periods\nnum_periods &lt;- dat[, max(acc)]\n\nAs you can see, the data is not in triangular form as per the diagram above, but is instead in long form where:\n\neach row of the data set consists of one observation\neach observation has the accident(acc), development(dev) and calendar(cal) period associated with it\nthe mu value is the mean value of the distribution from which pmts was simulated - this won’t form part of the analysis below so can be ignored\nthe final variable, train_ind is TRUE for past values and FALSE for future values.\n\nThis long format of data is standard for a lot of modelling and data analysis.\nWe can also show a visualisation of the data. This contains both past and future data (with a diagonal line marking the boundary). The plot uses shading to indicate the size of the payments (specifically, log(payments)). The step-up in claim size (acc &gt; 16 and dev &gt; 20) is quite obvious in this graphic. What is also apparent is this this increase only affects a small part of the past triangle (10 cells out of 820 to be exact), but impacts much of the future lower triangle.\n\n# get limits for use with raster plots\ndata_raster_limits &lt;- c(floor(dat[, min(log(pmts))]), ceiling(dat[, max(log(pmts))]))\n\nggplot(dat, aes(dev, acc)) +\n  geom_raster(aes(fill = log(pmts)))+\n  geom_line(aes(x=num_periods+1-acc, y=acc), colour=\"grey30\", size=2)+\n  scale_y_reverse()+\n  scale_fill_viridis_c(begin=1, end=0, limits=data_raster_limits)+\n  theme_classic()+\n  labs(x=\"Development quarter\", y=\"Accident quarter\", title=\"Log(payments)\")+\n  NULL\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThe step-up in payments can be modelled as an interaction between accident and development quarters. Because it affects only a small number of past values, it may be difficult for some of the modelling approaches to capture this. It’s likely, therefore, that the main differentiator between the performance of different methods for this data set will be how much well they capture this interaction.\nOne thing to note is that the Chainladder will struggle with this data set. Both the calendar period terms and the interaction are not effects that the Chainladder can model - it assumes that only accident and development period main effects (i.e. no interactions) are present. So an actuary using the Chainladder for this data set would need to overlay judgement to obtain a good result.\nWe need to modify this data a little before proceeding further:\n\nWe’ll add factor versions (essentially a categorical version) of acc and dev - these are needed later for fitting the Chainladder\nWe’ll add a unique ID associated with each row\n\n\n# data.table code, := is assignment\n# if you see an underscore under the := in the code below, then this is a formatting thing - ignore the underscore here and elsewhere\n# the code should look like this: dat[, accf := as.factor(acc)]\ndat[, accf := as.factor(acc)]\ndat[, devf := as.factor(dev)]\n\n\n# data.table code, .N = number of rows\ndat[, row_id := 1:.N]\n\ntail(dat)\n\n        pmts acc dev cal        mu train_ind accf devf row_id\n1: 125261750  40  35  74 109039367     FALSE   40   35   1595\n2:  62657370  40  36  75  82853302     FALSE   40   36   1596\n3:  63467681  40  37  76  62682720     FALSE   40   37   1597\n4:  26041979  40  38  77  47227843     FALSE   40   38   1598\n5:  33947274  40  39  78  35444881     FALSE   40   39   1599\n6:  37258687  40  40  79  26503298     FALSE   40   40   1600\n\n\n\n\n9.5.3 Create a mlr3 task\nNow create an mlr3 task - making an object to hold data and set roles for the data. These tasks abstract away (i.e. do it for you) all the hard work in getting the data into the right form for each model (learner in mlr3 parlance) - just specify what the target is and what the features are, and mlr3 will take care of the rest.\nWe need to make a regression task since we will be estimating payments. So we make a new instance of the TaskRegr class via TaskRegr$new. When loading the data in via the backend, we’ve omitted mu since this is not used in the modelling. We’ve also omitted the factor versions of acc and dev.\n\n# note can also use the sugar function tsk()\ntask &lt;- TaskRegr$new(id = \"reserves\",\n                     backend = dat[, .(pmts, train_ind, acc, dev, cal, row_id)],\n                     target = \"pmts\")\n\n# look at the task\ntask\n\n&lt;TaskRegr:reserves&gt; (1600 x 6)\n* Target: pmts\n* Properties: -\n* Features (5):\n  - int (4): acc, cal, dev, row_id\n  - lgl (1): train_ind\n\n\nWe need to ensure everything is correctly assigned:\n\nFuture data must not be used for model training. Currently all parts of the triangle are used because we loaded all the data into the backend.\nThe right features must be used in model training. Right now row_id and train_ind are also included in the features which is incorrect.\n\nFor the first, we’ll use row_roles to ensure that future data is not used for model training. There are two types of row_roles - use and validation. Right now all rows are in use.\n\n# number of entries in use\nprint(length(task$row_roles$use))\n## [1] 1600\n\n# for brevity just print first 10 elements\nprint(task$row_roles$use[1:10])\n##  [1]  1  2  3  4  5  6  7  8  9 10\n\n# number of entries in validation\nprint(length(task$row_roles$validation))\n## [1] 0\n\nMove all future values (train_ind==FALSE) into validation so they are not used in the model.\nWe can’t edit row_roles directly, so use the method (set_row_roles()) to do this. Supply the list of future row_ids in the first argument.\n\ntask$set_row_roles(dat[train_ind==FALSE, row_id], roles=\"holdout\")\n\nCheck that the task has been updated correctly.\n\n# number of entries in use\nprint(length(task$row_roles$use))\n## [1] 820\n\n# for brevity just print the end of the second accident period - row id 80 should be in validate\nprint(task$row_roles$use[70:90])\n##  [1] 70 71 72 73 74 75 76 77 78 79 81 82 83 84 85 86 87 88 89 90 91\n\n# number of entries in holdout\nprint(length(task$row_roles$holdout))\n## [1] 780\nprint(task$row_roles$validation[1:10])\n## NULL\n\nThis looks right (we’ve checked more than this, but have limited what we display in this notebook).\nNow we need to remove row_id and train_ind from the list of features:\n\nwe’ll ignore train_ind (so it won’t have any role)\nwe’ll set row_id to be a name. This could be used in plots to label points if needed.\n\n\n# make row_id a name\ntask$set_col_roles(\"row_id\", roles=\"name\")\n\n# drop train_ind from the feature list\ntask$col_roles$feature &lt;- setdiff(task$feature_names, \"train_ind\")  \n\n# check the task has the correct variables now\ntask\n## &lt;TaskRegr:reserves&gt; (820 x 4)\n## * Target: pmts\n## * Properties: -\n## * Features (3):\n##   - int (3): acc, cal, dev\n\n# check alll the col_roles\ntask$col_roles\n## $feature\n## [1] \"acc\" \"cal\" \"dev\"\n## \n## $target\n## [1] \"pmts\"\n## \n## $name\n## [1] \"row_id\"\n## \n## $order\n## character(0)\n## \n## $stratum\n## character(0)\n## \n## $group\n## character(0)\n## \n## $weight\n## character(0)"
  },
  {
    "objectID": "Foundations/07_mlr3example.html#tuning-process-for-hyper-parameters",
    "href": "Foundations/07_mlr3example.html#tuning-process-for-hyper-parameters",
    "title": "9  ML modelling on triangles - a worked example",
    "section": "9.6 Tuning process for hyper-parameters",
    "text": "9.6 Tuning process for hyper-parameters\nMachine learning models have a number of hyper-parameters that control the model fit. Tweaking the hyper-parameters that control model fitting (e.g. for decision trees, the depth of the tree, or for random forests, the number of trees to average over) can have a significant impact on the quality of the fit. The standard way of doing this is to use a train and test data set where:\n\nThe model is trained (built) on the train data set\nThe performance of the model is evaluated on the test data set\nThis is repeated for a number of different combinations of hyper-parameters, and the best performing one is selected.\n\nEvaluating the models on a separate data set not used in the model fitting helps to control over-fitting. Usually we would then select the hyper-parameters that lead to the best results on the test data set and use these to predict our future values.\nThere are various ways we can select the test and train data sets. For this work we use cross-validation. We’ll give a brief overview of it below. Note that we will be discussing validation options in a separate article.\n\n9.6.1 Cross-validation\nThe simplest implementation of a train and test partition is a random one where each point in the data set is allocated to train and test at random. Typically, the train part might be around 70-80% of the data and the test part the remainder.\nRandom test data set\n\n\n\n\n\nCross-validation repeats this type of split a number of times.\nIn more detail, the steps are:\n\nRandomly partition the data into k equal-sized folds (between 5-10 folds are common choices). These folds are then fixed for the remainder of the algorithm.\n\nThis means that we do a train/test split like the triangle above but repeat this k times.\n\nFit the model using data from k-1 of the folds, using the remaining fold as the test data. Do this k times, so each fold is used as test data once.\nCalculate the performance metrics for each model on the corresponding test fold.\nAverage the performance metrics across all the folds.\n\nA simplified representation of the process is given below.\n5-fold cross validation\n\n\n\n\n\n\n\n\n\nCross-validation provides an estimate of out-of-sample performance even though all the data is used for training and testing. It is often considered more robust due to its use of the full dataset for testing, but can be computationally expensive for larger datasets. Here we only have a small amount of data, so there is an advantage to using the full dataset, whilst the computational cost is manageable.\n\n\n9.6.2 Setting up cross-validation in mlr3\nWe can set up a cross-validation resampler in mlr3 that can then be applied to any model. As always, the book is a useful starting reference point.\nHere we will use 6-fold cross validation.\n\n# create object\ncrossval = rsmp(\"cv\", folds=6)  \n\n\nset.seed(42)  # reproducible results\n\n# populate the folds\n# NB: mlr3 will only use data where role was set to “use”\ncrossval$instantiate(task)   \n\nBelow we show the allocation of points into test and train for each fold - the dark blue points are the test subset in each of the 6 folds.\n\n\n\n\n\n\n\n9.6.3 Performance measure\nAs well as the cross-validation process, we want to set up a performance measure to use. Here we will use RMSE (root mean square error).\n\n# to see all measures use \n# mlr_measures$help()\n# follow the links for a specific one, or you can type\n# ?mlr_measures_regr.rmse to get help for RMSE, and then insert the \n# other names for others, eg ?mlr_measures_regr.rmsle etc\n\nmeasure &lt;- msr(\"regr.rmse\")\n\n\n\n9.6.4 Searching hyper-parameter space\nSearching hyper-parameter space needs to be customised to each model, so we can’t set up a common framework here. However, to control computations, it is useful to set a limit on the number of searches that can be performed in a tuning exercise. mlr3 offers a number of different options for terminating searches - here we are just going to use the simplest one - where only a limited number of evaluations are permitted.\nGiven how we set up the tuning process below we need 25 evaluations for the decision tree and random forest models. We need many more evaluations for the XGBoost model since we tune more parameters. However, this can take a long time to run. In practice, using a more powerful computer, or running things in parallel can help.\nSo we’ve set the number of evaluations to a low number here (25). For the XGBoost model, we used 500 evaluations to tune the hyper-parameters and saved the results for use here.\n\n# setting we used to get the xgboost results below\n#evals_trm = trm(\"evals\", n_evals = 500)\n\n# using a low number so things run quickly\nevals_trm = trm(\"evals\", n_evals = 25)"
  },
  {
    "objectID": "Foundations/07_mlr3example.html#fitting-some-ml-models",
    "href": "Foundations/07_mlr3example.html#fitting-some-ml-models",
    "title": "9  ML modelling on triangles - a worked example",
    "section": "9.7 Fitting some ML models",
    "text": "9.7 Fitting some ML models\nNow it’s time to fit and tune the following ML models using mlr3:\n\nDecision tree\nRandom forest\nXGBoost\n\n\n9.7.1 Decision tree\nA decision tree is unlikely to be a very good model for this data, and tuning often doesn’t help much. Nonetheless, it’s a simple model, so it’s useful to fit it to see what happens.\nTo illustrate the use of mlr3, we’ll first show how to fit a model using the default hyper-parameters. We’ll then move onto the code needed to run hyper-parameter tuning using cross-validation.\n\n\n9.7.2 Fitting a single model\nFirst let’s fit a decision tree using default parameters.\n\n# create a learner, ie a container for a decision tree model\nlrn_rpart_default &lt;- lrn(\"regr.rpart\")\n\n# fit the model on all the past data (ie where row_roles==use)\nlrn_rpart_default$train(task)  \n\n# Visualise the tree\nrpart.plot::rpart.plot(lrn_rpart_default$model, roundint = FALSE)\n\n\n\n\nIf you are unfamiliar with decision tree diagrams, the model specifies that, for example, for accident years greater than 17, with development periods less than 7.5 (ie 7 or under) and less than 4.5, the predicted claim cost is $43m.\nWhile the tree is fairly simple, we do see splits for acc&lt;17 and dev&lt;21, which match the location of the interaction.\nHere’s a list of the parameters - the documentation on rpart.control will have more information.\n\nlrn_rpart_default$param_set\n\n&lt;ParamSet&gt;\n                id    class lower upper nlevels        default value\n 1:             cp ParamDbl     0     1     Inf           0.01      \n 2:     keep_model ParamLgl    NA    NA       2          FALSE      \n 3:     maxcompete ParamInt     0   Inf     Inf              4      \n 4:       maxdepth ParamInt     1    30      30             30      \n 5:   maxsurrogate ParamInt     0   Inf     Inf              5      \n 6:      minbucket ParamInt     1   Inf     Inf &lt;NoDefault[3]&gt;      \n 7:       minsplit ParamInt     1   Inf     Inf             20      \n 8: surrogatestyle ParamInt     0     1       2              0      \n 9:   usesurrogate ParamInt     0     2       3              2      \n10:           xval ParamInt     0   Inf     Inf             10     0\n\n\nLet’s try tuning the cp and minsplit parameters.\n\n9.7.2.1 Tuning the decision tree\nTo set up the tuning we need to specify:\n\nThe ranges of values to search over\nA resampling strategy (already have this - crossvalidation)\nAn evaluation measure (this is RMSE)\nA termination criterion so searches don’t go on for ever (our evals_trm)\nThe search strategy (e.g. grid search, random search, etc - see, e.g., this post )\n\nWe first set ranges of values to consider for these using functionality from the paradox library.\n\ntune_ps_rpart &lt;- ps(\n  cp = p_dbl(lower = 0.001, upper = 0.1),\n  minsplit = p_int(lower = 1, upper = 10)\n)\n\nWe can see what’s going to be searched over if we specify a 5x5 grid\n\n# to see what's searched if we set a grid of 5\nrbindlist(generate_design_grid(tune_ps_rpart, 5)$transpose())\n\n         cp minsplit\n 1: 0.00100        1\n 2: 0.00100        3\n 3: 0.00100        5\n 4: 0.00100        8\n 5: 0.00100       10\n 6: 0.02575        1\n 7: 0.02575        3\n 8: 0.02575        5\n 9: 0.02575        8\n10: 0.02575       10\n11: 0.05050        1\n12: 0.05050        3\n13: 0.05050        5\n14: 0.05050        8\n15: 0.05050       10\n16: 0.07525        1\n17: 0.07525        3\n18: 0.07525        5\n19: 0.07525        8\n20: 0.07525       10\n21: 0.10000        1\n22: 0.10000        3\n23: 0.10000        5\n24: 0.10000        8\n25: 0.10000       10\n         cp minsplit\n\n\nGiven we only have 25 points (in a grid of 5), we can easily evaluate every option so we’ll use a grid search strategy. In practice other search strategies may be preferable - e.g. a random search often gets similar results to a grid search, in a much smaller amount of time. Another possible choice is Bayesian optimisation search.\nSo now we have everything we need to tune our hyper-parameters so we can set this up in mlr3 now.\n\n# create the hyper-parameter for this particular learner/model\ninstance_rpart &lt;- TuningInstanceSingleCrit$new(\n  task = task,\n  learner = lrn(\"regr.rpart\"),\n  resampling = crossval,\n  measure = measure,\n  search_space = tune_ps_rpart,\n  terminator = evals_trm\n)\n\n\n# create a grid-search tuner for a 5-point grid\ntuner &lt;- tnr(\"grid_search\", resolution = 5)\n\nFinally, we run the tuning on the instance_rpart space.\n\n# suppress log output for readability\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\n\ntuner$optimize(instance_rpart) \n\n# restart log output\nlgr::get_logger(\"bbotk\")$set_threshold(\"info\")\nlgr::get_logger(\"mlr3\")$set_threshold(\"info\")\n\nThe parameters in the best fit may be accessed here:\n\ninstance_rpart$result_learner_param_vals\n\n$xval\n[1] 0\n\n$cp\n[1] 0.001\n\n$minsplit\n[1] 3\n\n\nSo we can now fit a final model to all the data using these optimised parameters.\n\nlrn_rpart_tuned &lt;- lrn(\"regr.rpart\")\nlrn_rpart_tuned$param_set$values = instance_rpart$result_learner_param_vals\nlrn_rpart_tuned$train(task)  \n\n# plot the model\nrpart.plot::rpart.plot(lrn_rpart_tuned$model, roundint = FALSE)\n\n\n\n\nThis model is much more complicated than the original and looks overfitted - we’ll see if this is the case when we evaluate the model performance later in the article.\n\n\n\n9.7.3 Random forest fitting\nThe ranger random forest package is implemented in mlr3learners (the regression version is regr.ranger).\nLet’s have a look at the hyper-parameters.\n\nlrn(\"regr.ranger\")$param_set\n\n&lt;ParamSet&gt;\n                              id    class lower upper nlevels        default\n 1:                        alpha ParamDbl  -Inf   Inf     Inf            0.5\n 2:       always.split.variables ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;\n 3:                      holdout ParamLgl    NA    NA       2          FALSE\n 4:                   importance ParamFct    NA    NA       4 &lt;NoDefault[3]&gt;\n 5:                   keep.inbag ParamLgl    NA    NA       2          FALSE\n 6:                    max.depth ParamInt     0   Inf     Inf               \n 7:                min.node.size ParamInt     1   Inf     Inf              5\n 8:                     min.prop ParamDbl  -Inf   Inf     Inf            0.1\n 9:                      minprop ParamDbl  -Inf   Inf     Inf            0.1\n10:                         mtry ParamInt     1   Inf     Inf &lt;NoDefault[3]&gt;\n11:                   mtry.ratio ParamDbl     0     1     Inf &lt;NoDefault[3]&gt;\n12:            num.random.splits ParamInt     1   Inf     Inf              1\n13:                  num.threads ParamInt     1   Inf     Inf              1\n14:                    num.trees ParamInt     1   Inf     Inf            500\n15:                    oob.error ParamLgl    NA    NA       2           TRUE\n16:                     quantreg ParamLgl    NA    NA       2          FALSE\n17:        regularization.factor ParamUty    NA    NA     Inf              1\n18:      regularization.usedepth ParamLgl    NA    NA       2          FALSE\n19:                      replace ParamLgl    NA    NA       2           TRUE\n20:    respect.unordered.factors ParamFct    NA    NA       3         ignore\n21:              sample.fraction ParamDbl     0     1     Inf &lt;NoDefault[3]&gt;\n22:                  save.memory ParamLgl    NA    NA       2          FALSE\n23: scale.permutation.importance ParamLgl    NA    NA       2          FALSE\n24:                    se.method ParamFct    NA    NA       2        infjack\n25:                         seed ParamInt  -Inf   Inf     Inf               \n26:         split.select.weights ParamUty    NA    NA     Inf               \n27:                    splitrule ParamFct    NA    NA       3       variance\n28:                      verbose ParamLgl    NA    NA       2           TRUE\n29:                 write.forest ParamLgl    NA    NA       2           TRUE\n                              id    class lower upper nlevels        default\n       parents value\n 1:  splitrule      \n 2:                 \n 3:                 \n 4:                 \n 5:                 \n 6:                 \n 7:                 \n 8:                 \n 9:  splitrule      \n10:                 \n11:                 \n12:  splitrule      \n13:                1\n14:                 \n15:                 \n16:                 \n17:                 \n18:                 \n19:                 \n20:                 \n21:                 \n22:                 \n23: importance      \n24:                 \n25:                 \n26:                 \n27:                 \n28:                 \n29:                 \n       parents value\n\n\nHere, we’ll try tuning number of trees (num.trees) and the minimum node size (min.node.size). max.depth, mtry and sample.fraction are also often helpful to tune.\nWe’ll follow the same steps as for decision trees - set up a parameter space for searching, combine this with RMSE and cross-validation and then tune using grid search.\n\ntune_ps_ranger &lt;- ps(\n  num.trees = p_int(lower = 100, upper = 900),\n  min.node.size = p_int(lower = 1, upper = 5)\n)\n\n# to see what's searched if we set a grid of 5\n#rbindlist(generate_design_grid(tune_ps_ranger, 5)$transpose())\n\ninstance_ranger &lt;- TuningInstanceSingleCrit$new(\n  task = task,\n  learner = lrn(\"regr.ranger\"),\n  resampling = crossval,\n  measure = measure,\n  search_space = tune_ps_ranger,\n  terminator = evals_trm\n)\n\n# suppress output\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\n#We can reuse the tuner we set up for the decision tree\n#this was tuner &lt;- tnr(\"grid_search\", resolution = 5)\n\ntuner$optimize(instance_ranger) \n\n# turn output back on\nlgr::get_logger(\"bbotk\")$set_threshold(\"info\")\nlgr::get_logger(\"mlr3\")$set_threshold(\"info\")\n\nGet the best fit\n\ninstance_ranger$result_learner_param_vals\n\n$num.threads\n[1] 1\n\n$num.trees\n[1] 300\n\n$min.node.size\n[1] 2\n\n\nNow we fit the model to all the parameters\n\nlrn_ranger_tuned &lt;- lrn(\"regr.ranger\")\nlrn_ranger_tuned$param_set$values = instance_ranger$result_learner_param_vals\n\nlrn_ranger_tuned$train(task)  \nlrn_ranger_tuned$model\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      case.weights = task$weights$weight, num.threads = 1L, num.trees = 300L,      min.node.size = 2L) \n\nType:                             Regression \nNumber of trees:                  300 \nSample size:                      820 \nNumber of independent variables:  3 \nMtry:                             1 \nTarget node size:                 2 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       14255125223769820 \nR squared (OOB):                  0.9224571 \n\n\n\n\n9.7.4 XGBoost\nHere are the XGBoost parameters.\n\n# Can just run\n# lrn(\"regr.xgboost\")$param_set\n\n# the code below just removes some of the columns for display purposes\nas.data.table(lrn(\"regr.xgboost\")$param_set)[, .(id, class, default)]\n\n                             id    class          default\n 1:                       alpha ParamDbl                0\n 2:               approxcontrib ParamLgl            FALSE\n 3:                  base_score ParamDbl              0.5\n 4:                     booster ParamFct           gbtree\n 5:                   callbacks ParamUty        &lt;list[0]&gt;\n 6:           colsample_bylevel ParamDbl                1\n 7:            colsample_bynode ParamDbl                1\n 8:            colsample_bytree ParamDbl                1\n 9: disable_default_eval_metric ParamLgl            FALSE\n10:       early_stopping_rounds ParamInt                 \n11:          early_stopping_set ParamFct             none\n12:                         eta ParamDbl              0.3\n13:                 eval_metric ParamUty             rmse\n14:            feature_selector ParamFct           cyclic\n15:                       feval ParamUty                 \n16:                       gamma ParamDbl                0\n17:                 grow_policy ParamFct        depthwise\n18:     interaction_constraints ParamUty   &lt;NoDefault[3]&gt;\n19:              iterationrange ParamUty   &lt;NoDefault[3]&gt;\n20:                      lambda ParamDbl                1\n21:                 lambda_bias ParamDbl                0\n22:                     max_bin ParamInt              256\n23:              max_delta_step ParamDbl                0\n24:                   max_depth ParamInt                6\n25:                  max_leaves ParamInt                0\n26:                    maximize ParamLgl                 \n27:            min_child_weight ParamDbl                1\n28:                     missing ParamDbl               NA\n29:        monotone_constraints ParamUty                0\n30:              normalize_type ParamFct             tree\n31:                     nrounds ParamInt   &lt;NoDefault[3]&gt;\n32:                     nthread ParamInt                1\n33:                  ntreelimit ParamInt                 \n34:           num_parallel_tree ParamInt                1\n35:                   objective ParamUty reg:squarederror\n36:                    one_drop ParamLgl            FALSE\n37:                outputmargin ParamLgl            FALSE\n38:                 predcontrib ParamLgl            FALSE\n39:                   predictor ParamFct    cpu_predictor\n40:             predinteraction ParamLgl            FALSE\n41:                    predleaf ParamLgl            FALSE\n42:               print_every_n ParamInt                1\n43:                process_type ParamFct          default\n44:                   rate_drop ParamDbl                0\n45:                refresh_leaf ParamLgl             TRUE\n46:                     reshape ParamLgl            FALSE\n47:             sampling_method ParamFct          uniform\n48:                 sample_type ParamFct          uniform\n49:                   save_name ParamUty                 \n50:                 save_period ParamInt                 \n51:            scale_pos_weight ParamDbl                1\n52:          seed_per_iteration ParamLgl            FALSE\n53:                   skip_drop ParamDbl                0\n54:                strict_shape ParamLgl            FALSE\n55:                   subsample ParamDbl                1\n56:                       top_k ParamInt                0\n57:                    training ParamLgl            FALSE\n58:                 tree_method ParamFct             auto\n59:      tweedie_variance_power ParamDbl              1.5\n60:                     updater ParamUty   &lt;NoDefault[3]&gt;\n61:                     verbose ParamInt                1\n62:                   watchlist ParamUty                 \n63:                   xgb_model ParamUty                 \n                             id    class          default\n\n\nWith gradient boosting, it is often helpful to optimise over hyper-parameters, so we will vary some parameters and select the best performing set. For speed reasons we will just consider tweedie_variance_power, eta, max_depth , and nrounds but in practice, we could consider doing more.\nThe steps are the generally same as before for decision trees and random forests. However as we are searching over a greater number of parameters, we’ve switched to a random search to try to achieve better results. Depending on your computer, this step takes a while to run (since our terminator is 500 evaluations) - so it might be a good point to grab a cup of coffee or tea! Alternatively reduce the number of evaluations in eval_trm.\n\ntune_ps_xgboost &lt;- ps(\n  # ensure non-negative values\n  objective = p_fct(\"reg:tweedie\"),\n  tweedie_variance_power = p_dbl(lower = 1.01, upper = 1.99),\n\n  # eta can be up to 1, but usually it is better to use low eta, and tune nrounds for a more fine-grained model\n  eta = p_dbl(lower = 0.01, upper = 0.3),\n  # select value for gamma\n  # tuning can help overfitting but we don't investigate this here\n  gamma = p_dbl(lower = 0, upper = 0),\n  \n  # We know that the problem is not that deep in interactivity so we search a low depth\n  max_depth = p_int(lower = 2, upper = 6),\n  \n  # nrounds to stop overfitting\n  nrounds = p_int(lower = 100, upper = 500)\n)\n\ninstance_xgboost &lt;- TuningInstanceSingleCrit$new(\n  task = task,\n  learner = lrn(\"regr.xgboost\"),\n  resampling = crossval,\n  measure = measure,\n  search_space = tune_ps_xgboost,\n  terminator = evals_trm\n)\n\n# need to make a new tuner with resolution 4\n#tuner &lt;- tnr(\"grid_search\", resolution = 4)\nset.seed(84)  # for random search for reproducibility\ntuner &lt;- tnr(\"random_search\")\n\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\ntime_bef &lt;- Sys.time()\ntuner$optimize(instance_xgboost) \n##      objective tweedie_variance_power       eta gamma max_depth nrounds\n## 1: reg:tweedie               1.555728 0.1107844     0         4     467\n##    learner_param_vals  x_domain regr.rmse\n## 1:          &lt;list[9]&gt; &lt;list[6]&gt;  79119552\n\nlgr::get_logger(\"bbotk\")$set_threshold(\"info\")\nlgr::get_logger(\"mlr3\")$set_threshold(\"info\")\n\n\nSys.time() - time_bef\n## Time difference of 46.43769 secs\n\nGet the best fit:\n\ninstance_xgboost$result_learner_param_vals\n\n$nrounds\n[1] 467\n\n$nthread\n[1] 1\n\n$verbose\n[1] 0\n\n$early_stopping_set\n[1] \"none\"\n\n$objective\n[1] \"reg:tweedie\"\n\n$tweedie_variance_power\n[1] 1.555728\n\n$eta\n[1] 0.1107844\n\n$gamma\n[1] 0\n\n$max_depth\n[1] 4\n\n\nHowever, remember that these results are from 25 evaluations only, and with 4 hyper-parameters, we’d really like to use more evaluations. The model above isn’t a particularly good one\nThe table below shows the results we got from 500 evaluations along with the results we got in earlier development work for this article (the hyper-parameter tuning results are subject to randomness, first due to the specification of the cross-validation folds and second due to the random search). We’ll use these when looking at the model.\n\n\n\n\n\n\n\nWe’ll fit both models here:\n\nlrn_xgboost_tuned - the results found from running the tuning code with 500 evaluations\nlrn_xgboost_prevtuned - the best results we found in previous work\n\n\nlrn_xgboost_prevtuned = lrn(\"regr.xgboost\", objective=\"reg:tweedie\", nrounds=233, \n                            tweedie_variance_power=1.01, eta=0.3, gamma=0, max_depth=3)\nlrn_xgboost_prevtuned$train(task)\n\n\nlrn_xgboost_tuned = lrn(\"regr.xgboost\", objective=\"reg:tweedie\", nrounds=265, \n                            tweedie_variance_power=1.011768, eta=0.2310797, gamma=0, max_depth=3)\nlrn_xgboost_tuned$train(task)\n\nIf, instead, you would like to fit the XGBoost model found after 25 evaluations, you can use the code below to pick up the selected values after tuning. This replaces the model fitted above using the hyper-parameters found after 500 evaluations.\n\nlrn_xgboost_tuned = lrn(\"regr.xgboost\")\nlrn_xgboost_tuned$param_set$values = instance_xgboost$result_learner_param_vals\nlrn_xgboost_tuned$train(task)\n\nWe haven’t run this code so our lrn_xgboost_tuned uses the 500 evaluations results.\n\n\n9.7.5 Consolidate results\nNow we will consolidate results for these models. We will gather together:\n\nRMSE for past and future data\npredictions for each model.\n\nNote that mlr3 has various benchmarking tools which we haven’t used here - partially because we want to focus on the concepts rather than the code and partially because we want to compare the results to a couple of other models not fitted in the same framework. If you’re interested in learning more then the section on benchmarking in the mlr3 book is a good start. Pipelines offer more flexible functionality too.\nFirst, we’ll set up a data.table to hold the model projections for each model and populate these projections for each model\n\n# make a new copy of the data, deleting the row_id, accf and devf columns\nmodel_forecasts &lt;- copy(dat)[, c(\"row_id\", \"accf\", \"devf\") := NULL]\n\n# add the model projections - specify list of learners to use and their names\nlrnrs &lt;- c(lrn_rpart_tuned, lrn_ranger_tuned, lrn_xgboost_tuned, lrn_xgboost_prevtuned)\n\n# the id property is used to name the variables - since we have 2 xgboosts, adjust id where necessary\nlrn_xgboost_prevtuned$id &lt;- \"regr.xgboost_prev\"\n\n\nfor(l in lrnrs){\n  #learner$predict(task, row_ids) is how to get predicted values, returns 3 cols with preds in response\n  # use row_ids to tell it to use entire data set, ie \"use\" and validation rows\n  #model_forecasts[, (nl) := l$predict(task, row_ids=1:nrow(dat))$response]\n  \n  model_forecasts[, l$id := l$predict(task=task, row_ids=1:nrow(dat))$response]\n}\n\ntail(model_forecasts)\n\n        pmts acc dev cal        mu train_ind regr.rpart regr.ranger\n1: 125261750  40  35  74 109039367     FALSE 3276251556  1145325092\n2:  62657370  40  36  75  82853302     FALSE 3276251556  1145309205\n3:  63467681  40  37  76  62682720     FALSE 3276251556  1145296469\n4:  26041979  40  38  77  47227843     FALSE 3276251556  1145284081\n5:  33947274  40  39  78  35444881     FALSE 3276251556  1145273028\n6:  37258687  40  40  79  26503298     FALSE 3276251556  1145296139\n   regr.xgboost regr.xgboost_prev\n1:    437965920         750788160\n2:    280243104         527827456\n3:    808473344        1523731712\n4:    374183648         677887360\n5:    233077136         364650848\n6:    233077136         364650848\n\n\nBefore we do any analysis, let’s fit a couple more models to compare these results against:\n\na Chainladder model\na LASSO model\n\n\n\n9.7.6 Chainladder - the baseline model\nSince this is a traditional triangle, it seems natural to compare any results to the Chainladder result. So here, we will get the predicted values and reserve estimate for the Chain ladder model.\nIt’s important to note that we will just use a volume-all Chainladder (i.e. include all periods in the estimation of the development factors) and will not attempt to impose any judgement over the results. In practice, of course, models like the Chainladder are often subject to additional analysis, reasonableness tests and manual assumption selections, so it’s likely the actual result would be different, perhaps significantly, from that returned here.\nAt the same time, no model, whether it be the Chainladder, or a more sophisticated ML model should be accepted without further testing, so on that basis, comparing Chainladder and ML results without modification is a reasonable thing to do. Better methods should require less human intervention to return a reasonable result.\n\n9.7.6.1 Getting the Chainladder reserve estimates\nThe Chainladder reserve can also be calculated using a GLM with:\n\nAccident and development factors\nThe Poisson or over-dispersed Poisson distribution\nThe log link.\n\nWe’ve used this method here as it’s easy to set up in R but practical work using the Chain ladder may be better done with a package like the R ChainLadder package.\nThe easiest way to do this is to work outside mlr3 and use the glm() function directly - this is because our mlr3 task doesn’t contain accf and devf.\nHere’s the code using glm() and the predict() method to add predicted values into model_forecasts.\n\ncl &lt;- glm(data=dat[train_ind==TRUE, .(pmts, accf, devf)], formula=pmts ~ accf + devf, family=quasipoisson(link=\"log\"))\nmodel_forecasts[, \"regr.glm\" := predict(cl, newdata=dat[, .(pmts, accf, devf)], type=\"response\")]\n\nIt is possible to fit this in mlr3 too - but as we need to create a new task, it’s easier to fit it directly uisng glm() as above. However, we’ve included the mlr3 code below for those who are interested. There’s a good bit more code here since we need to set up a new task and a new learner.\nIt isn’t necessary to run this code (we haven’t used it below) but if you do, the end result will be that it recalculates the regr.glm variable in the model_forecasts data.table.\n\n#-------------------------------------\n# create a task with the factor versions of acc and dev [accf and devf] as the only features\ntask_cl &lt;- TaskRegr$new(id = \"reserves\",\n                     backend = dat[, .(pmts, train_ind, row_id, accf, devf)],\n                     target = \"pmts\")\n\ntask_cl$set_row_roles(dat[train_ind==FALSE, row_id], roles=\"validation\")\n\ntask_cl$set_col_roles(\"row_id\", roles=\"name\")\n\n# drop train_ind from the feature list\ntask_cl$col_roles$feature &lt;- setdiff(task_cl$feature_names, \"train_ind\")  \n\ntask_cl\n\n\n#-------------------------------------\n# fit the GLM - no hyper-parameter tuning since we know exactly the type of model we want to fit\n\nlrn_glm &lt;- lrn(\"regr.glm\", family=\"quasipoisson\", link=\"log\")\nlrn_glm$train(task_cl)\n\nsummary(lrn_glm$model)\n\n\n#-------------------------------------\n# add the predicted values to model_forecasts in the same way as we did for the other models.\n\nmodel_forecasts[, lrn_glm$id := lrn_glm$predict(task=task_cl, row_ids=1:nrow(dat))$response]\n\n\n\n\n9.7.7 LASSO (regularised regression)\nFinally, we will use the LASSO to fit a model. We are going to fit the same model as we did in our previous chapter. We’ll include a bare-bones description of the model here. If you haven’t seen it before, then you may want to just take the model as given and skip ahead to the next section. Once you’ve read this entire article you may then wish to read the post on the LASSO model to understand the specifics of this model.\nThe main things to note about this model are:\n\nFeature engineering is needed to produce continuous functions of accident / development / calendar quarters for the LASSO to fit.\nThe blog post and related paper detail how to do this - essentially we create a large group of basis functions from the primary acc, dev and cal variables that are flexible enough to capture a wide variety of shapes.\nIf we want to use mlr3 to do this, then we need to create a task that contains these basis functions as features.\nCross-validation can be done within the underlying LASSO package (glmnet) rather than via mlr3. This is what we’ve done here.\n\nThere’s a reasonable amount of code to run this model, whether we use mlr3 or not:\n\nWe need to create all the basis functions\nmlr3 only - we need to set up a task\nWe need to train the model using glmnet’s cross-validation function, cv_glmnet().\nWe then need to predict using the values for a particular penalty setting (the regularisation parameter) - following the paper, we use the penalty value that leads to the lowest cross-validation error.\n\nSince the blog post shows how to do this using glmnet directly, here we use some mlr3 code to fit the model to show how it would work in mlr3. As with the Chainladder/GLM example, it’s more efficient code-wise to do the fitting outside of mlr3.\n\n\n9.7.8 Basis functions\nThe first step is to create all the basis functions that we need. The functions below create the ramp and step functions needed (over 4000 of these!). The end result is a data.table of the original payments and all the basis functions. As noted above, all the details are in the blog post and paper.\n\n#---------------------------\n# linear spline function - used in data generation and in spline generation below\nLinearSpline &lt;- function(var, start, stop){\n    pmin(stop - start, pmax(0, var - start))\n}\n\n\n# function to calculate scaling factors for the basis functions\n# scaling is discussed in the paper\nGetScaling &lt;- function(vec) {\n  fn &lt;- length(vec)\n  fm &lt;- mean(vec)\n  fc &lt;- vec - fm\n  rho_factor &lt;- ((sum(fc^2))/fn)^0.5\n}\n\n\n# function to create the ramps for a particular primary vector\nGetRamps &lt;- function(vec, vecname, np, scaling){\n  \n  # vec = fundamental regressor\n  # vecname = name of regressor\n  # np = number of periods\n  # scaling = scaling factor to use\n  \n  # pre-allocate the matrix to hold the results for speed/efficiency\n  n &lt;- length(vec)\n  nramps &lt;- (np-1)\n  \n  mat &lt;- matrix(data=NA, nrow=n, ncol=nramps)\n  cnames &lt;- vector(mode=\"character\", length=nramps)\n  \n\n  col_indx &lt;- 0\n\n  for (i in 1:(np-1)){\n    col_indx &lt;- col_indx + 1\n\n    mat[, col_indx] &lt;- LinearSpline(vec, i, 999) / scaling\n    cnames[col_indx] &lt;- paste0(\"L_\", i, \"_999_\", vecname)\n  }\n  \n  colnames(mat) &lt;- cnames\n  \n  return(mat)\n}\n\n\n# create the step (heaviside) function interactions\nGetInts &lt;- function(vec1, vec2, vecname1, vecname2, np, scaling1, scaling2) {\n  \n  # pre-allocate the matrix to hold the results for speed/efficiency\n  n &lt;- length(vec1)\n  nints &lt;- (np-1)*(np-1)\n  \n  mat &lt;- matrix(data=NA_real_, nrow=n, ncol=nints)\n  cnames &lt;- vector(mode=\"character\", length=nints)\n  \n\n  col_indx &lt;- 0\n\n  for (i in 2:np){\n    \n    ivec &lt;- LinearSpline(vec1, i-1, i) / scaling1\n    iname &lt;- paste0(\"I_\", vecname1, \"_ge_\", i)\n    \n    if (length(ivec[is.na(ivec)]&gt;0)) print(paste(\"NAs in ivec for\", i))\n    \n    for (j in 2:np){\n      col_indx &lt;- col_indx + 1  \n      mat[, col_indx] &lt;- ivec * LinearSpline(vec2, j-1, j) / scaling2\n      cnames[col_indx] &lt;- paste0(iname, \"xI_\", vecname2, \"_ge_\", j)\n      \n      jvec &lt;- LinearSpline(vec2, j-1, j) / scaling2\n      if (length(jvec[is.na(jvec)]&gt;0)) print(paste(\"NAs in jvec for\", j))\n\n    }\n  }\n  \n  colnames(mat) &lt;- cnames\n  \n  return(mat)\n\n  \n}\n\nNow the functions are defined, we’ll create a data.table of basis functions here which we’ll use later when fitting the LASSO. The dat_plus table will hold them.\n\n# get the scaling values\nrho_factor_list &lt;- vector(mode=\"list\", length=3)\nnames(rho_factor_list) &lt;- c(\"acc\", \"dev\", \"cal\")\n\nfor (v in c(\"acc\", \"dev\", \"cal\")){\n  rho_factor_list[[v]] &lt;- GetScaling(dat[train_ind == TRUE, get(v)])\n}\n\n\n# main effects - matrix of values\n\nmain_effects_acc &lt;- GetRamps(vec = dat[, acc], vecname = \"acc\", np = num_periods, scaling = rho_factor_list[[\"acc\"]])\nmain_effects_dev &lt;- GetRamps(vec = dat[, dev], vecname = \"dev\", np = num_periods, scaling = rho_factor_list[[\"dev\"]])\nmain_effects_cal &lt;- GetRamps(vec = dat[, cal], vecname = \"cal\", np = num_periods, scaling = rho_factor_list[[\"cal\"]])\n                             \nmain_effects &lt;- cbind(main_effects_acc, main_effects_dev, main_effects_cal)\n\n\n# interaction effects\nint_effects &lt;- cbind(\n    GetInts(vec1=dat[, acc], vecname1=\"acc\", scaling1=rho_factor_list[[\"acc\"]], np=num_periods, \n            vec2=dat[, dev], vecname2=\"dev\", scaling2=rho_factor_list[[\"dev\"]]),\n\n    GetInts(vec1=dat[, dev], vecname1=\"dev\", scaling1=rho_factor_list[[\"dev\"]], np=num_periods, \n            vec2=dat[, cal], vecname2=\"cal\", scaling2=rho_factor_list[[\"cal\"]]),\n    \n    GetInts(vec1=dat[, acc], vecname1=\"acc\", scaling1=rho_factor_list[[\"acc\"]], np=num_periods, \n            vec2=dat[, cal], vecname2=\"cal\", scaling2=rho_factor_list[[\"cal\"]])\n)\n\n\nvarset &lt;- cbind(main_effects, int_effects)\n\n\n# drop any constant columns over the training data set\n# do this by identifying the constant columns and dropping them\nvarset_train &lt;- varset[dat$train_ind, ]\n\nrm_cols &lt;- varset_train[, apply(varset_train, MARGIN=2, function(x) max(x, na.rm = TRUE) == min(x, na.rm = TRUE))]\nvarset &lt;- varset[, !(colnames(varset) %in% colnames(rm_cols))]\n\n# now add these variables into an extended data object\n# remove anything not used in modelling\ndat_plus &lt;- cbind(dat[, .(pmts, train_ind, row_id)], varset)\n\n\n\n9.7.9 mlr3 setup for LASSO\nFirst we need to set up a task. This differs from the tree-based models task as follows in that the input variables are all the basis functions we created and not the raw accident / development / calendar quarter terms - the dat_plus data.table.\nAlso we don’t need to set up a cross-validation resampling for this task since the glmnet package has in-built cross-validation.\n\ntask_lasso &lt;- TaskRegr$new(id = \"reserves_lasso\", backend = dat_plus, target = \"pmts\")\n\n# sort out row and column roles\ntask_lasso$set_row_roles(dat[train_ind==FALSE, row_id], roles=\"holdout\")\n\n# turn row_id into a name\ntask_lasso$set_col_roles(\"row_id\", roles=\"name\")\n\n# drop train_ind from the feature list\ntask_lasso$col_roles$feature &lt;- setdiff(task_lasso$feature_names, \"train_ind\")  \n\n\n\n9.7.10 Model fitting\nAs noted above, the glmnet package has an inbuilt cross-validation function to fit LASSO models so we’ll use that rather than running cross validation via mlr3. We’ll use a Poisson distribution so the cross-validation will look to minimise the Poisson deviance.\nThe results will not be identical to those in the paper - that used 8-fold cross-validation and, even if the number of folds were the same, the random number seeds will be different, so the composition of the folds would be different.\n\nuse_pmax  &lt;- num_periods^2   # max number of variables ever to be nonzero\nuse_dfmax &lt;- num_periods*10  \n\nset.seed(777)  # for reproducibility\n\n# create the mlr3 learner\nlrn_cv_glmnet &lt;- lrn(\n    \"regr.cv_glmnet\", \n    family = \"poisson\",   \n    nfolds = 6,\n    thresh = 1e-08, \n    lambda.min.ratio = 0,\n    # additional parameter for mlr3 - means that the model used when predicting is the min CV error model rather than default 1se model\n    s=\"lambda.min\",   \n    dfmax = use_dfmax, \n    pmax = use_pmax, \n    alpha = 1, \n    standardize = FALSE, \n    maxit = 200000)    \n\n# train the model\nlrn_cv_glmnet$train(task_lasso)\n\nThe s parameter requires a little more explanation. The regr.cv_glmnet learner (which uses the cv_glmnet() function) fits models for many different possible penalty values (termed the lambda vector). For each of these, the average error is calculated across all folds. These are then plotted (together with error bars in the special plot created when we plot a cv.glmnet results object).\n\n# Check the path includes a minimum value of error\nplot(lrn_cv_glmnet$model)\n\n\n\n\nThe two dotted lines in this plot represent two special penalty (or lambda values):\n\nthe lambda.min value - the penalty value which has the lowest cross-validation error (the left-most dotted line)\nthe lambda.1se value - the penalty value where the cross-validation error is 1 standard error from the minimum value and the model is simpler (the right-most dotted line).\n\nThese names come from the glmnet package - cv.glmnet objects include these values. It’s important to check that a true minimum has been found - if the left-most line (the lambda.min value) is the final value where the penalty is lowest, this suggests that the lambda vector needs to be lengthened and have smaller values than the current minimum. It’s possible to supply a user-defined vector of penalty values in this case.\nWhen working directly with glmnet we can easily access the model corresponding to any parameter value. When using regr.cvglmnet in mlr3, we need to specify which penalty value to use for predictions. By default, the mlr3 predict method uses lambda.1se, so we specify s=\"lambda.min\" to use the lambda.min value as in the LASSO blog post.\nNow we add the predictions to the model_forecasts table.\n\nmodel_forecasts[, lrn_cv_glmnet$id := lrn_cv_glmnet$predict(task_lasso, row_ids = 1:nrow(dat))$response]"
  },
  {
    "objectID": "Foundations/07_mlr3example.html#model-analysis",
    "href": "Foundations/07_mlr3example.html#model-analysis",
    "title": "9  ML modelling on triangles - a worked example",
    "section": "9.8 Model analysis",
    "text": "9.8 Model analysis\nThis is the interesting bit - let’s look at all the models to see how they’ve performed (but remember not to draw too many conclusions about model performance from this example as discussed earlier!)\nBefore we go any further though, we will make a long version of the data set (1 model result per row) as that will make a lot of calculations easier. data.table has a melt method for doing this. Tidyverse users may be more familiar with pivot_longer() which also does the same thing.\n\n# get list of variables to form observations by - this is why we used a common naming structure\nmodel_names &lt;- names(model_forecasts)[ names(model_forecasts) %like% \"regr\"]\n\n# wide -&gt; long\nmodel_forecasts_long &lt;- melt(model_forecasts, \n                             measure.vars = model_names, \n                             id.vars=c(\"acc\", \"dev\", \"cal\", \"pmts\", \"train_ind\"))\n\n# rename columns\nsetnames(model_forecasts_long, c(\"variable\", \"value\"), c(\"model\", \"fitted\"))\n\nhead(model_forecasts_long)\n##    acc dev cal       pmts train_ind      model   fitted\n## 1:   1   1   1   242671.2      TRUE regr.rpart 18315018\n## 2:   1   2   2   164001.3      TRUE regr.rpart 18315018\n## 3:   1   3   3  3224477.8      TRUE regr.rpart 18315018\n## 4:   1   4   4  3682530.8      TRUE regr.rpart 18315018\n## 5:   1   5   5 10149368.6      TRUE regr.rpart 18315018\n## 6:   1   6   6 28578274.7      TRUE regr.rpart 18315018\ntail(model_forecasts_long)\n##    acc dev cal      pmts train_ind          model    fitted\n## 1:  40  35  74 125261750     FALSE regr.cv_glmnet 193852526\n## 2:  40  36  75  62657370     FALSE regr.cv_glmnet 216713450\n## 3:  40  37  76  63467681     FALSE regr.cv_glmnet 276806761\n## 4:  40  38  77  26041979     FALSE regr.cv_glmnet 353563579\n## 5:  40  39  78  33947274     FALSE regr.cv_glmnet 451604591\n## 6:  40  40  79  37258687     FALSE regr.cv_glmnet 576831776\n\nHere’s a reminder on the model names:\n\nregr.glm - Chainladder\nregr.cv_glmnet - LASSO\nregr.rpart - decision tree\nregr.ranger - random forest\nregr.xgboost and regr.xgboost_prev - the two XGBoost models.\n\n\n9.8.1 RMSE\nAs discussed above, we are going to do these calculations manually rather than use the mlr3 benchmark tools because we want to include the Chainladder and LASSO models as well (even though we’ve fitted them here using mlr3, we used separate tasks). In any case the calculations are easy to do in data.table (for those who know that package anyway!) and it can be helpful to see what’s going on..\nFor those who are unfamiliar, we’ve used a feature of data.table called chaining (similar in many ways to using the %&gt;% pipe, or the newly introduced base R pipe, |&gt;). The code does the following:\n\nthe first line calculates (fitted-pmts)^2 for each row of the long table (the squared error contribution)\nthe second line sums up all these contributions grouped by model and by train_ind. It also gets the count of values\nthe third line calculates the RMSE as the square root of these average contributions\n\n\nmodel_rmse &lt;- model_forecasts_long[, se_contrib := (fitted-pmts)^2\n                                   ][, .(se_contrib = sum(se_contrib), num = .N), by=.(model, train_ind)\n                                     ][, rmse := sqrt(se_contrib / num)]\n\nhead(model_rmse)\n\n          model train_ind             se_contrib num       rmse\n1:   regr.rpart      TRUE    3307128991724779520 820   63506568\n2:   regr.rpart     FALSE 2864339162983414890466 780 1916306264\n3:  regr.ranger      TRUE    3625163799701476352 820   66490085\n4:  regr.ranger     FALSE  525256075856879353846 780  820612713\n5: regr.xgboost      TRUE     825437127145255936 820   31727443\n6: regr.xgboost     FALSE  215007724850275385344 780  525024694\n\n\nLet’s have a look at these results separately for past and future data sets with results ranked.\nPast data - train_ind == TRUE\n\nmodel_rmse[train_ind==TRUE, .(model, num, rmse)][order(rmse),] |&gt; \n  datatable() |&gt; \n  formatRound(\"rmse\", digits = 0)\n\n\n\n\n\n\nOn the training data, all the tuned models perform reasonably well, with the XGBoost models having the lowest RMSE, followed by the LASSO model. Unsurprisingly, the Chainladder model (regr.glm) has high RMSE - we know that these data will not be well modelled by a Chain ladder.\nHowever, the key indicator is performance on a hold-out data set, in this case the future data.\nFuture data - train_ind == FALSE\n\nmodel_rmse[train_ind==FALSE, .(model, num, rmse)][order(rmse),] |&gt; \n  datatable() |&gt; \n  formatRound(\"rmse\", digits = 0)\n\n\n\n\n\n\nFor the future data, we see a very different story. As expected, the tuned decision tree does appear to be over-fitted - it performs poorer than the default decision tree. The LASSO model is the best, followed by the previously tuned XGBoost model. Despite having similar hyper-parameters, the two XGBoost models perform differently.\n\n\n9.8.2 Visualising the fit\nAlthough useful, the RMSE is just a single number so it’s helpful to visualise the fit. In particular, because these are models for a reserving data set, we can take advantage of that structure when analysing how well each model is performing.\n\n9.8.2.1 Fitted values\nFirst, lets show visualise the fitted payments. The graphs below show the log(fitted values), or log(payments) in the case of the actual values.\n\n# add in the actual values for comparison\nactuals &lt;- model_forecasts_long[model==\"regr.glm\",]\nactuals[, model := \"actual\"\n        ][, fitted := pmts]\n          \nmodel_forecasts_long_plot &lt;- rbind(actuals, model_forecasts_long)\n\n# calculate log_fitted\nmodel_forecasts_long_plot[, log_fitted := log(pmax(1, fitted))]\n\n# plot\nggplot(model_forecasts_long_plot, aes(dev, acc)) +\n  scale_y_reverse() +\n  geom_raster(aes(fill = log_fitted)) +\n  facet_wrap(~model, ncol=2)+\n  scale_fill_viridis_c(begin=1, end=0) +\n  geom_line(aes(x=num_periods+1-acc, y=acc), colour=dgrey, size=2)+\n  theme_classic()+\n  theme(strip.text = element_text(size=8,colour=dgrey), strip.background = element_rect(colour=\"white\", fill=\"white\"))+\n  theme(axis.title.x = element_text(size=8), axis.text.x  = element_text(size=7))+\n  theme(axis.title.y = element_text(size=8), axis.text.y  = element_text(size=7))+\n  theme(element_line(size=0.25, colour=dgrey))+\n  theme(legend.position=c(0.75, 0.1), legend.direction = \"horizontal\", legend.title=element_blank(), legend.text=element_text(size=8))+\n  labs(x=\"Accident Quarter\", y=\"Development Quarter\")+\n  NULL\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\nSome things are apparent from this:\n\nThe lack of interactions or diagonal effects in the Chain ladder (regr.glm)\nAll the machine learning models do detect and project the interaction\nThe blocky nature of the decision tree (regr.rpart) and the overfit to the past data\nThe random forest (regr.ranger) and XGBoost fit look smoother since they are a function of a number of trees.\nThe LASSO fit (regr.cv_glmnet) is the smoothest, which is not surprising since it consists of continuous functions.\nVisually, the LASSO seems to capture the interaction the best.\n\n\n\n9.8.2.2 Actual vs Fitted heat maps\nWe can also look at the model fits via heat maps of the actual/fitted values. The function defined below calculates the actual/fitted values, capping and collaring them at 400% and 25%. The values are then shaded so that:\n\nDark blue = 25%\nWhite = 100%\nDark red = 400%\n\nFor triangular reserving data, these types of plots are very helpful when examining plot fit.\nFirst, here’s a function to draw the heatmaps.\n\n# heat maps\n\nGraphHeatMap &lt;- function(dat, x=\"dev\", y=\"acc\", facet=\"model\", actual, fitted, lims=c(0.25, 4),\n                         xlab=\"Development quarter\", ylab=\"Accident Quarter\"){\n  \n  # copy data to avoid modifying original\n  localdat &lt;- copy(dat)\n  \n  # get fails if there is a variable with the same name so make local copies\n  local_x &lt;- x\n  local_y &lt;- y\n  local_actual &lt;- actual\n  local_fitted &lt;- fitted\n  \n  # make restricted Avs F for heatmap and set up past/future split line\n  np &lt;- max(localdat[[y]])\n  \n  localdat[, .avsf := get(local_actual) / get(local_fitted)\n           ][, .avsf_restrict_log := log(pmax(min(lims), pmin(max(lims), .avsf)))\n             ][, .past_line := np + 1 - get(local_y)]\n  \n  \n  g &lt;- ggplot(data=localdat, aes_string(x=local_x, y=local_y)) +\n    geom_tile(aes(fill = .avsf_restrict_log))+scale_y_reverse()+\n    facet_wrap(~get(facet), ncol=2)+\n    theme_classic()+\n    scale_fill_gradient2(name=\"AvF_min\", low=mblue, mid=\"white\", high=red, midpoint=0, space=\"Lab\", na.value=\"grey50\", guide=\"colourbar\")+\n    labs(x=xlab, y=ylab)+\n    geom_line(aes_string(x=\".past_line\", y=local_y), colour=dgrey, size=2)+\n    theme(strip.text = element_text(size=8,colour=\"grey30\"), strip.background = element_rect(colour=\"white\", fill=\"white\"))+\n    theme(axis.title.x = element_text(size=8), axis.text.x  = element_text(size=7))+\n    theme(axis.title.y = element_text(size=8), axis.text.y  = element_text(size=7))+\n    theme(element_line(size=0.25, colour=\"grey30\"))+\n    theme(legend.position=\"none\", )+  # legend.direction = \"horizontal\", legend.title=element_blank(), legend.text=element_text(size=8)\n    NULL    \n  \n\n  invisible(list(data=localdat, graph=g))\n  \n\n}\n\n\ng &lt;- GraphHeatMap(model_forecasts_long, x=\"dev\", y=\"acc\", facet=\"model\", actual=\"pmts\", fitted=\"fitted\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\ng$graph\n\n\n\n\nAll models have shortcomings, but the LASSO and previously tuned XGBoost models outperform the others.\n\n\n9.8.2.3 Quarterly tracking\nFinally, we will look at the predictions for specific parts of the data set. Quarterly tracking graphs:\n\nPlot the actual and fitted payments, including future values, by one of accident / development / calendar period\nHold one of the other triangle directions fixed, meaning that the third direction is then determined.\nAdditionally plot the underlying mean from which the data were simulated.\n\nWe’ll use a couple of functions because we want to do this repeatedly.\nThe first function (GraphModelVals) is from the blog on the LASSO model, and draws a single tracking graph. The second function (QTracking) generates the graphs for all models and uses the patchwork package to wrap them into a single plot.\n\nGraphModelVals&lt;-function(dat, primary_predictor, secondary_predictor, secondary_predictor_val, \n                         xaxis_label, yaxis_label, var_names, log_values = TRUE, include_st=FALSE, include_legend=FALSE, font_size=6){\n\n\n  # var_names must be list with names like this: list(actual=\"pmts\", mean=\"mu\", fitted=\"fitted\")\n  \n  # extract data we want to use\n  use_dat &lt;- dat[get(secondary_predictor) == secondary_predictor_val, ]\n  \n  # turn into long format using melt.data.table since that works better with ggplot\n  dat_long &lt;- melt(dat[get(secondary_predictor) == secondary_predictor_val, ],\n                   measure.vars = unlist(var_names),\n                   id.vars = primary_predictor)\n  \n  # make the names nicer - colnames and labels\n  setnames(dat_long, primary_predictor, \"predictor\")\n  \n  dat_long[variable == var_names$actual, variable := \"Simulated\"\n           ][variable == var_names$mean, variable := \"Underlying\"\n             ][variable == var_names$fitted, variable := \"Fitted\"]\n  \n  # get the levels of the variables right so that they are plotted in the right order\n  dat_long[, variable := factor(variable, levels=c(\"Fitted\", \"Simulated\", \"Underlying\"))]\n  \n  \n  if (log_values) dat_long[, value := log(value)]\n  \n  # figure out past data rectangle coordinates\n  xmin1 &lt;- use_dat[train_ind == TRUE, min(get(primary_predictor))]\n  xmax1 &lt;- use_dat[train_ind == TRUE, max(get(primary_predictor))]\n  \n  ymin1 &lt;- dat_long[, min(value)]*0.95\n  ymax1 &lt;- dat_long[, max(value)]*1.05\n  \n  \n  # draw the tracking plots\n  g &lt;- ggplot(data=dat_long, aes(x=predictor, y=value, group=variable))+\n    geom_line(aes(linetype=variable, colour=variable, size=variable, alpha=variable))+\n    geom_line(aes(linetype=variable, colour=variable))+\n    scale_colour_manual(name=\"\", values=c(red, dgrey, dgrey))+\n    scale_linetype_manual(name=\"\", values=c(\"solid\", \"solid\", \"dotted\"))+\n    scale_size_manual(name=\"\", values=c(2,1,1))+\n    scale_alpha_manual(name=\"\", values=c(0.8, 0.5, 0.5))+\n    theme_classic()+\n    annotate(geom=\"rect\", xmin=xmin1, xmax=xmax1, ymin=ymin1, ymax=ymax1, alpha=0.1)+\n    labs(x=xaxis_label, y=yaxis_label, title=paste(xaxis_label, \"tracking for\", secondary_predictor, \"=\", secondary_predictor_val)) +\n    theme(axis.title = element_text(size = font_size), axis.text = element_text(size = font_size-1))\n         \n  if(include_st==TRUE) g &lt;- g + labs(subtitle=\"Past data in grey rectangle\") + theme(plot.subtitle = element_text (size = font_size))\n\n  g &lt;- if(include_legend==TRUE) g + theme(legend.position=c(1.5, 0.5)) else g + theme(legend.position = \"none\")\n  \n  \n\n  # return the results  \n  invisible(list(data=dat_long, graph=g))\n}\n\nLet’s have a look at the tracking for accident quarter when development quarter = 5. We’ll wrap this in another function since we want to call this for a few different combinations.\n\nQTracking &lt;- function(dat,\n                      model_names,\n                      primary_predictor,\n                      secondary_predictor,\n                      secondary_predictor_val,\n                      xaxis_label,\n                      yaxis_label = \"Log(Payments)\",\n                      font_size = 8,\n                      plots_ncol=2){\n\n  # hold each plot\n  tracking_g &lt;- list()\n\n  # produce each plot  \n  for (model in model_names){\n    \n    # variable names in plot\n    vn &lt;- list(actual=\"pmts\", mean=\"mu\", fitted=model)\n    \n    # include subtitle or not in the graph (first plot only)\n    bool_st &lt;- if(model == model_names[1]) TRUE else FALSE\n    \n    # include legend or not in the graph (last plot only)\n    bool_legend &lt;- if(model == model_names[length(model_names)]) TRUE else FALSE\n    \n    # draw plot\n    tracking_g[[model]] &lt;- GraphModelVals(dat, \n                                          primary_predictor = primary_predictor, \n                                          secondary_predictor = secondary_predictor, \n                                          secondary_predictor_val = secondary_predictor_val, \n                                          xaxis_label = xaxis_label, \n                                          yaxis_label = yaxis_label, \n                                          var_names = vn, \n                                          include_st = bool_st, \n                                          include_legend = bool_legend,\n                                          font_size = font_size)$graph +\n      ggtitle(paste(\"Modelled values for\", model)) + \n      theme (plot.title = element_text (size = (font_size+1)))\n  }\n\n  # use patchwork to wrap all plots into a single display  \n  wrap_plots(tracking_g, ncol=2)\n  \n    \n}\n\n\nQTracking(model_forecasts,\n          model_names = model_names,\n          primary_predictor = \"acc\",\n          secondary_predictor = \"dev\",\n          secondary_predictor_val = 5,\n          xaxis_label = \"Accident Quarter\")\n\n\n\n\nBecause this is mostly old data and because it is not affected by the interaction, the tracking should be generally good, even for the Chain ladder - which is the case (apart from the final quarter being a bit wild as can happen with a Chain ladder). The decision tree model is poor, the random forest and XGBoost model show some evidence of overfitting. The LASSO model seems to strike the right balance.\nHere are a few more tracking plots to help further illustrate the model fits.\nTracking for accident quarter when development quarter = 24\n\nQTracking(model_forecasts,\n          model_names = model_names,\n          primary_predictor = \"acc\",\n          secondary_predictor = \"dev\",\n          secondary_predictor_val = 24,\n          xaxis_label = \"Accident Quarter\")\n\n\n\n\nDevelopment quarter 24 is impacted by the interactions for accident quarters&gt;17. All models reflect this to different extents, with XGBoost and the LASSO doing the best.\nTracking for accident quarter when development quarter = 35\n\nQTracking(model_forecasts,\n          model_names = model_names,\n          primary_predictor = \"acc\",\n          secondary_predictor = \"dev\",\n          secondary_predictor_val = 35,\n          xaxis_label = \"Accident Quarter\")\n\n\n\n\nThis is quite hard for the models since most of the values are in the future. XGBoost (previously tuned) and the LASSO are the best here.\nWe can look at similar plots by development quarter for older and newer accident quarters\nTracking for development quarter when accident quarter = 5\n\nQTracking(model_forecasts,\n          model_names = model_names,\n          primary_predictor = \"dev\",\n          secondary_predictor = \"acc\",\n          secondary_predictor_val = 5,\n          xaxis_label = \"Development Quarter\")\n\n\n\n\nTracking for development quarter when accident quarter = 20\n\nQTracking(model_forecasts,\n          model_names = model_names,\n          primary_predictor = \"dev\",\n          secondary_predictor = \"acc\",\n          secondary_predictor_val = 20,\n          xaxis_label = \"Development Quarter\")"
  },
  {
    "objectID": "Foundations/07_mlr3example.html#reserves",
    "href": "Foundations/07_mlr3example.html#reserves",
    "title": "9  ML modelling on triangles - a worked example",
    "section": "9.9 Reserves",
    "text": "9.9 Reserves\nFinally, we’ll look at the reserve estimates from the different models. For convenience, here’s the mapping of model name to model type again:\n\nregr.glm - Chainladder\nregr.cv_glmnet - LASSO\nregr.rpart - decision tree\nregr.ranger - random forest\nregr.xgboost and regr.xgboost_prev - the two XGBoost models.\n\n\n# summarise the future payments and each model projection by accident quarter\n# remember ml is a character vector of all the model names, which are columns of\n#   fitted values in model_forecasts\n\n# get reserves and payments by accident quarter\nos_acc &lt;- model_forecasts_long[train_ind == FALSE, .(actual=sum(pmts)/1e9, reserve=sum(fitted)/1e9), by=.(model, acc)]\n\n# make full tidy data set by stacking actuals\nos_acc &lt;- rbind(os_acc[model==\"regr.glm\",][, reserve := actual][, model := \"actual\"],\n                os_acc)\n\n# adjust levels of factor to ger correct ordering\nos_acc[, model := factor(as.character(model), level=c(\"actual\", \"regr.cv_glmnet\", \"regr.glm\", \"regr.ranger\", \n                                                      \"regr.rpart\", \"regr.xgboost\", \"regr.xgboost_prev\"))]\n\nsetkey(os_acc, model, acc)\n\n\n# create a factor variable from model so we can order it in the plot as we wish\ng1 &lt;- ggplot(data=os_acc, aes(x=acc, y=reserve, group=model)) +\n  geom_line(aes(linetype=model, colour=model, size=model, alpha=model))+\n  scale_colour_manual(name=\"\", values=c(dgrey, red, dgrey, fuscia, dgrey, mblue, gold))+\n  scale_linetype_manual(name=\"\", values=c(\"solid\", \"solid\", \"dotted\", \"dotdash\", \"dashed\", \"longdash\", \"solid\"))+\n  scale_size_manual(name=\"\", values=c(2.5, 2, 1, 1.25, 1, 1.25, 1.5))+\n  scale_alpha_manual(name=\"\", values=c(1, 0.9, 0.5, 0.7, 0.5, 0.7, 0.9))+\n  coord_cartesian(ylim=c(0, 100), xlim=c(0, 40))+\n  theme_classic() +\n  theme(legend.position = \"bottom\")+\n  labs(y=\"Reserve (Bn)\", x=\"Accident Quarter\")+\n  NULL\n\ng1\n\n\n\n\nLet’s look at a zoomed in version of this plot.\n\ng1 + coord_cartesian(ylim=c(0, 40), xlim=c(10, 40))\n\n\n\n\nFinally, here are the overall reserves, summed over all accident quarters\n\nos &lt;- os_acc[, .(reserve=sum(reserve), actual=sum(actual)), by=.(model)]\nos[, ratio := reserve / actual][, actual := NULL]\n\n\n# sort\nos[, ratio_diff := abs(ratio-100)]\nos &lt;- os[order(ratio_diff)][, ratio_diff := NULL]\n\n# output table\nsetnames(os, c(\"model\", \"reserve\", \"ratio\"), c(\"Model\", \"Reserve(Bn)\", \"Ratio to actual(%)\"))\n\nos |&gt; \n  datatable() |&gt; \n  formatRound(\"Reserve(Bn)\", digits = 1) |&gt; \n  formatPercentage(\"Ratio to actual(%)\", digits = 0)\n\n\n\n\n\n\nThe best performers are the LASSO and the previously tuned XGBoost model. The overall reserve for the Chainladder model (regr.glm) hides the fact that this result is actually significant under-estimation in most accident quarters balanced by significant over-estimation in the last."
  },
  {
    "objectID": "Foundations/07_mlr3example.html#commentary",
    "href": "Foundations/07_mlr3example.html#commentary",
    "title": "9  ML modelling on triangles - a worked example",
    "section": "9.10 Commentary",
    "text": "9.10 Commentary\nWhat’s going on with the XGBoost results?\nAs we noted when we fitted the XGBoost model, there are a few components of randomness from one run to another, with different seeds:\n\nThe cross validation folds will be different\nThe random search parameters will be different.\n\nFor a small data set like this one, this has the potential to alter the results - afterall, getting a good result on the future data depends on whether the tuning gets lucky in terms of estimating the magnitude of the interaction.\nWe could reduce the variability by uisng a grid-search over a fixed set of parameters but - full disclosure - that led to even worse results.\nSo what’s going on? It might be helpful to have another look at the folds used in the cross-validation. These are shown below - the yellow dots represent training data points in each fold, the blue dots are the test data. The grey rectangle marks the interaction.\n\n\n\n\n\nThe first thing that is apparent is that the interaction only applies to a very small number of points (10) in the past data. So in that sense, it’s quite remarkable that the models perform as well as they actually do - they all detect the interaction. Where they start to slip up is that they do not necessarily follow the development quarter shape in the tail.\nWith cross-validation, all points are in the test data set once and once only. So it seems to be the case that the particular allocation of data into folds in this notebook leads to poorer results for the XGBoost model. In other words, in our previous tuning for XGBoost we got lucky.\nSo far, XGBoost seems to be the only model impacted in this way. Based on looking at a few different sets of results, the LASSO model has generally been good for different folds and for 5-, 6- and 8-fold validation. The performance of the ranger model doesn’t appear to change all that much.\nHowever, given XGBoost’s popularity and flexibility, this result is not ideal. The question is: can we improve matters?\nSuccess in machine learning often comes down to optimising the following:\n\nWhat you model\nHow you model (i.e. what features you use)\nYour performance measure\nYour train/test split.\n\nReserving is a forecasting problem - we have a time series of payments and we want to forecast into the future. This suggests that cross-validation is not an ideal method for tuning reserving models - we would be better with a train/test split where the test data is in the future relative to the past data.\nThis problem has been discussed in the context of time series modelling, where the use of rolling window cross-validation techniques has been advocated. We’ll be discussing this in a future article."
  },
  {
    "objectID": "Foundations/07_mlr3example.html#conclusion",
    "href": "Foundations/07_mlr3example.html#conclusion",
    "title": "9  ML modelling on triangles - a worked example",
    "section": "9.11 Conclusion",
    "text": "9.11 Conclusion\nWe set out to demonstrate how to fit a number of reserving models to a data set. Our results have been mixed - but we expected that in advance. This work lays a baseline for our upcoming articles where we will look at ways of improving the results through:\n\nexpanding the range of models\nconsidering more appropriate validation techniques.\n\n\n9.11.1 What next?\nYou can try running this code and changing some things - e.g. the hyper-parameters tuned, search strategy, number of iterations etc.\nYou can also try running the code for different data sets. It’s best to initially work with data sets that are familiar to you, so your own data sets or other simulated data sets may be useful here. In the code block below, we’ve shared the code used to generate the data set we used. You can generate other data sets as described in the LASSO paper using the code block below. More details are in the paper, but in brief:\n\nData set 1 = a Chainladder model\nData set 2 = a Chainladder model + calendar period effects\nData set 3 = the data set used here\nData set 4 = data set 2 but where the strength of the calendar period effect varies by development period.\n\n\nlibrary(data.table)\n\n# this function generates the four different forms of data set as described in the LASSO paper\n\nCreateSyntheticData&lt;-function(whichsim, numperiods)\n{\n  \n    # create the acc/dev/cal parameters\n    kk &lt;- rep(1:numperiods, each = numperiods) #AQ\n    jj &lt;- rep(1:numperiods, times= numperiods) #DQ\n    tt &lt;- kk+jj-1 # PQ\n\n    # set alpha/beta/gamma - hard-code up the sim values\n    if (whichsim == 1){\n        alpha &lt;- log(100000)+0.1*LinearSpline(kk,1,15)+0.2*LinearSpline(kk,15,20) - 0.05*LinearSpline(kk,30,40) \n        beta  &lt;- (16/3 - 1)*log(jj)- (1/3)*jj\n        gamma &lt;- 0\n        mu &lt;- exp( alpha + beta + gamma)  \n    }\n    else if (whichsim == 2){\n        alpha &lt;- log(100000)+0.1*LinearSpline(kk,1,15)+0.2*LinearSpline(kk,15,20) - 0.05*LinearSpline(kk,30,40) \n        beta  &lt;- (16/3 - 1)*log(jj)- (1/3)*jj  # a is 16/3, b is 1/3 \n        gamma &lt;- gammafunc(tt)\n        mu &lt;- exp( alpha + beta + gamma)  \n    }\n    else if (whichsim == 3){\n        alpha &lt;- log(100000)+0.1*LinearSpline(kk,1,15)+0.2*LinearSpline(kk,15,20) - 0.05*LinearSpline(kk,30,40) \n        beta  &lt;- (16/3 - 1)*log(jj)- (1/3)*jj  # a is 16/3, b is 1/3 \n        gamma &lt;- gammafunc(tt)\n        mu &lt;- exp( alpha + beta + gamma + 0.3*beta*ifelse(kk&gt;16 & jj&gt;20,1,0))  \n    }\n    else if (whichsim == 4){\n        alpha &lt;- log(100000)+0.1*LinearSpline(kk,1,15)+0.2*LinearSpline(kk,15,20) - 0.05*LinearSpline(kk,30,40) \n        beta  &lt;- (16/3 - 1)*log(jj)- (1/3)*jj  # a is 16/3, b is 1/3 \n        gamma &lt;- gammafunc(tt)\n        mu &lt;- exp( alpha + beta + gamma*((numperiods-1)-LinearSpline(jj,1,numperiods))/(numperiods-1) )  # need to check\n    }\n    \n    varbase &lt;- (0.3 * mu[  kk==1 & jj ==16] )^2 # can scale variance up and down here\n    CC  &lt;-  varbase / mu[  kk==1 & jj ==16]\n    \n    vars   &lt;- CC*mu\n    tausq  &lt;- log (vars / (mu^2) + 1)\n    \n    pmts &lt;- exp( rnorm( numperiods^2, mean = log(mu)-0.5*tausq , sd = sqrt(tausq)  ) )\n    \n    # indicator for past/future = traint/test\n    train_ind&lt;-(tt&lt;=numperiods)\n    \n    ### data fram for output\n    full&lt;-data.table(pmts, acc=as.integer(kk), dev=as.integer(jj), cal=as.integer(tt), mu, train_ind )\n    full\n}\n\n\n#---------------------------\n# function to generate calendar period effects used in CreateSyntheticData()\n\ngammafunc &lt;- function(t){\n    gg &lt;- \n        ifelse( t&lt;=12, gg &lt;- 0.0075*LinearSpline(t,1,12),\n                ifelse(t&lt;=24,  gg &lt;- 0.0075*LinearSpline(12,1,12) + 0.001* (t-12)*(t-11)/2,\n                       ifelse(t&lt;=32, gg &lt;- 0.0075*LinearSpline(12,1,12) + 0.001* (24-12)*(24-11)/2,\n                           ifelse(t&lt;=40, gg &lt;- 0.0075*LinearSpline(12,1,12) + 0.001* (24-12)*(24-11)/2 + 0.002*(t-32)*(t-31)/2,\n                               0.0075*LinearSpline(12,1,12) + 0.001* (24-12)*(24-11)/2 + 0.002*(40-32)*(40-31)/2\n                           ))))\n    1*gg  #can scale up shape here if desired\n}\n\n\n\n#---------------------------\n# linear spline function - used in data generation and in spline generation below\n# this was defined earlier on in this worked example, but including the definition here so it is self-contained\nLinearSpline &lt;- function(var, start, stop){\n    pmin(stop - start, pmax(0, var - start))\n}\n\n\n#---------------------------\n# How to run the code\n# The code below, including the seed value, recreate the data we used above\n\n# vary the seed to create different data sets of the same form\nset.seed(130)  \n\n# whichsim can be 1, 2, 3, 4 to produce data sets in the form above\n# numperiods sets the dimensions of the triangle\ndat &lt;- CreateSyntheticData(whichsim = 3, numperiods = 40)"
  },
  {
    "objectID": "Foundations/07_mlr3example.html#session-details",
    "href": "Foundations/07_mlr3example.html#session-details",
    "title": "9  ML modelling on triangles - a worked example",
    "section": "9.12 Session details",
    "text": "9.12 Session details\nThe details of the R session used to generate the results are below.\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_Ireland.utf8  LC_CTYPE=English_Ireland.utf8   \n[3] LC_MONETARY=English_Ireland.utf8 LC_NUMERIC=C                    \n[5] LC_TIME=English_Ireland.utf8    \n\ntime zone: Europe/Dublin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] DT_0.30                      rpart.plot_3.1.1            \n[3] rpart_4.1.19                 patchwork_1.1.3             \n[5] ggplot2_3.4.4                data.table_1.14.8           \n[7] mlr3extralearners_0.7.1-9000 mlr3verse_0.2.8             \n[9] mlr3_0.16.1                 \n\nloaded via a namespace (and not attached):\n  [1] gridExtra_2.3          rlang_1.1.1            magrittr_2.0.3        \n  [4] clue_0.3-65            compiler_4.3.1         flexmix_2.3-19        \n  [7] vctrs_0.6.4            pkgconfig_2.0.3        shape_1.4.6           \n [10] crayon_1.5.2           fastmap_1.1.1          ellipsis_0.3.2        \n [13] backports_1.4.1        labeling_0.4.3         utf8_1.2.4            \n [16] rmarkdown_2.25         mlr3cluster_0.1.8      xfun_0.40             \n [19] glmnet_4.1-8           modeltools_0.2-23      cachem_1.0.8          \n [22] mlr3misc_0.13.0        jsonlite_1.8.7         uuid_1.1-1            \n [25] fpc_2.2-10             mlr3measures_0.5.0     mlr3fselect_0.11.0    \n [28] parallel_4.3.1         prabclus_2.3-3         cluster_2.1.4         \n [31] R6_2.5.1               bslib_0.5.1            ranger_0.15.1         \n [34] parallelly_1.36.0      jquerylib_0.1.4        diptest_0.76-0        \n [37] xgboost_1.7.5.1        Rcpp_1.0.11            iterators_1.0.14      \n [40] knitr_1.44             future.apply_1.11.0    Matrix_1.5-4.1        \n [43] splines_4.3.1          nnet_7.3-19            tidyselect_1.2.0      \n [46] rstudioapi_0.15.0      yaml_2.3.7             viridis_0.6.4         \n [49] mlr3tuning_0.19.0      mlr3viz_0.6.1          codetools_0.2-19      \n [52] listenv_0.9.0          lattice_0.21-8         tibble_3.2.1          \n [55] withr_2.5.1            evaluate_0.22          future_1.33.0         \n [58] survival_3.5-5         mclust_6.0.0           kernlab_0.9-32        \n [61] pillar_1.9.0           mlr3mbo_0.2.1          mlr3filters_0.7.1     \n [64] checkmate_2.3.0        renv_1.0.3             foreach_1.5.2         \n [67] stats4_4.3.1           generics_0.1.3         bbotk_0.7.2           \n [70] munsell_0.5.0          scales_1.2.1           globals_0.16.2        \n [73] class_7.3-22           glue_1.6.2             tools_4.3.1           \n [76] mlr3pipelines_0.5.0-1  robustbase_0.99-0      mlr3hyperband_0.4.5   \n [79] grid_4.3.1             crosstalk_1.2.0        mlr3data_0.7.0        \n [82] colorspace_2.1-0       paradox_0.11.1         palmerpenguins_0.1.1  \n [85] cli_3.6.1              spacefillr_0.3.2       fansi_1.0.5           \n [88] viridisLite_0.4.2      dplyr_1.1.3            gtable_0.3.4          \n [91] DEoptimR_1.1-3         sass_0.4.7             digest_0.6.33         \n [94] lgr_0.4.4              htmlwidgets_1.6.2      farver_2.1.1          \n [97] htmltools_0.5.6.1      lifecycle_1.0.3        mlr3learners_0.5.6    \n[100] mlr3tuningspaces_0.4.0 MASS_7.3-60"
  },
  {
    "objectID": "Foundations/08_intro_glm_gam_xgboost.html#introduction",
    "href": "Foundations/08_intro_glm_gam_xgboost.html#introduction",
    "title": "10  Getting to grips with GLM, GAM and XGBoost",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nCongratulations! You’ve just completed your introductory course in data science and analytics in R, and you’re ready to start enriching your day-to-day actuarial work with new and exciting models.\nBut where to get started? This blog post aims to highlight a useful video resource for beginners looking to gain some insights into the realities of applying machine learning models in practice.\nIf you’d like to try out some alternatives to GLMs within R, in a general insurance setting, I’d recommend Insurance Risk Pricing with GLM, GAM and XGBoost by Matthew Evans and Callum Hughes.\n\n10.1.1 Is it relevant for reserving actuaries?\nBefore going any further, we should talk about why a webinar on Risk Pricing is relevant for reserving actuaries.\nHaving worked across pricing and reserving in my career, I’ve found the leap from traditional models to machine learning approaches to be easier on pricing problems than reserving tasks. Even if your end goal is to enhance your reserving models, this video is still worth a watch. It will act as a useful stepping stone to more advanced presentations on applications of machine learning approaches to triangular data.\n\n\n10.1.2 What does the material cover?\nThis 30 minute video will give you an overview of three models:\n\nGeneralized Linear Models\nGeneralized Additive Models\nXGBoost\n\nRather than going into lots of detail on a particular model, it gives an overview, as well as some key pros and cons to consider, and then gets straight into fitting it.\nIt will then take you through the application of these models to some simulated data in R, and an example of how to compare the results.\nThe level of detail in this video, and the complexity of the accompanying code, is at a great level for anyone who has some awareness of these models, and has some experience of coding in R, but wouldn’t consider themselves an expert.\n\n\n10.1.3 Who would find this most useful?\nIf you’ve completed a course in R, but haven’t built up a large amount of practical experience using it, then this code and video will be a good way to refresh your knowledge.\nIf you have worked with GLMs before, then putting 30 minutes aside to watch the video in isolation would be a good way to get a basic overview of XGBoost and GAMs, as well as an example of model tuning using the Caret package.\nThe code itself is also well annotated, and at just over 300 lines, it won’t take long to work through.\n\n\n10.1.4 What next?\nHaving worked through the code and watched the video, a great next step into applying these techniques can be found in the following articles or pages:\n\nResources on the Foundations workstream page\nReserving using GLMs in Chapter 7\nML modelling on triangles - a worked example"
  },
  {
    "objectID": "Foundations/08_intro_glm_gam_xgboost.html#summary",
    "href": "Foundations/08_intro_glm_gam_xgboost.html#summary",
    "title": "10  Getting to grips with GLM, GAM and XGBoost",
    "section": "10.2 Summary",
    "text": "10.2 Summary\nThis video and code are a great way to dip your toe into using machine learning techniques within R, in a short period of time. It should set you up well for more complex code and presentations, in either a pricing or reserving setting."
  },
  {
    "objectID": "Foundations/08_intro_glm_gam_xgboost.html#notes-on-the-video-and-code",
    "href": "Foundations/08_intro_glm_gam_xgboost.html#notes-on-the-video-and-code",
    "title": "10  Getting to grips with GLM, GAM and XGBoost",
    "section": "10.3 Notes on the video and code",
    "text": "10.3 Notes on the video and code\nAuthors: Matthew Evans, Callum Hughes\nVideo: Insurance Risk Pricing with GLM, GAM and XGBoost - YouTube\nCode: xgboost-virtual-data-science-seminar/xgb presentation (005).R at master · mdevans21/xgboost-virtual-data-science-seminar · GitHub"
  },
  {
    "objectID": "Data/data.html",
    "href": "Data/data.html",
    "title": "Data",
    "section": "",
    "text": "The aims of the Data Workstream are to:\n\nIdentify publicly available datasets that can be used to illustrate reserving techniques.\nProvide a summary of the features available in each dataset.\nProvide notebook examples of how to generate simulated datasets.\n\nIn this section we will cover some data simulation tools which have been created both to meet the need for data given the lack of publicly available data and to have data with known features available to evaluaiton new techniques."
  },
  {
    "objectID": "Data/simulationmachine.html#the-right-time",
    "href": "Data/simulationmachine.html#the-right-time",
    "title": "11  simulationmachine",
    "section": "11.1 The right time",
    "text": "11.1 The right time\nIf you build a polygon from rods and hinges, what is the only shape to hold firm when you push on the edges? It is a triangle. Our three sided friend is everywhere in construction - look out the next time you walk past a pylon or bridge. We can think of the triangle as the shape’s shape; irreducible, and good for approximating other shapes e.g. computer graphics represent complex surfaces by covering them in a mesh of small triangles and zooming out. In the world of insurance, if you zoom out far enough, individual claims data morphs into the familiar development triangle. The development triangle has the effect of watering claims data down into a thin porridge: any ten-year annual paid history - whether your portfolio contains dozens of claims or millions - is diluted to just 55 numbers for a reserve study. All fine for chain ladder methods, with a small appetite for bumps and edges, but machine learning algorithms are data hungry. If we want to test the full range of machine learning methods available then we need to start to zoom in on the triangle.\nMany actuaries can now source transactional data from tools managed by their employers, but this poses two problems:\n\nCompany data cannot be released into the public domain for others to use, so the company actuary is unable to share the details of her research with outsiders.\nIt is not unheard of for company data to contain errors. It is more difficult to probe the pros and cons of a data-driven technique if the input has missing values, accident dates that occur after reporting dates, large positive and negative dummy transactions that offset, or other fun and amusing diversions. Of course, reserving is chiefly a practical data exercise and at some point this means machine learning cannot demand perfect data from the actuary. However, perhaps there are interesting questions to be answered first."
  },
  {
    "objectID": "Data/simulationmachine.html#make-along-with-me",
    "href": "Data/simulationmachine.html#make-along-with-me",
    "title": "11  simulationmachine",
    "section": "11.2 Make along with me",
    "text": "11.2 Make along with me\nFortunately for those interested in applying machine learning to a reserving context, Gabrielli and Wüthrich (2018) have released an infinite supply of polished datasets using a simulation approach set out in a 2018 paper.\nBriefly, they have built a tool in the R environment which mimics a company dataset containing twelve years of history for about ten million claims.\nThe data is generated at a claim level and includes the paid amount each year in addition to non-financial features. For example, the data includes factors for claim code and line of business, and the machine allows some assumptions to be varied at whim.\nKuo (2019) has helpfully gift-wrapped the simulation machine in an R package that allows us to easily generate simulation outputs.\nLet’s look at some example code below.\nFirst of all, the package is not on CRAN so it must be installed from github as follows:\n\n# install.packages(\"remotes\")   # uncomment to install the remotes package if you don't already have it\nremotes::install_github(\"kasaai/simulationmachine\")   # install from github as not on CRAN\n\n# or - if you are using renv for package management\nrenv::install(\"kasaai/simulationmachine\")\n\n\nlibrary(simulationmachine) #Kuo package\nlibrary(data.table) #my preferred package for manipulating data\nlibrary(ggplot2) #plotting package\nlibrary(scales) #for adding commas to the numbers in plot axis labels\noptions(scipen = 999) #stop R from showing big numbers in scientific notation\n\n\n#set up the simulation\ncharm &lt;- simulation_machine(\n  num_claims = 10000, #Parameter for the expected total number of claims in the simulation output \n  lob_distribution = c(0.25, 0.25, 0.25, 0.25), #there are 4 lines of business, so the proportions must sum to 1\n  inflation = c(0.03,0.01,0.01,0.01), #inflation per year for each lob\n  sd_claim = 0.5, #how volatile are claim amounts?\n  sd_recovery = 0.75 #how volatile are recovery payments?\n)\n\nWarning: `as_integer()` is deprecated as of rlang 0.4.0\nPlease use `vctrs::vec_cast()` instead.\nThis warning is displayed once every 8 hours.\n\n\nWarning: `as_double()` is deprecated as of rlang 0.4.0\nPlease use `vctrs::vec_cast()` instead.\nThis warning is displayed once every 8 hours.\n\n#simulate the data and store in a variable\nrecords &lt;- as.data.table(conjure(charm, seed = 142857)) #setting a seed is optional but ensures the same output for a given seed\n\n#convert some fields to factors for convenience later\nrecords$lob &lt;- as.factor(records$lob)\nrecords$cc &lt;- as.factor(records$cc)\nrecords$injured_part &lt;- as.factor(records$injured_part)\n\nThe paper describes the simulation approach in full mathematical detail. Here we just give a brief description of some of the less obvious fields:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nreport_delay\nDifference between reporting date and occurence date in years\n\n\nlob\nLine of business (1 - 4)\n\n\ncc\nClaim code factor\n\n\nage\n..of claimant (15 - 70)\n\n\ninjured_part\nFactor coding body part injured\n\n\nclaim_status_open\nIs the claim open at the end of the development year?\n\n\n\nHere is a quick look at some data snipped from the top of the table:\n\nhead(records)\n\n   claim_id accident_year development_year accident_quarter report_delay lob cc\n1:        1          1994                0                4            0   2  6\n2:        1          1994                1                4            0   2  6\n3:        1          1994                2                4            0   2  6\n4:        1          1994                3                4            0   2  6\n5:        1          1994                4                4            0   2  6\n6:        1          1994                5                4            0   2  6\n   age injured_part paid_loss claim_status_open\n1:  42           36         0                 1\n2:  42           36         0                 0\n3:  42           36         0                 0\n4:  42           36         0                 0\n5:  42           36         0                 0\n6:  42           36         0                 0\n\n\nPulling up a data summary is more useful and easy to do in R:\n\nsummary(records)\n\n   claim_id         accident_year  development_year accident_quarter\n Length:120036      Min.   :1994   Min.   : 0.00    Min.   :1.000   \n Class :character   1st Qu.:1997   1st Qu.: 2.75    1st Qu.:2.000   \n Mode  :character   Median :2000   Median : 5.50    Median :3.000   \n                    Mean   :2000   Mean   : 5.50    Mean   :2.507   \n                    3rd Qu.:2003   3rd Qu.: 8.25    3rd Qu.:4.000   \n                    Max.   :2005   Max.   :11.00    Max.   :4.000   \n                                                                    \n  report_delay     lob             cc             age         injured_part  \n Min.   :0.00000   1:30636   17     :14124   Min.   :15.00   36     :17412  \n 1st Qu.:0.00000   2:29724   6      :10092   1st Qu.:24.00   51     :10824  \n Median :0.00000   3:29748   31     : 6936   Median :34.00   12     : 8412  \n Mean   :0.09157   4:29928   19     : 6648   Mean   :35.02   53     : 8328  \n 3rd Qu.:0.00000             47     : 5220   3rd Qu.:44.00   35     : 6900  \n Max.   :9.00000             45     : 4560   Max.   :70.00   54     : 4920  \n                             (Other):72456                   (Other):63240  \n   paid_loss        claim_status_open\n Min.   :-43525.0   Min.   :0.00000  \n 1st Qu.:     0.0   1st Qu.:0.00000  \n Median :     0.0   Median :0.00000  \n Mean   :   146.1   Mean   :0.09481  \n 3rd Qu.:     0.0   3rd Qu.:0.00000  \n Max.   :292685.0   Max.   :1.00000  \n                                     \n\n\nObservations:\n\nThere are 120,036 rows, close to the expected number (10,000 expected claims x 12 years of annual paid history = 120,000 rows)\nNote - the simulation still generates an annual paid of 0 and sets claim_status_open = 1 for the years prior to the claim being reported\nClaims are usually reported within a year of occurrence\nAge ranges from 15 to 70 and averages 35\nAccident year and development year have the ranges we expect\nPaid is nil most of the time\nThe claims are evenly allocated to the four lines of business, as we expect\nThe most common injured_part and cc take up c. 10% - 15% of the data\n\nA glance at the classic paid development chart reveals a pattern that looks fairly typical:\n\n#cumulative paid development\nchart_data &lt;- records[,.(paid = sum(paid_loss)), by = c(\"accident_year\",\"development_year\")] #sum paid transactions by acc and dev year\nchart_data$cumulative_paid &lt;- chart_data[, .(paid = cumsum(paid)), by = c(\"accident_year\")]$paid #convert to cumulative paid\n\n#cumulative paid development by accident year\nggplot(data = chart_data[accident_year + development_year&lt;=2005],  \n       aes(x = development_year, y =cumulative_paid, colour = as.factor(accident_year))) + \n  geom_point() + geom_line() + scale_y_continuous(labels = comma) + ggtitle(\"Cumulative paid by accident year\") +  \n  theme(legend.title = element_blank(), axis.title.y = element_blank(), plot.caption = element_text(hjust = 0, face= \"italic\")) + \n  scale_colour_viridis_d() +\n  theme_bw()+\n  labs(x = \"Development year\", y=\"Cumulative paid\", caption = \"Real data - definitely maybe\", colour=\"Accident year\")\n\n\n\n\nIt is straightforward to go beyond the basic chart and plot the paid in finer detail - here by line of business:\n\n#cumulative paid development\nchart_data &lt;- records[,.(paid = sum(paid_loss)), by = c(\"accident_year\",\"development_year\", \"lob\")] #sum paid by acc yr, dev yr and lob \nchart_data$cumulative_paid &lt;- chart_data[, .(paid = cumsum(paid)), by = c(\"accident_year\", \"lob\")]$paid #convert to cumulative paid\n\n\n#cumulative paid development by accident year and line of business\nggplot(data = chart_data[accident_year + development_year&lt;=2005],  \n       aes(x = development_year, y =cumulative_paid, colour = as.factor(accident_year))) + \n  geom_point() + \n  geom_line() + \n  scale_y_continuous(labels = comma) + \n  ggtitle(\"Cumulative paid by accident year and line of business\") +  \n  theme(legend.title = element_blank(), axis.title.y = element_blank()) + \n  scale_colour_viridis_d() +\n  theme_bw()+\n  facet_grid(cols = vars(lob))  + \n  labs(x = \"Development year\", colour=\"Accident year\")\n\n\n\n\nNext we can create a claim level summary of total paid, analyse the size distribution, and see how much average paid varies with LOB, age and reporting delay:\n\nclaim_summary &lt;- records[,.(paid=sum(paid_loss)), by = c(\"claim_id\", \"lob\", \"age\", \"report_delay\")] #calculate total paid at claim level and retain info columns\n\n#Add a column to band the claims by size\nclaim_summary[, value_band:=cut(paid, breaks = c(-Inf,0, 100, 1000, 10000, 100000, Inf), dig.lab = 6)] \n\n\n#Sumarise\nsummary(claim_summary)\n\n   claim_id         lob           age         report_delay    \n Length:10003       1:2553   Min.   :15.00   Min.   :0.00000  \n Class :character   2:2477   1st Qu.:24.00   1st Qu.:0.00000  \n Mode  :character   3:2479   Median :34.00   Median :0.00000  \n                    4:2494   Mean   :35.02   Mean   :0.09157  \n                             3rd Qu.:44.00   3rd Qu.:0.00000  \n                             Max.   :70.00   Max.   :9.00000  \n      paid                   value_band  \n Min.   :     0.0   (-Inf,0]      :2842  \n 1st Qu.:     0.0   (0,100]       : 345  \n Median :   278.0   (100,1000]    :4710  \n Mean   :  1752.8   (1000,10000]  :1914  \n 3rd Qu.:   787.5   (10000,100000]: 168  \n Max.   :799159.0   (100000, Inf] :  24  \n\n\nThe paid by claim is characterised by lots of low value claims occasionally distorted by larger claims - the nil rate is around 28% and the third quartile is under half the value of the mean. Possibly we would want to consider modelling the nil claims and claims over 100k separately.\n\n#Summarise the average paid by reporting delay\nclaim_summary[,.(average_paid = mean(paid), claim_count = .N), by = report_delay][order(report_delay)]\n\n   report_delay average_paid claim_count\n1:            0    1830.8375        9146\n2:            1     919.9178         827\n3:            2     509.9444          18\n4:            3     519.1667           6\n5:            4    5951.5000           2\n6:            5    1545.0000           2\n7:            8       0.0000           1\n8:            9       0.0000           1\n\n\nThe relationship here is unclear as around 90% of claims have a reporting delay of 0.\n\n#Summarise the average paid by line of business\nclaim_summary[, .(average_paid = mean(paid), num_claims = .N), by = lob][order(lob)]\n\n   lob average_paid num_claims\n1:   1     784.2143       2553\n2:   2    2049.8405       2477\n3:   3    2935.6910       2479\n4:   4    1273.3629       2494\n\n\nThe average cost appears to vary by lob. This could be helpful information and lead to tracking the underlying lob mix, or possibly analysing the lobs separately.\n\n#For positive paid only, how does average claim cost vary with claimant age?\nggplot(claim_summary[paid&gt;0, .(average_paid = mean(paid)), by = age], aes(x = age, y = average_paid)) + \n  geom_point() +\n  scale_colour_viridis_d() +\n  theme_bw()+\n  labs(y=\"Average paid\")\n\n\n\n\nThe average paid appears to increase up to around the age of 50. The values for ages around 55 may be worthy of further investigation, or just a small number of high value claims causing distortion."
  },
  {
    "objectID": "Data/simulationmachine.html#little-by-little",
    "href": "Data/simulationmachine.html#little-by-little",
    "title": "11  simulationmachine",
    "section": "11.3 Little by little",
    "text": "11.3 Little by little\nThis post has introduced an R package for accessing the Gabrielli and Wüthrich claims simulation machine and looked at one example simulation. The machine has obvious uses as a potential dartboard for comparing the accuracy of various machine learning and reserving methods, and as a playground for honing your R reserving skills. The charts and summaries presented here serve as a prompt for further analysis. Enjoy!"
  },
  {
    "objectID": "Data/splice.html",
    "href": "Data/splice.html",
    "title": "12  SPLICE",
    "section": "",
    "text": "This article was written by Greg Taylor and Sarah MacDonnell originally published here on 14 November 2022\nThe R data simulation package SynthETIC has been overtaken by an updated version, SPLICE (Synthetic Paid Loss and Incurred Cost Experience).\nSPLICE, whilst still based on SynthETIC, has now been extended to simulate case estimates, and hence incurred claims. It can be accessed on CRAN, along with other relevant resources including a reference manual.\nSPLICE is a useful tool for producing simulated datasets for testing out various reserving, including machine learning, methods. It generates datasets of triangles, as well as individual claims transactions, showing paid and incurred developments by occurrence as well as notification and settlement times.\nThe user can set up specific features in the datasets. For example in the Al-Mudafer thesis which we have previously featured, 4 different types of claims triangles were generated:\n\nsimple short tail claims\na gradual increase in claims processing speed\nan inflation shock\nhigh volatility long tail claims.\n\nThe MLR WP used these same datasets in our GIRO 21 workshop Machine Learning Reserving on Triangle Data.\nWe recommend using SPLICE over SynthETIC going forward as we understand that SPLICE is the one that will continue to be supported and updated.\nOur previous article introducing SynthETIC can be found here."
  },
  {
    "objectID": "Literature/literature.html",
    "href": "Literature/literature.html",
    "title": "Literature review",
    "section": "",
    "text": "The Literature review workstream reviews current literature on the use of ML in reserving. In this part of the book, we cover some innovative techniques that we have learned about in the literature - the use of GBM models in individual claim reerving and the possibilities of neural networks."
  },
  {
    "objectID": "Literature/baudry.html#introduction",
    "href": "Literature/baudry.html#introduction",
    "title": "13  Non-parametric individual claim reserving in insurance",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nIn this chapter, we provide code to replicate the central scenario in Maximilien Baudry’s paper “NON-PARAMETRIC INDIVIDUAL CLAIM RESERVING IN INSURANCE. Before running the code, we recommend that you read the original paper first.\n\nhttps://www.institutdesactuaires.com/global/gene/link.php?doc_id=11747&fg=1\n\nhttp://chaire-dami.fr/files/2016/10/Reserving-article.pdf"
  },
  {
    "objectID": "Literature/baudry.html#simulating-the-data",
    "href": "Literature/baudry.html#simulating-the-data",
    "title": "13  Non-parametric individual claim reserving in insurance",
    "section": "13.2 Simulating the data",
    "text": "13.2 Simulating the data\nBelow we step through the process to create a single simulated dataset. Having stepped through the data creation process, we present the code in the form of a function that returns a simulated policy and claim dataset at the end for later use.\nThe second notebook details the process for creating a reserving database and the third outlines the process for creating reserves using machine learning.\nBefore we start we import a few pre-requisite R packages.\n\n# Importing packages\nlibrary(data.table)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(patchwork)\nlibrary(xgboost)\nlibrary(tidymodels)\nlibrary(SHAPforxgboost)\n\n\n13.2.1 Create Policy Data set\nStart by simulating number of policies sold by day and by policy coverage type.\n\nPolicy count by date\n\n# set a seed to control reproducibility\nset.seed(123)\n\n# polices sold between start 2016 to end 2017\ndt_policydates &lt;- data.table(date_UW = seq(as.Date(\"2016/1/1\"), as.Date(\"2017/12/31\"), \"day\"))\n\n# number of policies per day follows Poisson process with mean 700 (approx 255,500 pols per annum)\ndt_policydates[, ':='(policycount = rpois(.N,700),\n                      date_lapse = date_UW %m+% years(1),\n                      expodays = as.integer(date_UW %m+% years(1) - date_UW),\n                      pol_prefix = year(date_UW)*10000 + month(date_UW)*100 + mday(date_UW))]\n\n\n\nPolicy covers by by date\nAdd policy coverage columns in proportions:\n\n25% Breakage\n45% Breakage and Oxidation\n30% Breakage, Oxidation and Theft.\n\n\n# Add columns defining Policy Covers   \ndt_policydates[, Cover_B := round(policycount * 0.25)]\ndt_policydates[, Cover_BO := round(policycount * 0.45)]\ndt_policydates[, Cover_BOT := policycount - Cover_B - Cover_BO]\n\ndatatable(head(dt_policydates))\n\n\n\n\n\n\n\n\nPolicy transaction file\nNext create a policy transaction file 1 row per policy, with columns to indicate policy coverage details.\n\nPolicy date & number\nFirst step is to create policy table with 1 row per policy and underwriting date.\n\n# repeat rows for each policy by UW-Date\ndt_policy &lt;- dt_policydates[rep(1:.N, policycount),c(\"date_UW\", \"pol_prefix\"), with = FALSE][,pol_seq:=1:.N, by=pol_prefix]\n\n# Create a unique policy number \ndt_policy[, pol_number := as.character(pol_prefix * 10000 + pol_seq)]\n\nhead(dt_policy) |&gt; datatable()\n\n\n\n\n\n\n\n\nPolicy coverage type\nCan now add the coverage appropriate to each row.\n\n# set join keys\nsetkey(dt_policy,'date_UW')\nsetkey(dt_policydates,'date_UW')  \n\n# remove pol_prefix before join\ndt_policydates[, pol_prefix := NULL]  \n\n# join cover from summary file (dt_policydates)\ndt_policy &lt;- dt_policy[dt_policydates]  \n\n# now create Cover field for each policy row\ndt_policy[,Cover := 'BO']\ndt_policy[pol_seq &lt;= policycount- Cover_BO,Cover := 'BOT']\ndt_policy[pol_seq &lt;= Cover_B,Cover := 'B']  \n\ndt_policy[, Cover_B := as.factor(Cover)]  \n\n# remove interim calculation fields\ndt_policy[, ':='(pol_prefix = NULL,\n                 policycount = NULL,\n                 pol_seq = NULL,\n                 Cover_B = NULL,\n                 Cover_BOT = NULL,\n                 Cover_BO = NULL)]\n\n# check output\nhead(dt_policy) |&gt; datatable()\n\n\n\n\n\n\n\n\nPolicy Brand, Price, Model features\nNow add policy brand, model and price details to the policy dataset\n\n# Add remaining policy details\ndt_policy[, Brand := rep(rep(c(1,2,3,4), c(9,6,3,2)), length.out = .N)]\ndt_policy[, Base_Price := rep(rep(c(600,550,300,150), c(9,6,3,2)), length.out = .N)]\n\n# models types and model cost multipliers\nfor (eachBrand in unique(dt_policy$Brand)) {\n  dt_policy[Brand == eachBrand, Model := rep(rep(c(3,2,1,0), c(10, 7, 2, 1)), length.out = .N)]\n  dt_policy[Brand == eachBrand, Model_mult := rep(rep(c(1.15^3, 1.15^2, 1.15^1, 1.15^0), c(10, 7, 2, 1)), length.out = .N)]\n}\n\ndt_policy[, Price := ceiling (Base_Price * Model_mult)]\n\n# check output\nhead(dt_policy) |&gt; datatable()\n\n\n\n\n\n\n\n\nTidy and save\nKeep only columns of interest and save resulting policy file.\n\n# colums to keep\ncols_policy &lt;- c(\"pol_number\",\n                 \"date_UW\",\n                 \"date_lapse\",\n                 \"Cover\",\n                 \"Brand\",\n                 \"Model\",\n                 \"Price\")\n\ndt_policy &lt;- dt_policy[, cols_policy, with = FALSE]\n\n# check output\nhead(dt_policy) |&gt; datatable()\n\n\n\n\n\n\n\n\n\n\n13.2.2 Create claims file\n\nSample Claims from Policies\nClaim rates vary by policy coverage and type.\n\nBreakage Claims\nStart with breakages claims. Sample from policies data set to create claims.\n\n# All policies have breakage cover\n# claims uniformly sampled from policies\nclaim &lt;- sample(nrow(dt_policy), size = floor(nrow(dt_policy) * 0.15))\n\n# Claim serverity multiplier sampled from beta distn\ndt_claim &lt;- data.table(pol_number = dt_policy[claim, pol_number],\n                       claim_type = 'B',\n                       claim_count = 1,\n                       claim_sev = rbeta(length(claim), 2,5))\n\n# check output\nhead(dt_claim) |&gt; datatable()\n\n\n\n\n\n\n\n\nOxidation Claims\n\n# identify all policies with Oxidation cover\ncov &lt;- which(dt_policy$Cover != 'B')\n\n# sample claims from policies with cover\nclaim &lt;- sample(cov, size = floor(length(cov) * 0.05))\n\n# add claims to table \ndt_claim &lt;- rbind(dt_claim,\n                  data.table(pol_number = dt_policy[claim, pol_number],\n                             claim_type = 'O',\n                             claim_count = 1,\n                             claim_sev = rbeta(length(claim), 5,3)))\n\n# check output\nhead(dt_claim) |&gt; datatable()\n\n\n\n\n\n\n\n\nTheft Claims\nIn the original paper the distribution for Theft severity claims is stated to be Beta(alpha = 5, beta = 0.5).\n\n# identify all policies with Theft cover\n# for Theft claim frequency varies by Brand\n# So need to consider each in turn...\n\nfor(myModel in 0:3) {\n\n    cov &lt;- which(dt_policy$Cover == 'BOT' & dt_policy$Model == myModel)\n    claim &lt;- sample(cov, size = floor(length(cov) * 0.05*(1 + myModel)))\n  \n    dt_claim &lt;- rbind(dt_claim,\n                      data.table(pol_number = dt_policy[claim, pol_number],\n                                 claim_type = 'T',\n                                 claim_count = 1,\n                                 claim_sev = rbeta(length(claim), 5,.5)))\n}\n\n# check output\nhead(dt_claim) |&gt; datatable()\n\n\n\n\n\ntail(dt_claim) |&gt; datatable()\n\n\n\n\n\n\n\n\n\nClaim dates and costs\n\nPolicy UW_date and value\nNow need to add details to claims, such as policy underwriting date and phone cost. These details come from the policy table.\n\n# set join keys\nsetkey(dt_policy, pol_number)\nsetkey(dt_claim, pol_number)\n\n#join Brand and Price from policy to claim\ndt_claim[dt_policy,\n         on = 'pol_number',\n         ':='(date_UW = i.date_UW,\n              Price = i.Price,\n              Brand = i.Brand)]\n\n# check output\nhead(dt_claim) |&gt; datatable()\n\n\n\n\n\n\n\n\nAdd simulated Claim occrrence, reporting and payment delays\nOccurence delay is assumed uniform over policy exposure period. Reporting and payment delays assumed to follow Beta distribution.\n\n# use lubridate %m+% date addition operator \ndt_claim[, date_lapse := date_UW %m+% years(1)]\ndt_claim[, expodays := as.integer(date_lapse - date_UW)]\ndt_claim[, occ_delay_days := floor(expodays * runif(.N, 0,1))]\n\ndt_claim[ ,delay_report := floor(365 * rbeta(.N, .4, 10))]  \ndt_claim[ ,delay_pay := floor(10 + 40* rbeta(.N, 7,7))]  \n\ndt_claim[, date_occur := date_UW %m+% days(occ_delay_days)]\ndt_claim[, date_report := date_occur %m+% days(delay_report)]\ndt_claim[, date_pay := date_report %m+% days(delay_pay)]\n\ndt_claim[, claim_cost := round(Price * claim_sev)]\n\n# check output\nhead(dt_claim) |&gt; datatable()\n\n\n\n\n\n\n\n\nClaim key and tidy\nSimple tidying and addition of a unique claim key. Then save out the file for future use.\nThe original paper spoke of using a “competing hazards model” for simulating claims. I have taken this to mean that a policy can only give rise to one claim. Where the above process has simulated multiple claims against the same policy I keep only the first occuring claim.\n\n# add a unique claimkey based upon occurence date\n\ndt_claim[, clm_prefix := year(date_occur)*10000 + month(date_occur)*100 + mday(date_occur)]\ndt_claim[, clm_seq := seq_len(.N), by = clm_prefix]\ndt_claim[, clm_number := as.character(clm_prefix * 10000 + clm_seq)]\n\n\n# keep only first claim against policy (competing hazards)\nsetkeyv(dt_claim, c(\"pol_number\", \"clm_prefix\"))\ndt_claim[, polclm_seq := seq_len(.N), by = .(pol_number)]\ndt_claim &lt;- dt_claim[polclm_seq == 1,]\n\n# colums to keep\ncols_claim &lt;- c(\"clm_number\",\n                \"pol_number\",\n                \"claim_type\",\n                \"claim_count\",\n                \"claim_sev\",\n                \"date_occur\",\n                \"date_report\",\n                \"date_pay\",\n                \"claim_cost\")\n\ndt_claim &lt;- dt_claim[, cols_claim, with = FALSE]\n\n# check output\nhead(dt_claim) |&gt; datatable()\n\n\n\n\n\n\n\n\n\n\n13.2.3 Checking exhibits\nBaudry’s paper produced a number of summary exhibits from the simulated data. Let’s recreate them to get some comfort that we have correctly recreated the data.\nYou can see that the severity exhbit, Chart B, is inconsistent with that presented in the original paper. The cause of that difference is unclear at the time of writing. It’s likely to be because something nearer to a Beta(alpha = 5, beta = 0.05) has been used. However using that creates other discrepancies likely to be due to issues with the competing hazards implementation. For now we note the differences and continue with the data as created here.\n\n\nWarning: The dot-dot notation (`..prop..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(prop)` instead.\n\n\n\n\n\n\ndt_claim[, Days_reportdelay := as.numeric(difftime(date_report, date_occur, units=\"days\"))]\n\n\nhist(dt_claim[, Days_reportdelay],breaks = 50)\n\n\n\n\n\ndt_claim[, Days_paymentdelay := as.numeric(difftime(date_pay, date_report, units=\"days\"))]\nhist(dt_claim[, Days_paymentdelay],breaks = 60)\n\n\n\n\nThe final set of exhibits are those on slide 44. The only difference of note here is in Chart B, the Claim Rate by phone brand.\nBaudry’s exhibits show Brand 1 to have a 5% higher claim frequency than other Brands. From reading the paper I can’t see why we should expect that to be the case. Claim rate varies by phone Model but Model incidence doesn’t vary by Brand. Therefore I can’t see how the Chart B equivalent could be correct given the details in the paper.\nI leave the code as is noting the difference but recognising that it will not affect the wider aims of the paper.\n\n\n\n\n\n\n\n13.2.4 Function to create policy and claim data\nThe above code can be wrapped into a function which returns a list containing the policy and claim datasets.\n\nsimulate_central_scenario &lt;- function(seed = 1234){\n  \n  #seed = 1234  \n  set.seed(seed)\n  \n  # Policy data\n  #~~~~~~~~~~~~~~~~~\n  \n  # polices sold between start 2016 to end 2017\n  dt_policydates &lt;- data.table(date_UW = seq(as.Date(\"2016/1/1\"), as.Date(\"2017/12/31\"), \"day\"))\n  \n  # number of policies per day follows Poisson process with mean 700 (approx 255,500 pols per annum)\n  dt_policydates[, ':='(policycount = rpois(.N,700),\n                        date_lapse = date_UW %m+% years(1),\n                        expodays = as.integer(date_UW %m+% years(1) - date_UW),\n                        pol_prefix = year(date_UW)*10000 + month(date_UW)*100 + mday(date_UW))]\n  \n  \n  # Add columns defining Policy Covers   \n  dt_policydates[, Cover_B := round(policycount * 0.25)]\n  dt_policydates[, Cover_BO := round(policycount * 0.45)]\n  dt_policydates[, Cover_BOT := policycount - Cover_B - Cover_BO]\n  \n  \n  # repeat rows for each policy by UW-Date\n  dt_policy &lt;- dt_policydates[rep(1:.N, policycount),c(\"date_UW\", \"pol_prefix\"), with = FALSE][,pol_seq:=1:.N, by=pol_prefix]\n  \n  # Create a unique policy number \n  dt_policy[, pol_number := as.character(pol_prefix * 10000 + pol_seq)]\n  \n  # set join keys\n  setkey(dt_policy,'date_UW')\n  setkey(dt_policydates,'date_UW')  \n  \n  # remove pol_prefix before join\n  dt_policydates[, pol_prefix := NULL]  \n  \n  # join cover from summary file (dt_policydates)\n  dt_policy &lt;- dt_policy[dt_policydates]  \n  \n  # now create Cover field for each policy row\n  dt_policy[,Cover := 'BO']\n  dt_policy[pol_seq &lt;= policycount- Cover_BO,Cover := 'BOT']\n  dt_policy[pol_seq &lt;= Cover_B,Cover := 'B']  \n  \n  # remove interim calculation fields\n  dt_policy[, ':='(pol_prefix = NULL,\n                   policycount = NULL,\n                   pol_seq = NULL,\n                   Cover_B = NULL,\n                   Cover_BOT = NULL,\n                   Cover_BO = NULL)]\n  \n  # Add remaining policy details\n  dt_policy[, Brand := rep(rep(c(1,2,3,4), c(9,6,3,2)), length.out = .N)]\n  dt_policy[, Base_Price := rep(rep(c(600,550,300,150), c(9,6,3,2)), length.out = .N)]\n  \n  # models types and model cost multipliers\n  for (eachBrand in unique(dt_policy$Brand)) {\n    dt_policy[Brand == eachBrand, Model := rep(rep(c(3,2,1,0), c(10, 7, 2, 1)), length.out = .N)]\n    dt_policy[Brand == eachBrand, Model_mult := rep(rep(c(1.15^3, 1.15^2, 1.15^1, 1.15^0), c(10, 7, 2, 1)), length.out = .N)]\n  }\n  \n  dt_policy[, Price := ceiling (Base_Price * Model_mult)]\n  \n  \n  # colums to keep\n  cols_policy &lt;- c(\"pol_number\",\n                   \"date_UW\",\n                   \"date_lapse\",\n                   \"Cover\",\n                   \"Brand\",\n                   \"Model\",\n                   \"Price\")\n  \n  dt_policy &lt;- dt_policy[, cols_policy, with = FALSE]\n  \n  # check output\n  #head(dt_policy)\n  \n  #save(dt_policy, file = \"./dt_policy.rda\")\n  \n  \n  # Claims data\n  #~~~~~~~~~~~~~~~~~\n  \n # All policies have breakage cover\n  # claims uniformly sampled from policies\n  claim &lt;- sample(nrow(dt_policy), size = floor(nrow(dt_policy) * 0.15))\n  \n  # Claim serverity multiplier sampled from beta distn\n  dt_claim &lt;- data.table(pol_number = dt_policy[claim, pol_number],\n                         claim_type = 'B',\n                         claim_count = 1,\n                         claim_sev = rbeta(length(claim), 2,5))\n  \n  # identify all policies with Oxidation cover\n  cov &lt;- which(dt_policy$Cover != 'B')\n  \n  # sample claims from policies with cover\n  claim &lt;- sample(cov, size = floor(length(cov) * 0.05))\n  \n  # add claims to table \n  dt_claim &lt;- rbind(dt_claim,\n                    data.table(pol_number = dt_policy[claim, pol_number],\n                               claim_type = 'O',\n                               claim_count = 1,\n                               claim_sev = rbeta(length(claim), 5,3)))\n  \n  \n  # identify all policies with Theft cover\n  # for Theft claim frequency varies by Brand\n  # So need to consider each in turn...\n  \n  for(myModel in 0:3) {\n    \n    cov &lt;- which(dt_policy$Cover == 'BOT' & dt_policy$Model == myModel)\n    claim &lt;- sample(cov, size = floor(length(cov) * 0.05*(1 + myModel)))\n    \n    dt_claim &lt;- rbind(dt_claim,\n                      data.table(pol_number = dt_policy[claim, pol_number],\n                                 claim_type = 'T',\n                                 claim_count = 1,\n                                 claim_sev = rbeta(length(claim), 5,.5)))\n  }\n  \n  # set join keys\n  setkey(dt_policy, pol_number)\n  setkey(dt_claim, pol_number)\n  \n  #join Brand and Price from policy to claim\n  dt_claim[dt_policy,\n           on = 'pol_number',\n           ':='(date_UW = i.date_UW,\n                Price = i.Price,\n                Brand = i.Brand)]\n  \n  # use lubridate %m+% date addition operator \n  dt_claim[, date_lapse := date_UW %m+% years(1)]\n  dt_claim[, expodays := as.integer(date_lapse - date_UW)]\n  dt_claim[, occ_delay_days := floor(expodays * runif(.N, 0,1))]\n  \n  dt_claim[ ,delay_report := floor(365 * rbeta(.N, .4, 10))]  \n  dt_claim[ ,delay_pay := floor(10 + 40* rbeta(.N, 7,7))]  \n  \n  dt_claim[, date_occur := date_UW %m+% days(occ_delay_days)]\n  dt_claim[, date_report := date_occur %m+% days(delay_report)]\n  dt_claim[, date_pay := date_report %m+% days(delay_pay)]\n  \n  dt_claim[, claim_cost := round(Price * claim_sev)]\n  \n  dt_claim[, clm_prefix := year(date_report)*10000 + month(date_report)*100 + mday(date_report)]\n  \n  dt_claim[, clm_seq := seq_len(.N), by = clm_prefix]\n  dt_claim[, clm_number := as.character(clm_prefix * 10000 + clm_seq)]\n  \n  # keep only first claim against policy (competing hazards)\n  setkeyv(dt_claim, c(\"pol_number\", \"clm_prefix\"))\n  dt_claim[, polclm_seq := seq_len(.N), by = .(pol_number)]\n  dt_claim &lt;- dt_claim[polclm_seq == 1,]\n  \n\n  # colums to keep\n  cols_claim &lt;- c(\"clm_number\",\n                  \"pol_number\",\n                  \"claim_type\",\n                  \"claim_count\",\n                  \"claim_sev\",\n                  \"date_occur\",\n                  \"date_report\",\n                  \"date_pay\",\n                  \"claim_cost\")\n  \n  dt_claim &lt;- dt_claim[, cols_claim, with = FALSE]\n  \n  output &lt;- list()\n  output$dt_policy &lt;- dt_policy\n  output$dt_claim &lt;- dt_claim\n  \n  return(output)\n\n}\n\nBy calling the function with a seed you can simulate policy and claim datasets - which we will do in the next section."
  },
  {
    "objectID": "Literature/baudry.html#prepare-data-for-modelling",
    "href": "Literature/baudry.html#prepare-data-for-modelling",
    "title": "13  Non-parametric individual claim reserving in insurance",
    "section": "13.3 Prepare data for modelling",
    "text": "13.3 Prepare data for modelling\nIn this section we step through the process to create the underlying data structures that will be used in the machine learning reserving process, as set out in sections 2 and 3 of Baudry’s paper.\n\n13.3.1 A few words before we start\nBaudry assumes that a policy follows a Position Dependent Marked Poisson Process for which he uses the following graphical representation.\n Baudry explains the notation and database build in sections 2 and 3 of his paper. It is worth taking the time to familiarise yourself with this way of presenting reserving data as it is a different perspective on the traditional claim triangle.\nWhen I first tried to code this paper I had to re-read the approach to building the reserving database many times. Even then the overall process of getting the code to work took weeks and many iterations before it came together for me. So from personal experience I’d say it may take time to understand and follow the details of the approach. Hopefully this series of notebooks will help speed up that process of familiarisation and having made the investment of time exhibits such as the one below will become intuitive.\n\n\n\nGraphical representation of IBNR and RBNS claims.\n\n\nThe data requirements for this approach to reserving are more akin to those used in Pricing. Policy underwriting features are directly used in the reserving process requiring policy and claim records to be joined at the policy exposure period level of granularity. In fact I’d say the data requirements are equivalent to a pricing dataset with two added levels of complexity.\nThe first additional complexity is that the history of claim transactions are not aggregated over dimensions of time as they would be in Pricing. The second level of complexity is that by keeping those time dimensions, additional data relating to them may be included in the analysis that would not normally be relevant to Pricing annual policies.\nThis is easiest to explain with some examples:\n\nweather information can be included by joining on the claim occurrence time dimension which may improve IBNR forecasting\nclaim settlement features such as claim handler hours worked or claims system downtime can be included by joining on claim processing date. This, you could imagine, would help explain internal variation in claim payment patterns.\n\nThe table below summarises the reserve models Baudry builds and the classes of explanatory data features each model uses.\n\n\n\n\n\n\n\n\n\n\nFeature\nDescription\nRBNS\nIBNR Frequency\nIBNR Severity\n\n\n\n\n\\(T_{0,p}\\)\nUnderwriting features and policy underwriting date\nY\nY\nY\n\n\n\\(t_i - T_{0,p}\\)\nPolicy exposure from underwriting date \\(T_0\\) to valuation date \\(t_i\\)\nY\nY\nY\n\n\n\\(F_{t_{i,p}}\\)\nPolicy risk features at valuation date \\(t_i\\)\nY\nY\nY\n\n\n\\(E_{T_{0,p}}\\)\nExternal information at policy underwriting date \\(T_0\\)\nY\nY\nY\n\n\n\\(E_{T_{1,p}}\\)\nExternal information at claim occurrence \\(T_1\\)\nY\nN\nN\n\n\n\\(E_{T_{2,p}}\\)\nExternal information at claim reporting date \\(T_2\\)\nY\nN\nN\n\n\n\\(I_{t_{i,p}}\\)\nInternal claim related information up to valuation date \\(t_i\\)\nY\nN\nN\n\n\n\nBaudry shows how this extra data can benefit the reserving process and recognises that it is the adoption of machine learning techniques that enable us to work with larger and more granular volumes of data than we are able to with traditional chainladder reserving techniques.\n\nSimulate policy and claim data\nWe start with a simulated phone insurance policy and claim dataset. I am calling the function from Notebook 1 of this series to create the dataset. Using a fixed seed will ensure you get reproducible simulated dataset.\n\ndt_PhoneData &lt;- simulate_central_scenario(1234)\n\nWe can now inspect the returned Policy dataset and similarly inspect the returned Claim dataset.\nBoth have been created to be somewhat similar to standard policy and claim datasets that insurers would extract from their policy and claim administration systems.\n\nPolicy\n\n\n\n\n\n\n\n\n\nClaim\n\n\n\n\n\n\n\n\n\n\nJoin policy and claim data\nWe start with a simulated phone dataset policy and claim tables.\nWe wish to join claims to the appropriate policy exposure period. This will be a familiar process to pricing actuaries but may not be familiar to reserving actuaries as it is not a requirement in traditional chain ladder reserving.\nFor speed and convenience we will use the data.table::foverlaps R function, which needs the tables being joined to have common keys for policy number and time periods.\n\nsetnames(dt_policy, c('date_UW', 'date_lapse'), c('date_pol_start', 'date_pol_end'))\n  \n# set policy start and end dates in foverlap friendly format\ndt_policy[, date_pol_start:= floor_date(date_pol_start, unit= \"second\")]\ndt_policy[, date_pol_end:= floor_date(date_pol_end, unit= \"second\") - 1]\n  \n# create a dummy end claim occurrence date for foverlap\ndt_claim[, date_occur:= floor_date(date_occur, unit= \"second\")]\ndt_claim[, date_occur_end:= date_occur]\ndt_claim[, date_report:= floor_date(date_report, unit= \"second\")]\ndt_claim[, date_pay:= floor_date(date_pay, unit= \"second\")]\n  \n# set keys for claim join (by policy and dates)\nsetkey(dt_claim, pol_number, date_occur, date_occur_end)\nsetkey(dt_policy, pol_number, date_pol_start, date_pol_end)\n  \n# use foverlaps to attach claim to right occurrence period and policy\ndt_polclaim &lt;- foverlaps(dt_policy, dt_claim, type=\"any\") ## return overlap indices\ndt_polclaim[, date_occur_end := NULL]\n\nThe first few rows of the resulting table are shown below where we can see the first policy has an attached claim.\n\n\n\n\n\n\n\n\n\n13.3.1.1 Check for multi-claim policies\nIn a real world situation it is possible for policies to have multiple claims in an insurance period. In such circumstances care needs to be taken in matching policy exposure periods and claims, typically this is done by splitting a policy into sequences that stop at the date of each claim.\nOur simulated data does not have this complication, as this check shows, the maximum number of sequences is 1.\n\nsetkey(dt_polclaim, pol_number, date_pol_start)\n  \n# create 2 new cols that count how many claims against each policy\ndt_polclaim[,\n            ':='(pol_seq = seq_len(.N),\n                 pol_seq_max = .N),\n            by = c('pol_number', 'date_pol_start') ]\n  \ntable(dt_polclaim[, pol_seq_max])\n\n\n     1 \n512246 \n\n\nNot all policies have claims, resulting in NA fields in the joined dataset. To facilitate future processing we need to deal with NA fields in the joined policy and claims dataset. Missing dates are set to a long dated future point. Where there are no claims, we set claim counts and costs to zero, resulting in the following table.\n\n#set NA dates to 31/12/2999\nlst_datefields &lt;- grep(names(dt_polclaim),pattern = \"date\", value = TRUE)\n  \nfor (datefield in lst_datefields)\n  set(dt_polclaim,which(is.na(dt_polclaim[[datefield]])),datefield,as_datetime(\"2199-12-31 23:59:59 UTC\"))\n \n#set other NAs to zero (claim counts and costs)\nfor (field in c(\"claim_count\", \"claim_sev\", \"claim_cost\"))\n  set(dt_polclaim,which(is.na(dt_polclaim[[field]])),field,0)\n\n\n\n\n\n\n\n\n\n\n\n13.3.2 Timeslicing claim payments\nAlthough this paper works with individual policy and claim transactions those transactions are collated into time slices.\nBaudry selected time slices of 30 days in length starting from 01 Jan 2016 (Section 5 page 13).\n\n\n\nGraphical representation of the PDMPP.\n\n\nIn the code below, for every individual policy and claim transaction; ie row in dt_polclaim, we are creating an extra column for each possible timeslice and recording in the column the cumulative claim cost up to that time slice.\nGiven that in the simple simulated dataset we only have one claim payment for any claim, the code to do this is rather more simple than would otherwise be the case. The code below would need to be amended if there are partial claim payments.\nThis time sliced dataset becomes the source of our RBNS and IBNER datasets used in subsequent machine learning steps.\n\nlst_Date_slice &lt;- floor_date(seq(as.Date(\"2016/1/1\"), as.Date(\"2019/06/30\"), by = 30), unit= \"second\") \n\n# Time slice Policy & claims \n \nfor (i in 1:length(lst_Date_slice)){\n  dt_polclaim[date_pay&lt;= lst_Date_slice[i], paste0('P_t_', format(lst_Date_slice[i], \"%Y%m%d\")):= claim_cost]\n  set(dt_polclaim,which(is.na(dt_polclaim[[paste0('P_t_', format(lst_Date_slice[i], \"%Y%m%d\"))]])),paste0('P_t_', format(lst_Date_slice[i], \"%Y%m%d\")),0)\n}\n  \n# sort data by policynumber\nsetkey(dt_polclaim, pol_number)\n\nLooking at the data can see the output of timeslicing. You’ll need to scroll to the right of the table to see the columns labeled P_t_20160101 through to P_t_20190614.\n\n\n\n\n\n\n\n\n\n13.3.3 Creating RBNS and IBNER datasets\nBefore we create the RBNS and IBNER datasets we must pick a valuation point from the available timeslices. I’m selecting the 10th point for illustration, which is the 10th 30 day period from 01/01/2016, ie a valuation date of 27/09/2016.\n\n#_ 2.1 Set initial variables ----\n#~~~~~~~~~~~~~~~~~~~~~~~\n    \ni &lt;- valuation &lt;- 10\nt_i &lt;- lst_Date_slice[i] \ndelta &lt;- min(i, length(lst_Date_slice) - i + 1)\n\nIn a traditional approach we would be creating reserving triangles with ten 30-day duration development periods.\nAs we will see the approach adopted by Baudry, in section 3 of his paper, is somewhat different and is much closer to the sort of datasets that Pricing Actuaries are familiar with.\n\n13.3.3.1 Creating RBNS dataset\nWe start with the RBNS dataset.\nThe training data for the for the RBNS reserves will consist of all claims reported prior to the valuation date. This data has been joined to the policy exposure data to enable policy related features to be used as explanatory variables in the RBNS reserve calculation.\nA training dataset will enable us to build a model of RBNS reserve requirements at historic valuation dates. But what we really require is a view of the RBNS reserve at the valuation date. To calculate this we need to create a test dataset that contains values of the explanatory features at each future claim payment period.\nBy making model prediction for each row of the test dataset we can calculate the RBNS reserve as the sum of predicted future payments.\n\n\n13.3.3.2 RBNS dataset functions\nTo create the RBNS train and test datasets we have provided two functions RBNS_Train_ijk and RBNS_Test_ijk which take as an input the joined policy and claims dataset and return data in a format suitable for applying machine learning.\nThey are actually returning the ‘triangle’ data for all occurrence periods, i, for a specified:\n\nclaim development delay, j; and\nmodel type k\n\nFor values of k equal to 1 the model data is essentially including all claim transactions up to the last calendar period prior to the valuation date. When k is set to 2 effectively the most recent calendar period’s transactions are ignored. Although in this example only models with a k value of 1 are created, Baudry’s framework allows for multiple k value models to be built. In doing so multiple models are created, effectively using aged data which would allow an ensemble modeling approach to be used. Such use of multiple k values would be similar to a Bornhuetter-Ferguson approach to modeling.\n\n13.3.3.2.1 RBNS_Train\nThe code for creating the RBNS datasets is rather involved so detailed explanation has been skipped over here.\nInterested readers are encouraged to come back to this stage and inspect the code and Baudry’s paper once they have an overview of the wider process.\n\nRBNS_Train_ijk &lt;- function(dt_policy_claim, date_i, j_dev_period, k, reserving_dates, model_vars) {\n  \n  # # debugging\n  # #~~~~~~~~~~~~~\n  # dt_policy_claim = dt_polclaim\n  # date_i = t_i\n  # j_dev_period = 1\n  # k = 1\n  # reserving_dates = lst_Date_slice\n  # #~~~~~~~~~~~~~\n  #   \n  \n  date_i &lt;- as.Date(date_i)\n  date_k &lt;- (reserving_dates[which(reserving_dates == date_i) - k + 1])\n  date_j &lt;- (reserving_dates[which(reserving_dates == date_k) - j_dev_period])\n  \n  #i - j - k + 1 (predictor as at date)\n  date_lookup &lt;- (reserving_dates[which(reserving_dates == (date_i)) - j_dev_period -k + 1]) \n  \n  #i - k to calculate target incremental paid\n  target_lookup &lt;- (reserving_dates[which(reserving_dates == (date_i)) - k]) \n  \n  #i -k + 1 to calculate target incremental paid\n  target_lookup_next &lt;- (reserving_dates[which(reserving_dates == (date_i)) - k + 1]) \n  \n  #definition of reported but not settled\n  dt_policy_claim &lt;- dt_policy_claim[(date_report &lt;= date_lookup) & (date_pay &gt; date_lookup)] \n  \n  #simulated data assumes one payment so just need to check date paid in taget calc\n  dt_policy_claim[, ':='(date_lookup = date_lookup,\n                         delay_train = as.numeric(date_lookup - date_pol_start), #extra feature\n                         j = j_dev_period,\n                         k = k,\n                         target = ifelse(date_pay&lt;=target_lookup,0,ifelse(date_pay&lt;=target_lookup_next,claim_cost,0)))]\n  \n  return(dt_policy_claim[, model_vars, with = FALSE])\n}\n\n\n\n13.3.3.2.2 RBNS_Test\nThe code for creating the RBNS datasets is rather involved so detailed explanation has been skipped over here.\nInterested readers are encouraged to come back to this stage and inspect the code and Baudry’s paper once they have an overview of the wider process.\n\nRBNS_Test_ijk &lt;- function(dt_policy_claim, date_i,j_dev_period, k, reserving_dates, model_vars) {\n  \n  # # debugging\n  # #~~~~~~~~~~~~~\n  # dt_policy_claim = dt_polclaim\n  # date_i = t_i\n  # j_dev_period = 1\n  # k = 1\n  # reserving_dates = lst_Date_slice\n  # #~~~~~~~~~~~~~\n  #   \n  date_i &lt;- as.Date(date_i)\n  \n  #i - j - k + 1 (predictor as at date)\n  date_lookup &lt;- (reserving_dates[which(reserving_dates == (date_i))]) \n  \n  #i - k to calculate target incremental paid\n  target_lookup &lt;- (reserving_dates[which(reserving_dates == (date_i)) +j_dev_period - 1]) \n  \n  #i -k + 1 to calculate targte incremental paid  \n  target_lookup_next &lt;- (reserving_dates[which(reserving_dates == (date_i)) + j_dev_period]) \n  \n  #definition of reported but not settled\n  # P_te_RBNS rowids of policies needing an RBNS reserve\n  dt_policy_claim &lt;- dt_policy_claim[date_report &lt;= date_lookup & date_lookup &lt; date_pay] \n  \n  #model assumes one payment so just need to check date paid\n  dt_policy_claim[, ':='(date_lookup = date_lookup,\n                         delay_train = as.numeric(date_lookup - date_pol_start), #extra feature\n                         j = j_dev_period,\n                         k = k,\n                         target = ifelse(date_pay&lt;=target_lookup,0,ifelse(date_pay&lt;=target_lookup_next,claim_cost,0)))] \n  \nreturn(dt_policy_claim[, model_vars, with = FALSE])\n  \n}\n\n\n\n\n13.3.3.3 RBNS function calls\nThe RBNS train and test datasets are created by calling the RBNS_Train and RBNS_Test functions passing, as parameters to them, the names of the joined policy and claim dataset, valuation dates and model features.\nThe functions RBNS_Train and RBNS_Test iterate over valid values of j and k calling the RBNS_Train_ijk and RBNS_Test_ijk functions to create the complete train and test datasets as illustrated below.\n\n\n\n\n\n\n\n(a) Left chart, j=1, k=1\n\n\n\n\nFigure 13.1: \n\n\n\nRBNS_Train &lt;- function(dt_policy_claim, date_i, i, k, reserving_dates, model_vars) {\n# Create a combined TRAIN dataset across all k and j combos\n  for (k in 1:k){\n    if (k==1) dt_train &lt;- NULL\n    for (j in 1:(i - k + 1)){\n      dt_train &lt;- rbind(dt_train, RBNS_Train_ijk(dt_polclaim, date_i, j, k,reserving_dates, model_vars))\n    }\n  }  \n  return(dt_train)\n}\n\n\nRBNS_Test &lt;- function(dt_policy_claim, date_i, delta, k, reserving_dates, model_vars) {\n  \n  # Create a combined TEST dataset across all k and j combos\n  for (k in 1:k){\n    if (k==1) dt_test &lt;- NULL\n    for (j in 1:(delta - k + 1)){\n      dt_test &lt;- rbind(dt_test, RBNS_Test_ijk(dt_polclaim, date_i, j, k,reserving_dates, model_vars))\n    }\n  }\n  \n  return(dt_test)\n}\n\nThe animation below attempts to summarise the overall data preparation and model prediction process.\n\n\n\nFigure 13.2: Illustration of the data preparation and model prediction process\n\n\nSo having given a rough outline of the data creation process let’s now call the functions to create the RBNS datasets.\n\n#define modelVars\nRBNS_model_vars &lt;- c(\"clm_number\",\n                     \"pol_number\",\n                     \"j\",\n                     \"k\",\n                     \"date_pol_start\",\n                     \"date_occur\",\n                     \"date_report\",\n                     \"date_pay\",\n                     \"Cover\",\n                     \"claim_type\",\n                     \"Brand\",\n                     \"Model\",\n                     \"Price\",\n                     \"target\"\n    )\n\n\n# Create a combined TRAIN dataset for k = 1 and all valid j delay values\ndt_RBNS_train &lt;- RBNS_Train(dt_polclaim, t_i, i, k = 1, lst_Date_slice, RBNS_model_vars)\n\n# Create a combined TEST dataset for k = 1 and all valid j delay values\ndt_RBNS_test &lt;- RBNS_Test(dt_polclaim, t_i, delta, k = 1, lst_Date_slice, RBNS_model_vars)\n\nThe train and test datasets are then joined into a single dataset and a small amount of tidying is done to make them ready for use.\n\n# Add a flag to determine which rows are from the trainset and which from the test set\ndt_RBNS_train[, flgTrain := 1]\ndt_RBNS_test[, flgTrain := 0]\n\n# combine into a single RBNS dataset   \ndt_All_RBNS &lt;- rbind(dt_RBNS_train, dt_RBNS_test)\n\nThe important aspects of the tidying relate to creating usable delay metrics from the numerous dates and converting some character features such as cover and claim type into factors.\n\n# order and create some delay fields\nsetkey(dt_All_RBNS, clm_number, k, j)\n    \ndt_All_RBNS[, Count := .N , by =clm_number]\n\n#create delay measure by converting from seconds to day periods\ndt_All_RBNS[, ':='(\n  delay_uw_occ = ifelse(year(date_occur) == 2199,\n                        -1,\n                        ceiling((as.numeric(date_occur) - as.numeric(date_pol_start)) / (24 * 60 * 60))\n                        ),\n  delay_occ_rep = ifelse(year(date_occur) == 2199,\n                         -1,\n                         ceiling((as.numeric(date_report) - as.numeric(date_occur)) / (24 * 60 * 60))\n                         ),\n  delay_uw_val = ceiling((as.numeric(t_i) - as.numeric(date_pol_start)) / (24 * 60 * 60)),\n  delay_rep_pay = ceiling((as.numeric(date_pay) - as.numeric(date_report)) / (24 * 60 * 60)),\n  \n  date_uw = ceiling(as.numeric(date_pol_start) / (24 *  60 * 60)),\n  Cover = as.factor(Cover),\n  claim_type = as.factor(claim_type)\n  )]\n\n\n\n13.3.3.4 Creating IBNR dataset\nThe IBNR dataset creation follows a similar process to RBNS but is a little more complex. Any policy with a live exposure can give rise to an IBNR claim so the training dataset consists of all policy exposure periods prior to the valuation date.\nFrom this we train two models:\n\na frequency model to predict if there will be an IBNR claim; and\na severity model to predict the expected cost of any IBNR claim\n\nThis is very similar to the traditional pricing approach except that we can add information relating to the claim occurrence date (eg weather information could be useful for Storm losses) and we also predict the incremental run-off of the exposure period.\n\n\n13.3.3.5 IBNR dataset functions\n\n13.3.3.5.1 IBNR_Frequency Train\nThe code for creating the IBNR datasets is rather involved so detailed explanation has been skipped over here.\nInterested readers are encouraged to come back to this stage and inspect the code and Baudry’s paper once they have an overview of the wider process.\n\nIBNR_Freq_Train_ijk &lt;- function(dt_policy_claim, date_i, j_dev_period, k, reserving_dates, model_vars, verbose = FALSE) {\n  \n  # # debugging\n  # #~~~~~~~~~~~~~\n  # dt_policy_claim = dt_polclaim\n  # date_i = t_i\n  # j_dev_period = 1\n  # k = 1\n  # reserving_dates = lst_Date_slice\n  # model_vars &lt;- IBNR_model_vars\n  # #~~~~~~~~~~~~~\n  \n  date_i &lt;- as.Date(date_i)\n  date_k &lt;- (reserving_dates[which(reserving_dates == date_i) - k + 1])\n  date_j &lt;- (reserving_dates[which(reserving_dates == date_k) - j_dev_period])\n  date_lookup &lt;- (reserving_dates[which(reserving_dates == (date_i)) - j_dev_period -k + 1]) #i - j - k + 1 (predictor as at date)\n  target_lookup &lt;- (reserving_dates[which(reserving_dates == (date_i)) - k]) #i - k to calculate target incremental paid\n  target_lookup_next &lt;- (reserving_dates[which(reserving_dates == (date_i)) - k + 1]) #i -k + 1 to calculate targte incremental paid\n  \n  if(verbose) cat(paste(\"Valn date\", date_i, \", j = \", j_dev_period, \", k =\", k, \"\\n\"))\n  \n  dt_policy_claim &lt;- dt_policy_claim[date_pol_start &lt; date_lookup  & date_lookup &lt; date_report] #definition of IBNR\n  \n  dt_policy_claim[, ':='(date_lookup = date_lookup,\n                         delay_train = as.numeric(date_lookup - date_pol_start), #extra feature\n                         j = j_dev_period,\n                         k = k,\n                         exposure = round((pmin(as.numeric(as.numeric(date_pol_end)), as.numeric(floor_date(date_i, unit= \"second\")))\n                                             - as.numeric(date_pol_start))/(24*60*60*365), 3),\n                         target = ifelse(target_lookup &lt;= date_pay &  date_pay&lt; target_lookup_next & date_occur &lt;= date_lookup ,1,0))]\n  \n  dt_policy_claim &lt;- dt_policy_claim [,.(exposure = sum(exposure)), by= c(setdiff(model_vars, 'exposure')) ]\n  \n  return(dt_policy_claim[, model_vars, with = FALSE])\n  \n}\n\n\n\n13.3.3.5.2 IBNR_Loss Train\nThe code for creating the IBNR datasets is rather involved so detailed explanation has been skipped over here.\nInterested readers are encouraged to come back to this stage and inspect the code and Baudry’s paper once they have an overview of the wider process.\n\nIBNR_Loss_Train_ijk &lt;- function(dt_policy_claim, date_i, j_dev_period, k, reserving_dates, model_vars, verbose = FALSE) {\n  \n  \n  # # debugging\n  # #~~~~~~~~~~~~~\n  # dt_policy_claim = dt_polclaim\n  # date_i = t_i\n  # j_dev_period = 1\n  # k = 1\n  # reserving_dates = lst_Date_slice\n  # model_vars &lt;- IBNR_model_vars\n  # #~~~~~~~~~~~~~\n  \n  date_i &lt;- as.Date(date_i)\n  date_k &lt;- (reserving_dates[which(reserving_dates == date_i) - k + 1])\n  date_j &lt;- (reserving_dates[which(reserving_dates == date_k) - j_dev_period])\n  date_lookup &lt;- (reserving_dates[which(reserving_dates == (date_i)) - j_dev_period -k + 1]) #i - j - k + 1 (predictor as at date)\n  target_lookup &lt;- (reserving_dates[which(reserving_dates == (date_i)) - k]) #i - k to calculate target incremental paid\n  target_lookup_next &lt;- (reserving_dates[which(reserving_dates == (date_i)) - k + 1]) #i -k + 1 to calculate targte incremental paid\n  \n  if(verbose) cat(paste(\"Valn date\", date_i, \", j = \", j_dev_period, \", k =\", k, \"\\n\"))\n  \n  dt_policy_claim &lt;- dt_policy_claim[(date_lookup &lt; date_report) & (date_occur &lt; date_lookup) & (target_lookup &gt;= date_pay  & date_pay &lt; target_lookup_next)] #definition of reported but not settled\n  dt_policy_claim[, ':='(date_lookup = date_lookup,\n                         delay_train = as.numeric(date_lookup - date_pol_start), #extra feature\n                         j = j_dev_period,\n                         k = k,\n                         exposure = 1, #all claims trated equal\n                         \n                         target = ifelse(target_lookup &gt;= date_pay & date_pay &lt; target_lookup_next,claim_cost,0) #model assumes one payment so just need to check date paid\n                         \n  )]\n  \n  return(dt_policy_claim[, model_vars, with = FALSE])\n}\n\n\n\n13.3.3.5.3 IBNR Test\nThe code for creating the IBNR datasets is rather involved so detailed explanation has been skipped over here.\nInterested readers are encouraged to come back to this stage and inspect the code and Baudry’s paper once they have an overview of the wider process.\n\nIBNR_Test_ijk &lt;- function(dt_policy_claim, date_i,j_dev_period, k, reserving_dates, model_vars, verbose = FALSE) {\n  \n  ## debugging\n  ##~~~~~~~~~~~~~\n  #dt_policy_claim = dt_polclaim\n  #date_i = t_i\n  #j_dev_period = 8\n  #k = 1\n  #reserving_dates = lst_Date_slice\n  #model_vars &lt;- IBNR_model_vars\n  ##~~~~~~~~~~~~~\n  \n  date_i &lt;- as.Date(date_i)\n  date_lookup &lt;- (reserving_dates[which(reserving_dates == (date_i))]) #i - j - k + 1 (predictor as at date)\n  target_lookup &lt;- (reserving_dates[which(reserving_dates == (date_i)) +j_dev_period - 1]) #i - k to calculate target incremental paid\n  target_lookup_next &lt;- (reserving_dates[which(reserving_dates == (date_i)) + j_dev_period]) #i -k + 1 to calculate targte incremental paid  \n  \n  if(verbose) cat(paste(\"Valn date\", date_i, \", j = \", j_dev_period, \", k =\", k, \"\\n\"))\n  \n  # P_te_IBNR rowids of policies needing an RBNS reserve\n  dt_policy_claim &lt;- dt_policy_claim[date_pol_start &lt;= date_lookup & date_lookup &lt; date_report] #IBNR\n  \n  dt_policy_claim[, ':='(date_lookup = date_lookup,\n                         delay_train = as.numeric(date_lookup - date_pol_start), #extra feature\n                         j = j_dev_period,\n                         k = k,\n                         exposure = round((pmin(as.numeric(as.numeric(date_pol_end)), as.numeric(floor_date(date_i, unit= \"second\")))\n                                             - as.numeric(date_pol_start))/(24*60*60*365),3),\n                         target = ifelse(target_lookup &lt;= date_pay &  date_pay &lt; target_lookup_next & date_occur &lt;= date_lookup ,claim_cost,0))]  #model assumes one payment so just need to check date paid\n  \n  dt_policy_claim &lt;- dt_policy_claim [,.(exposure = sum(exposure)), by= c(setdiff(model_vars, 'exposure')) ]\n  \n  return(dt_policy_claim[, model_vars, with = FALSE])\n  \n}\n\n\n\n\n13.3.3.6 IBNR function calls\nThe IBNR train and test datasets are created by calling the IBNR_Train and IBNR_Test functions passing, as parameters to them, the names of the joined policy and claim dataset, valuation dates and model features.\nThe functions IBNR_Train and IBNR_Test iterate over valid values of j and k calling the IBNR_Freq_ijk, IBNR_Loss_ijk and IBNR_Test_ijk functions to create the complete train and test datasets as set out in the code below.\nThe princple and code is similar to that of RBNS, except that the training data covers both claim counts and costs.\n\nIBNR_Train &lt;- function(dt_policy_claim, date_i, i, k, reserving_dates, model_vars, verbose = FALSE) {\n\n  # Create a combined TRAIN dataset across all k and j combos\n    for (k in 1:k){\n      if (k==1){\n        dt_train_Freq &lt;- NULL\n        dt_train_Loss &lt;- NULL\n      }\n      \n      for (j in 1:(i - k + 1)){\n        dt_train_Freq &lt;- rbind(dt_train_Freq, IBNR_Freq_Train_ijk(dt_policy_claim, date_i, j, k,reserving_dates, model_vars, verbose))\n        dt_train_Loss &lt;- rbind(dt_train_Loss, IBNR_Loss_Train_ijk(dt_policy_claim, date_i, j, k,reserving_dates, model_vars, verbose))\n      }\n    }\n\n  return(list(Freq = dt_train_Freq, Loss = dt_train_Loss))\n}\n\n\nIBNR_Test &lt;- function(dt_policy_claim, date_i, delta, k, reserving_dates, model_vars, verbose = FALSE) {\n \n  # Create a combined TEST dataset across all k and j combos\n  for (k in 1:k){\n    if (k==1) dt_test &lt;- NULL\n    for (j in 1:(delta - k + 1)){\n      dt_test &lt;- rbind(dt_test, IBNR_Test_ijk(dt_policy_claim, date_i, j, k,reserving_dates, model_vars, verbose))\n    }\n  }\n  return(dt_test)\n}\n\nSo having given a rough outline of the data creation process lets now call the functions to create the IBNR datasets.\n\n#define IBNR modelVars\nIBNR_model_vars &lt;- c(\"clm_number\",\n                     \"pol_number\",\n                     \"j\",\n                     \"k\",\n                     \"exposure\",\n                     \"date_pol_start\",\n                     \"date_occur\",\n                     \"date_report\",\n                     \"date_pay\",\n                     \"Cover\",\n                     \"Brand\",\n                     \"Model\",\n                     \"Price\",\n                     \"target\")\n    \n# Create a combined TRAIN dataset for k = 1 and all valid j delay values\nlst_IBNR_train &lt;- IBNR_Train(dt_polclaim, t_i, i, k = 1,lst_Date_slice, IBNR_model_vars)\n\n# Create a combined TEST dataset for k = 1 and all valid j delay values\ndt_IBNR_test &lt;- IBNR_Test(dt_polclaim, t_i, delta, k = 1,lst_Date_slice, IBNR_model_vars)\n\nThe train and test datasets are then joined into a single dataset and a small amount of tidying is done to make them ready for use.\n\nlst_IBNR_train$Freq[, flgTrain := 1]\nlst_IBNR_train$Loss[, flgTrain := 2]\ndt_IBNR_test[, flgTrain := 0]\n\ndt_All_IBNR &lt;- rbind(lst_IBNR_train$Freq, lst_IBNR_train$Loss, dt_IBNR_test)\n\nThe important aspects of the tidying relate to creating useable delay metrics from the numerous dates and converting some character features such as cover and claim type into factors.\n\n# order and create some delay fields\nsetkey(dt_All_IBNR, clm_number, k, j)\n    \ndt_All_IBNR[, Count := .N , by =clm_number]\ndt_All_IBNR[,':='( delay_uw_occ = fifelse(year(date_occur) == 2199,\n                                        -1,\n                                        ceiling((as.numeric(date_occur) - as.numeric(date_pol_start))\n                                                  /(24*60*60))\n                                          ),\n                   delay_occ_rep = fifelse(year(date_occur) == 2199,\n                                          -1,\n                                          ceiling((as.numeric(date_report) - as.numeric(date_occur))\n                                                  /(24*60*60))\n                                          ),\n                   delay_rep_pay = fifelse(year(date_occur) == 2199,\n                                          -1,\n                                          ceiling((as.numeric(date_pay) - as.numeric(date_report))\n                                                  /(24*60*60))\n                                          ),\n                   delay_uw_val = ceiling((as.numeric(t_i) - as.numeric(date_pol_start))/(24*60*60)),\n                   date_uw = ceiling(as.numeric(date_pol_start)/(24*60*60)),\n                   Cover = as.factor(Cover))]\n\n\n\n\n13.3.4 Dataset inspection\nSo we now have two datasets;\n\ndt_ALL_RBNS for calculating RBNS reserves\n\ndt_ALL_IBNR for calculating IBNR reserves\n\nEach dataset contains both training rows and test rows. The test rows are used for model prediction; which in this case means RBNS or IBNR reserve calculation.\nIBNR reserves are calculated using a frequency * severity model approach. IBNR therefore requires two models and two training datasets. Training rows for the claim frequency model are identified as rows with a flgTrain column value of 1. Training rows for the claim severity model are identified as rows with a flgTrain column value of 2.\nIn both datsets the test rows are identified as rows with a flgTrain column value of 0.\nLet’s have a quick look at the datasets.\n\n13.3.4.1 RBNS\nThe RBNS dataset has 21 columns and 43,596 rows.\nIn a traditional reserving exercise this would have been summarised as a 10 x 10 triangle ie 50 rows. We have far more rows of data for 3 main reasons;\n\ntraining data is presented without aggregation so there is a row for each one of the 14,878 claims that have been reported up until 2016-09-27\nthe machine learning training dataset is not just the latest triangle of data as at the valuation date. It is also every possible historic triangle prior to the valuation date as illustrated in the animation above.\nthe dataset also contains test rows ie the features needed to predict each future period from the current valuation date\n\n\n\n\n\n\n\n\n\n\n13.3.4.2 IBNR\nThe IBNR dataset has 21 columns and 2,477,886 rows.\nIn a traditional reserving exercise this would have been summarised as a 10 x 10 triangle ie 50 rows. We have far more rows of data and far more that we did for the RBNS dataset.\nAgain the the same 3 principles apply to the IBNR row count. However this will lead to many more rows of data because we are training two models and the claim frequency model will require a row for every past policy exposure period and of course there should be orders of magnitude more exposure rows than claim rows!\n\n\n\n\n\n\n\n\n\n\n13.3.5 Summary\nI’ve deliberately rushed through the creation of the datasets so we can see the end output shown above. Interested readers are encourage to revisit and review the RBNS and IBNER data creation process to gain a deeper understanding. In doing so they will be better placed to adapt the code provided to their own circumstances.\nThe above code can be wrapped into a series of functions which, given a joined policy and claim dataset and a valuation date, will return the reserving datasets needed for machine learning. In notebook 3 of this series we will create and use such functions prior to fitting a machine learning model to the training data and then use it to make reserve predictions."
  },
  {
    "objectID": "Literature/baudry.html#modelling-and-analysing-the-data",
    "href": "Literature/baudry.html#modelling-and-analysing-the-data",
    "title": "13  Non-parametric individual claim reserving in insurance",
    "section": "13.4 Modelling and analysing the data",
    "text": "13.4 Modelling and analysing the data\nIn this section we step through the process to apply machine learning techniques in order to create reserve estimates following the techniques set out in sections 3 and 4 of Baudry’s paper.\n\n13.4.1 Model build\nI will use the xgboost R machine learning package to build the models. In Baudry’s original paper he used the extraTrees package; a variant upon RandomForests.\nXgboost has been selected as it is the most popular implementation of the gradient boosting machine (GBM) algorithm. GBMs have been known since around 2015 to be the most accurate algorithmic technique for regression and classification tasks.\nThe code and modelling process for each reserve type is very similar so they are shown in the tabbed sections below rather that repeating in-line.\n\n13.4.1.1 RBNS model\nStarting with the RBNS reserves we will first build a model upon the training data using a cross validated approach to enable selection of key xgboost hyperparameters.\nThen using optimal parameters we retrain on the complete training dataset. This trained model is then scored against the test data in order to create predictions from which the RBNS reserve can be calculated.\n\n13.4.1.1.1 Creating xgboost dataset\nXgboost requires its data to be in a specific format; a dense matrix form called a DMatrix, for which there is a function xgb.DMatrix. Not all variable types can be passed to xgb.DMatrix in particular categorical or nominal variables such as Brand have to be converted from text to numeric values.\nFor this example I’ve chosen to use the parsnip package and create a recipe that converts nominal predictor values into a one-hot-encoded form.\n\nRBNS_predictors &lt;- c(\"j\",\n                     \"k\",\n                     \"Cover\",\n                     \"claim_type\",\n                     \"Brand\",\n                     \"Model\",\n                     \"Price\",\n                     #\"date_uw\",\n                     #\"delay_uw_occ\",\n                     \"delay_occ_rep\")\n\nrowList_RBNS &lt;- list(train=dt_All_RBNS[, which(flgTrain==1)],\n                test=dt_All_RBNS[, which(flgTrain==0)],\n                all = 1:nrow(dt_All_RBNS))\n\n\nRBNS_rec &lt;- recipe( ~ ., data = dt_All_RBNS[, RBNS_predictors, with = FALSE])  |&gt; \n  step_dummy(all_nominal(), one_hot = TRUE) %&gt;%\n  prep()\n\n\ndf.RBNS_train &lt;- bake(RBNS_rec, new_data = dt_All_RBNS[rowList_RBNS$train,] )\ndf.RBNS_test &lt;- bake(RBNS_rec, new_data = dt_All_RBNS[rowList_RBNS$test,] )\ndf.RBNS_all &lt;- bake(RBNS_rec, new_data = dt_All_RBNS )\n\n\nxgb.RBNS_DMat.train &lt;- xgb.DMatrix(data = as.matrix(df.RBNS_train),\n                              label = dt_All_RBNS[rowList_RBNS$train, target])\n\nxgb.RBNS_DMat.test &lt;- xgb.DMatrix(data = as.matrix(df.RBNS_test),\n                              label = dt_All_RBNS[rowList_RBNS$test, target])\n\nxgb.RBNS_DMat.all &lt;- xgb.DMatrix(data = as.matrix(df.RBNS_all),\n                             label = dt_All_RBNS[, target])\n\n\n\n13.4.1.1.2 Fit initial model using cross validation\nHaving prepared the data for xgboost I can now fit an initial model. I’ve used a simple set of parameters and used cross validation to select the optimal number of boosted trees (nrounds) for these parameter selections by calling the xgb.cv function with early stopping.\nI have used the reg:tweedie objective function based upon inspection of the target variable.\n\nsummary(dt_All_RBNS[rowList_RBNS$train, target])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    0.00   76.44    0.00  913.00 \n\nhist(dt_All_RBNS[rowList_RBNS$train, target])\n\n\n\n\nIn a real world example a process of parameter tuning would be undertaken in order to select more optimal parameters.\n\nparam &lt;- list(\n  objective = \"reg:tweedie\",\n  max_depth = 2L,            # tree-depth\n  subsample = 0.75,          # randomly sample rows before fitting each tree\n  colsample_bytree = 0.8,    # randomly sample columns before fitting each tree\n  min.child.weight = 10,     # minimum weight per leaf\n  eta = 0.1                  # Learning rate\n)\n\n\n# Train model with cross validation\nset.seed(1984) # for repeatability\n\nxgb_RBNS_CV &lt;- xgb.cv(\n  params                 = param,\n  data                   = xgb.RBNS_DMat.train,\n  nrounds                = 500,        # Maximum number of trees to build\n  nfold = 5,\n  early_stopping_rounds  = 10L,        # Stops algorithm early if performance has not improved in \n  print_every_n          = 10L,        # How often to print to console\n  prediction             = TRUE        # Keeps the predictions\n)\n\n[1] train-tweedie-nloglik@1.5:197.388756+1.296594   test-tweedie-nloglik@1.5:197.391244+4.609216 \nMultiple eval metrics are present. Will use test_tweedie_nloglik@1.5 for early stopping.\nWill train until test_tweedie_nloglik@1.5 hasn't improved in 10 rounds.\n\n[11]    train-tweedie-nloglik@1.5:75.924330+0.469210    test-tweedie-nloglik@1.5:75.942832+1.753744 \n[21]    train-tweedie-nloglik@1.5:34.531029+0.276990    test-tweedie-nloglik@1.5:34.546202+0.679007 \n[31]    train-tweedie-nloglik@1.5:22.710654+0.268914    test-tweedie-nloglik@1.5:22.724562+0.308235 \n[41]    train-tweedie-nloglik@1.5:20.129801+0.104845    test-tweedie-nloglik@1.5:20.152850+0.320732 \n[51]    train-tweedie-nloglik@1.5:19.632863+0.090769    test-tweedie-nloglik@1.5:19.671306+0.309964 \n[61]    train-tweedie-nloglik@1.5:19.500483+0.086323    test-tweedie-nloglik@1.5:19.538232+0.308355 \n[71]    train-tweedie-nloglik@1.5:19.444729+0.082132    test-tweedie-nloglik@1.5:19.493532+0.308265 \n[81]    train-tweedie-nloglik@1.5:19.414329+0.078574    test-tweedie-nloglik@1.5:19.475748+0.309372 \n[91]    train-tweedie-nloglik@1.5:19.398779+0.078291    test-tweedie-nloglik@1.5:19.469874+0.307553 \n[101]   train-tweedie-nloglik@1.5:19.387133+0.077247    test-tweedie-nloglik@1.5:19.467463+0.308378 \n[111]   train-tweedie-nloglik@1.5:19.375232+0.076711    test-tweedie-nloglik@1.5:19.460278+0.304426 \n[121]   train-tweedie-nloglik@1.5:19.362611+0.075821    test-tweedie-nloglik@1.5:19.464628+0.312824 \nStopping. Best iteration:\n[116]   train-tweedie-nloglik@1.5:19.368173+0.075742    test-tweedie-nloglik@1.5:19.459806+0.308833\n\n\nHaving fit the model we store the out-of-fold predictions.\n\ndt_All_RBNS[rowList_RBNS$train, preds_oof := xgb_RBNS_CV$pred]\n\n\n\n13.4.1.1.3 Fit final model on all training data\nHaving fit the model using 5 fold cross validation we observe the optimum number of fitting rounds to be 116.\nWe can then use this to train a final model on all the data.\n\nxgb_RBNS_Fit &lt;- xgb.train(\n  params                 = param,\n  data                   = xgb.RBNS_DMat.train,\n  nrounds                = xgb_RBNS_CV$best_iteration,\n# base_score             = 1,\n  watchlist              = list(train=xgb.RBNS_DMat.train, test=xgb.RBNS_DMat.test) ,\n  print_every_n          = 10\n)\n\n[1] train-tweedie-nloglik@1.5:197.153822    test-tweedie-nloglik@1.5:82.790522 \n[11]    train-tweedie-nloglik@1.5:75.846599 test-tweedie-nloglik@1.5:32.097264 \n[21]    train-tweedie-nloglik@1.5:34.402921 test-tweedie-nloglik@1.5:14.722707 \n[31]    train-tweedie-nloglik@1.5:22.522187 test-tweedie-nloglik@1.5:9.660561 \n[41]    train-tweedie-nloglik@1.5:20.075724 test-tweedie-nloglik@1.5:8.625063 \n[51]    train-tweedie-nloglik@1.5:19.604785 test-tweedie-nloglik@1.5:8.390317 \n[61]    train-tweedie-nloglik@1.5:19.486131 test-tweedie-nloglik@1.5:8.323290 \n[71]    train-tweedie-nloglik@1.5:19.439339 test-tweedie-nloglik@1.5:8.290276 \n[81]    train-tweedie-nloglik@1.5:19.412141 test-tweedie-nloglik@1.5:8.275901 \n[91]    train-tweedie-nloglik@1.5:19.391867 test-tweedie-nloglik@1.5:8.265842 \n[101]   train-tweedie-nloglik@1.5:19.379859 test-tweedie-nloglik@1.5:8.260251 \n[111]   train-tweedie-nloglik@1.5:19.370579 test-tweedie-nloglik@1.5:8.260420 \n[116]   train-tweedie-nloglik@1.5:19.367158 test-tweedie-nloglik@1.5:8.260664 \n\ndt_All_RBNS[, preds_full := predict(xgb_RBNS_Fit,xgb.RBNS_DMat.all)]\n\n\n\n13.4.1.1.4 Inspect model fit\nHaving fitted the full model we can then inspect the model fit. The traditional way of inspecting global model feature importance is to use the gain chart.\n\n#default feature importance by gain\nfeatImp_RBNS &lt;- xgb.importance(xgb_RBNS_Fit, feature_names = colnames(xgb.RBNS_DMat.train))\nxgb.plot.importance(featImp_RBNS, main=\"Feature Importance - RBNS\")\n\n\n\n\nAn increasingly popular and more robust approach is to use SHAP values https://github.com/slundberg/shap. The SHAP equivalent of the feature importance chart is shown below.\n\n# Return the SHAP values and ranked features by mean|SHAP|\nshap_values &lt;- shap.values(xgb_model = xgb_RBNS_Fit, X_train = as.matrix(df.RBNS_train))\n\n# Prepare the long-format data:\nshap_long &lt;- shap.prep(shap_contrib = shap_values$shap_score, X_train =  as.matrix(df.RBNS_train))\n\n# **SHAP summary plot**\nshap.plot.summary(shap_long, dilute = nrow(df.RBNS_train)/10000)\n\n\n\n\nA second useful chart is the partial dependency plot. This shows how the values of a predictive a feature influence the predicted value, while holding all other values constant.\nHere we show the marginal plots for the top SHAP features.\n\nfig_list &lt;- lapply(names(shap_values$mean_shap_score)[1:4], \n                   shap.plot.dependence,\n                   data_long = shap_long,\n                   dilute = nrow(shap_long)/ 10000)\n\nwrap_plots(fig_list, ncol = 2)\n\n\n\n\nThe feature importance and partial dependency plots provide quick insight into the model.\n\nWe see that claim development period, j is the most important feature and that the RBNS reserve is smaller for larger values of j.\nThe second most important feature is claimtype where breakage claims are associated with lower RBNS reserves. This is as expected from the data generating process where breakage claims come from a Beta distribution with a lower mean.\nThe third most important feature is phone price where there is linear increase in RBNS reserve with increasing phone price. This is also as expected from the data generating process.\nThe fourth feature is phone brand which again follows expectations from the data generating process.\n\nSHAP values can also be used to show the components of a single prediction. In the following plot we show the top 4 components for each row of the data and zoom in at row 500.\n\n# choose to show top 4 features by setting `top_n = 4`, set 6 clustering groups.  \nplot_data &lt;- shap.prep.stack.data(shap_contrib = shap_values$shap_score,\n                                  data_percent = 10000/nrow(shap_long),\n                                  top_n = 4,\n                                  n_groups = 6)\n  \n# choose to zoom in at location 500, set y-axis limit using `y_parent_limit`  \n# it is also possible to set y-axis limit for zoom-in part alone using `y_zoomin_limit`  \nshap.plot.force_plot(plot_data, zoom_in_location = 500, y_parent_limit = c(-1,1))\n\n\n\n\n\n\n13.4.1.1.5 Summarising RBNS reserves\nBy comparing the model predictions to the simulated claims run-off we can get a feel for the accuracy of the machine learning approach. Below I aggregate the RBNS predictions by claim occurrence month and compare then to the known simulated claim run-off.\n\ndt_All_RBNS [, date_occur_YYYYMM := as.character(year(date_occur) + month(date_occur)/100 )]\n\ndt_RBNS_summary &lt;- dt_All_RBNS[rowList_RBNS$test,.(preds = sum(preds_full), target = sum(target)), keyby = date_occur_YYYYMM]\n\n\n\n\n\n\n\n\nYou can now jump back up to the beginning of the modeling section and select the IBNR frequency modeling tab.\n\n\n\n13.4.1.2 IBNR Frequency model\nIBNR reserves are built by multiplying the outpts of a claim frequency and claim severity model. The general process of building the model follows that of the RBNS reserves.\n\n13.4.1.2.1 Creating xgboost dataset\nAgain we convert the data into a dense matrix form called a DMatrix. However before doing so we aggregate the data for efficiency of model fit times. There could be a loss of accuracy in doing, so in practice this is something you would want to experiment with.\nThe other point to note is the values of flgTrain used to identify the training and test rows in our dataset. Recall from Notebook 2, in the IBNR dataset training rows for the frequency model have a flgtrain value of 1 whereas the Severity training rows have a value of 2.\n\nIBNR_predictors &lt;- c(\"j\",\n                     \"k\",\n                     \"Cover\",\n                     \"Brand\",\n                     \"Model\",\n                     \"Price\",\n                     \"date_uw\")\n\n# aggregate the data ... does this lead to loss of variance and accuracy?\n\ndt_All_IBNR [, date_pol_start_YYYYMM := as.character(year(date_pol_start) + month(date_pol_start)/100 )]\n\ndt_All_IBNR_F &lt;- dt_All_IBNR[, .(exposure = sum(exposure),\n                                   target_cost = sum(target),\n                                   target_count = sum(target&gt;0)),\n                               by= c(IBNR_predictors, \"date_pol_start_YYYYMM\", \"flgTrain\")]\n\ndt_All_IBNR_F &lt;- dt_All_IBNR_F[exposure&gt;0]\n\n\n# setup train and test rows\nrowList_IBNR_F &lt;- list(train=dt_All_IBNR_F[, which(flgTrain==1)],\n                     test=dt_All_IBNR_F[, which(flgTrain==0)],\n                     all = dt_All_IBNR_F[, which(flgTrain!=2)])\n\n# setup data for xgboost\n\nIBNR_rec &lt;- recipe( ~ ., data = dt_All_IBNR_F[, IBNR_predictors, with = FALSE]) %&gt;%\n  step_dummy(all_nominal(), one_hot = TRUE) %&gt;%\n  prep()\n\ndf.IBNR_F_train &lt;- bake(IBNR_rec, new_data = dt_All_IBNR_F[rowList_IBNR_F$train,] )\ndf.IBNR_F_test &lt;- bake(IBNR_rec, new_data = dt_All_IBNR_F[rowList_IBNR_F$test,] )\ndf.IBNR_F_all &lt;- bake(IBNR_rec, new_data = dt_All_IBNR_F[rowList_IBNR_F$all,] )\n\nxgb.IBNR_F_DMat.train &lt;- xgb.DMatrix(data = as.matrix(df.IBNR_F_train),\n                              weight = dt_All_IBNR_F[rowList_IBNR_F$train, exposure],\n                              label = dt_All_IBNR_F[rowList_IBNR_F$train, target_count])\n\nxgb.IBNR_F_DMat.test &lt;- xgb.DMatrix(data = as.matrix(df.IBNR_F_test),\n                             weight = dt_All_IBNR_F[rowList_IBNR_F$test, exposure],\n                             label = dt_All_IBNR_F[rowList_IBNR_F$test, target_count])\n\nxgb.IBNR_F_DMat.all &lt;- xgb.DMatrix(data = as.matrix(df.IBNR_F_all),\n                            weight = dt_All_IBNR_F[rowList_IBNR_F$all, exposure],\n                            label = dt_All_IBNR_F[rowList_IBNR_F$all, target_count])\n\n\n\n13.4.1.2.2 Fit initial model using cross validation\nHaving prepared the data for xgboost we now need to select xgboost hyperparameter and fit an initial model.\nI have used the count:poisson objective function based upon inspection of the target variable.\n\nhist(dt_All_IBNR_F[rowList_IBNR_F$train, target_count])\n\n\n\n\n\nparam &lt;- list(\n  objective = \"count:poisson\",\n  max_depth = 2L,           # tree-depth\n  subsample = 0.7,          # randomly sample rows before fitting each tree\n  colsample_bytree = 0.8,   # randomly sample columns before fitting each tree\n  min.child.weight = 10,    # minimum weight per leaf\n  eta = 0.1               # Learning rate\n  #monotone_constraints = monotone_Vec # Monotonicity constraints\n)\n\n\n# Train model with cross validation\nset.seed(1984) # for repeatability\n\nxgb_IBNR_F_CV &lt;- xgb.cv(\n params                 = param,\n data                   = xgb.IBNR_F_DMat.train,\n nrounds                = 2000,        # Maximum number of trees to build\n nfold = 5,\n\n early_stopping_rounds  = 50L,        # Stops algorithm early if performance has not improved in n rounds\n print_every_n          = 50L,        # How often to print to console\n #base_score             = 0.001,       # Model starting point\n   prediction             = TRUE        # Keeps the predictions\n)\n\n[1] train-poisson-nloglik:0.509649+0.000442 test-poisson-nloglik:0.509662+0.001670 \nMultiple eval metrics are present. Will use test_poisson_nloglik for early stopping.\nWill train until test_poisson_nloglik hasn't improved in 50 rounds.\n\n[51]    train-poisson-nloglik:0.162211+0.002027 test-poisson-nloglik:0.162435+0.005655 \n[101]   train-poisson-nloglik:0.136555+0.002231 test-poisson-nloglik:0.137046+0.007991 \n[151]   train-poisson-nloglik:0.132412+0.002202 test-poisson-nloglik:0.133890+0.008357 \n[201]   train-poisson-nloglik:0.130635+0.002154 test-poisson-nloglik:0.133224+0.008607 \n[251]   train-poisson-nloglik:0.129481+0.002143 test-poisson-nloglik:0.132928+0.008833 \n[301]   train-poisson-nloglik:0.128445+0.002096 test-poisson-nloglik:0.132870+0.008822 \n[351]   train-poisson-nloglik:0.127596+0.002078 test-poisson-nloglik:0.132839+0.008840 \n[401]   train-poisson-nloglik:0.126853+0.002010 test-poisson-nloglik:0.132943+0.008736 \nStopping. Best iteration:\n[354]   train-poisson-nloglik:0.127535+0.002085 test-poisson-nloglik:0.132833+0.008836\n\n\nHaving fit the model we store the out-of-fold predictions for both the claim frequency and the claim counts.\n\n dt_All_IBNR_F[rowList_IBNR_F$train, preds_oof_IBNR_F := xgb_IBNR_F_CV$pred]\n dt_All_IBNR_F[rowList_IBNR_F$train, preds_oof_IBNR_Nos := exposure * preds_oof_IBNR_F]\n\n\n\n13.4.1.2.3 Fit final model on all training data\nHaving fit the model using 5 fold cross validation we observe the optimum number of fitting rounds to be 354.\nWe can then us this to train a final model on all the data.\n\nxgb_IBNR_F_Fit &lt;- xgb.train(\n   params                 = param,\n   data                   = xgb.IBNR_F_DMat.train,\n   nrounds                = xgb_IBNR_F_CV$best_iteration,\n# base_score             = 1,\n   watchlist              = list(train=xgb.IBNR_F_DMat.train, test=xgb.IBNR_F_DMat.test) ,\n   print_every_n          = 50\n )\n\n[1] train-poisson-nloglik:0.509641  test-poisson-nloglik:0.497402 \n[51]    train-poisson-nloglik:0.162953  test-poisson-nloglik:0.121425 \n[101]   train-poisson-nloglik:0.136572  test-poisson-nloglik:0.087977 \n[151]   train-poisson-nloglik:0.132644  test-poisson-nloglik:0.084498 \n[201]   train-poisson-nloglik:0.130981  test-poisson-nloglik:0.083343 \n[251]   train-poisson-nloglik:0.129897  test-poisson-nloglik:0.082925 \n[301]   train-poisson-nloglik:0.129155  test-poisson-nloglik:0.082865 \n[351]   train-poisson-nloglik:0.128418  test-poisson-nloglik:0.083072 \n[354]   train-poisson-nloglik:0.128371  test-poisson-nloglik:0.083107 \n\n dt_All_IBNR_F[rowList_IBNR_F$all, preds_full_IBNR_Nos := predict(xgb_IBNR_F_Fit,xgb.IBNR_F_DMat.all)]\n\n\n\n13.4.1.2.4 Inspect model fit\nHaving fitted the full model we can then inspect the model fit. The traditional way of inspecting global model feature importance is to use gain.\n\n#default feature importance by gain\nfeatImp_IBNR_F &lt;- xgb.importance(xgb_IBNR_F_Fit, feature_names = colnames(xgb.IBNR_F_DMat.train))\nxgb.plot.importance(featImp_IBNR_F, main=\"Feature Importance - IBNR Frequency\")\n\n\n\n\nAn increasingly popular and more robust approach is to use SHAP values https://github.com/slundberg/shap. The SHAP equivalent of the feature importance chart is shown below.\n\n# Return the SHAP values and ranked features by mean|SHAP|\nshap_values &lt;- shap.values(xgb_model = xgb_IBNR_F_Fit, X_train = as.matrix(df.IBNR_F_train))\n\n# Prepare the long-format data:\nshap_long &lt;- shap.prep(shap_contrib = shap_values$shap_score, X_train =  as.matrix(df.IBNR_F_train))\n\n# **SHAP summary plot**\nshap.plot.summary(shap_long, dilute = nrow(df.IBNR_F_train)/10000)\n\n\n\n\nA second useful chart is the partial dependency plot. This shows how the values of a predictive a feature influence the predicted value, while holding all other values constant.\nHere we show the SHAP equivalent of partial dependency plots; marginal plots for the top SHAP features.\n\nfig_list &lt;- lapply(names(shap_values$mean_shap_score)[1:4], \n                   shap.plot.dependence,\n                   data_long = shap_long,\n                   dilute = nrow(shap_long)/ 10000)\n\nwrap_plots(fig_list, ncol = 2)\n\n\n\n\nThe feature importance and partial dependency plots provide quick insight into the model.\n\nWe see that claim development period, j is the most important feature and that the IBNR frequency is smaller for larger values of j.\nThe second most important feature is phone price with the IBNR claim count increasing as price increases. Although this is not a direct feature in the data generating process there is a link with higher theft frequencies and higher phone prices.\nThe third most important feature is phone model with IBNR frequency increasing with model type. This is expected from the data generating process as theft claim frequency increases with model type.\nThe fourth feature is phone cover and it should be no suprise to see that Breakage only, the most basic cover level is associated with lower IBNR claim counts.\n\nSHAP values can also be used to show the components of a single predction. In the following plot we show the top 4 components for each row of the data and zoom in at row 500.\n\n# choose to show top 4 features by setting `top_n = 4`, set 6 clustering groups.  \nplot_data &lt;- shap.prep.stack.data(shap_contrib = shap_values$shap_score,\n                                  data_percent = 10000/nrow(shap_long),\n                                  top_n = 4,\n                                  n_groups = 6)\n  \n# choose to zoom in at location 500, set y-axis limit using `y_parent_limit`  \n# it is also possible to set y-axis limit for zoom-in part alone using `y_zoomin_limit`  \nshap.plot.force_plot(plot_data, zoom_in_location = 500, y_parent_limit = c(-1,1))\n\n\n\n\n\n\n13.4.1.2.5 Summarising IBNR claim counts\nAgain we convert the data into a dense matrix form called a DMatrix. However before doing so we aggregate the data for efficient of model fit times. There could be a loss of accuracy in doing, so in practice this is something you’d want to experiment with.\n\ndt_All_IBNR_F_summary &lt;- dt_All_IBNR_F[rowList_IBNR_F$test,.(preds = sum(preds_full_IBNR_Nos), target = sum(target_count)), keyby = date_pol_start_YYYYMM]\n\n\n\n\n\n\n\n\nYou can now jump back up to the beginning of the modeling section and select the IBNR severity modeling tab.\n\n\n\n13.4.1.3 IBNR Severity model\nIBNR reserves are built by multiplying the outputs of a claim frequency and claim severity model. The general process of building the model follows that of the RBNS reserves.\n\n13.4.1.3.1 Creating xgboost dataset\nAgain we convert the data into a dense matrix form called a DMatrix.\nThe other point to note is the values of flgTrain used to identify the training and test rows in our dataset. Recall from Notebook 2, in the IBNR dataset training rows for the frequency model have a flgtrain value of 1 whereas the Severity training roes have a value of 2.\n\nIBNR_predictors &lt;- c(\"j\",\n                     \"k\",\n                     \"Cover\",\n                     \"Brand\",\n                     \"Model\",\n                     \"Price\",\n                     \"date_uw\")\n\ndt_All_IBNR [, date_pol_start_YYYYMM := as.character(year(date_pol_start) + month(date_pol_start)/100 )]\n\ndt_All_IBNR_S &lt;- dt_All_IBNR[, .(exposure = sum(target&gt;0),\n                                 target_cost = sum(target)),\n                            by= c(IBNR_predictors, \"date_pol_start_YYYYMM\", \"flgTrain\")]\n\ndt_All_IBNR_S &lt;- dt_All_IBNR_S[exposure&gt;0]\n\n# setup train and test rows\nrowList_IBNR_S &lt;- list(train=dt_All_IBNR_S[, which(flgTrain==2)],\n                     test=dt_All_IBNR_S[, which(flgTrain==0)],\n                     all = dt_All_IBNR_S[, which(flgTrain!=1)])\n\n# setup data for xgboost\n\nIBNR_rec &lt;- recipe( ~ ., data = dt_All_IBNR_S[, IBNR_predictors, with = FALSE]) %&gt;%\n  step_dummy(all_nominal(), one_hot = TRUE) %&gt;%\n  prep()\n\ndf.IBNR_S_train &lt;- bake(IBNR_rec, new_data = dt_All_IBNR_S[rowList_IBNR_S$train,] )\ndf.IBNR_S_test &lt;- bake(IBNR_rec, new_data = dt_All_IBNR_S[rowList_IBNR_S$test,] )\ndf.IBNR_S_all &lt;- bake(IBNR_rec, new_data = dt_All_IBNR_S[rowList_IBNR_S$all,] )\n\n\n\nxgb.IBNR_S_DMat.train &lt;- xgb.DMatrix(data = as.matrix(df.IBNR_S_train),\n                              weight = dt_All_IBNR_S[rowList_IBNR_S$train, exposure],\n                              label = dt_All_IBNR_S[rowList_IBNR_S$train, target_cost])\n\nxgb.IBNR_S_DMat.test &lt;- xgb.DMatrix(data = as.matrix(df.IBNR_S_test),\n                             weight = dt_All_IBNR_S[rowList_IBNR_S$test, exposure],\n                             label = dt_All_IBNR_S[rowList_IBNR_S$test, target_cost])\n\nxgb.IBNR_S_DMat.all &lt;- xgb.DMatrix(data = as.matrix(df.IBNR_S_all),\n                            weight = dt_All_IBNR_S[rowList_IBNR_S$all, exposure],\n                            label = dt_All_IBNR_S[rowList_IBNR_S$all, target_cost])\n\n\n\n13.4.1.3.2 Fit initial model using cross validation\nHaving prepared the data for xgboost.\nI have used the reg:gamma objective function based upon inspection of the target variable.\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    3.0   125.5   255.0   353.9   486.5  2011.0 \n\n\n\n\n\n\nparam &lt;- list(\n  objective = \"reg:gamma\",\n  max_depth = 2L,           # tree-depth\n  subsample = 0.7,          # randomly sample rows before fitting each tree\n  colsample_bytree = 0.8,   # randomly sample columns before fitting each tree\n  min.child.weight = 10,    # minimum weight per leaf\n  eta = 0.1               # Learning rate\n  #monotone_constraints = monotone_Vec # Monotonicity constraints\n)\n\n# Train model with cross validation\nset.seed(1984) # for repeatability\n\nxgb_IBNR_S_CV &lt;- xgb.cv(\n params                 = param,\n data                   = xgb.IBNR_S_DMat.train,\n nrounds                = 2000,        # Maximum number of trees to build\n nfold = 5,\n\n early_stopping_rounds  = 50L,        # Stops algorithm early if performance has not improved in n rounds\n print_every_n          = 50L,        # How often to print to console\n #base_score             = 0.001,       # Model starting point\n   prediction             = TRUE        # Keeps the predictions\n)\n\n[1] train-gamma-nloglik:725.773272+11.678322    test-gamma-nloglik:724.944332+48.085563 \nMultiple eval metrics are present. Will use test_gamma_nloglik for early stopping.\nWill train until test_gamma_nloglik hasn't improved in 50 rounds.\n\n[51]    train-gamma-nloglik:10.028557+0.083004  test-gamma-nloglik:10.030530+0.394223 \n[101]   train-gamma-nloglik:6.820644+0.017224   test-gamma-nloglik:6.827847+0.071690 \n[151]   train-gamma-nloglik:6.810497+0.017329   test-gamma-nloglik:6.825213+0.071157 \n[201]   train-gamma-nloglik:6.805045+0.017435   test-gamma-nloglik:6.825345+0.071623 \nStopping. Best iteration:\n[160]   train-gamma-nloglik:6.809366+0.017465   test-gamma-nloglik:6.825139+0.071178\n\n\nHaving fitted an initial model the out-of-fold predictions are stored.\n\n dt_All_IBNR_S[rowList_IBNR_S$train, preds_oof_IBNR_S := xgb_IBNR_S_CV$pred]\n dt_All_IBNR_S[rowList_IBNR_S$train, preds_oof_IBNR_Cost := exposure * preds_oof_IBNR_S]\n\n\n\n13.4.1.3.3 Fit final model on all training data\nHaving fit the model using 5 fold cross validation we observe the optimum number of fitting rounds to be 160.\nWe can then us this to train a final model on all the data.\n\nxgb_IBNR_S_Fit &lt;- xgb.train(\n   params                 = param,\n   data                   = xgb.IBNR_S_DMat.train,\n   nrounds                = xgb_IBNR_S_CV$best_iteration,\n# base_score             = 1,\n   watchlist              = list(train=xgb.IBNR_F_DMat.train, test=xgb.IBNR_F_DMat.test) ,\n   print_every_n          = 50\n )\n\n[1] train-gamma-nloglik:-0.520863   test-gamma-nloglik:-0.546764 \n[51]    train-gamma-nloglik:4.166508    test-gamma-nloglik:4.165525 \n[101]   train-gamma-nloglik:5.646629    test-gamma-nloglik:5.654574 \n[151]   train-gamma-nloglik:5.624110    test-gamma-nloglik:5.584954 \n[160]   train-gamma-nloglik:5.620330    test-gamma-nloglik:5.573002 \n\n\nHaving trained the model the predictions are stored.\n\n dt_All_IBNR_S[rowList_IBNR_S$all, preds_full_IBNR_Cost := predict(xgb_IBNR_S_Fit,xgb.IBNR_S_DMat.all)]\n\n\n\n13.4.1.3.4 Inspect model fit\nHaving fitted the full model we can then inspect the model fit. The traditional way of inspecting global model feature importance is to use the gains chart.\n\n#default feature importance by gain\nfeatImp_IBNR_S &lt;- xgb.importance(xgb_IBNR_S_Fit, feature_names = colnames(xgb.IBNR_S_DMat.train))\nxgb.plot.importance(featImp_IBNR_S, main=\"Feature Importance - IBNR Severity\")\n\n\n\n\nAn increasingly popular and more robust approach is to use SHAP values https://github.com/slundberg/shap. The SHAP equivalent of the feature importance chart is shown below.\n\n# Return the SHAP values and ranked features by mean|SHAP|\nshap_values &lt;- shap.values(xgb_model = xgb_IBNR_S_Fit, X_train = as.matrix(df.IBNR_S_train))\n\n# Prepare the long-format data:\nshap_long &lt;- shap.prep(shap_contrib = shap_values$shap_score, X_train =  as.matrix(df.IBNR_S_train))\n\n# **SHAP summary plot**\nshap.plot.summary(shap_long, dilute = max(nrow(df.IBNR_S_train),10000)/10000)\n\n\n\n\nA second useful chart is the partial dependency plot. This shows how the values of a predictive a feature influence the predicted value, while holding all other values constant.\nHere we show the SHAP equivalent of partial dependency plots; marginal plots for the top SHAP features.\n\nfig_list &lt;- lapply(names(shap_values$mean_shap_score)[1:4], \n                   shap.plot.dependence,\n                   data_long = shap_long,\n                   dilute = nrow(shap_long)/ 10000)\n\nwrap_plots(fig_list, ncol = 2)\n\n\n\n\nThe feature importance and partial dependency plots provide quick insight into the model.\n\nWe see that phone price is the most important feature and has a linear relationship with IBNR severity as we would expect from the data generating process.\nThe second and third most important features relate to phone cover. We see that cover BOT is associated with higher IBNR costs whereas cover B is associated with lower.\n\nThe fourth feature is phone brand and follows the pattern we would expect from the data simulation process.\n\nSHAP values can also be used to show the components of a single prediction. In the following plot we show the top 4 components for each row of the data and zoom in at row 500.\n\n# choose to show top 4 features by setting `top_n = 4`, set 6 clustering groups.  \nplot_data &lt;- shap.prep.stack.data(shap_contrib = shap_values$shap_score,\n                                  data_percent = 10000/max(nrow(shap_long),10000),\n                                  top_n = 4,\n                                  n_groups = 6)\n\nThe SHAP values of the Rest 5 features were summed into variable 'rest_variables'.\n\n# choose to zoom in at location 500, set y-axis limit using `y_parent_limit`  \n# it is also possible to set y-axis limit for zoom-in part alone using `y_zoomin_limit`  \nshap.plot.force_plot(plot_data, zoom_in_location = 500, y_parent_limit = c(-1,1))\n\nData has N = 1111 | zoom in length is 111 at location 500.\n\n\n\n\n\n\n\n13.4.1.3.5 Summarising IBNR claim costs\nAgain we convert the data into a dense matrix form called a DMatrix. However before doing so we aggregate the data for efficient of model fit times. There could be a loss of accuracy in doing, so in practice this is something you’d want to experiment with.\n\ndt_All_IBNR_S_summary &lt;- dt_All_IBNR_S[rowList_IBNR_S$test,.(preds = sum(preds_full_IBNR_Cost), target = sum(target_cost)), keyby = date_pol_start_YYYYMM]\n\n\n\n\n\n\n\n\n\n\n\n\n13.4.2 Summary\nIn this section we have stepped through the process to apply machine learning techniques in order to create reserve estimates following the techniques set out in sections 3 and 4 of Baudry’s paper.\nSpecifically we have shown how to apply the xgboost machine learning algorithm and illustrated how it can, with little human input, create reasonable reserve estimates as shown below.\n\n\n\n\n\n\n\nIn addition we have seen how we can create explanations for the reserve predictions using feature importance and partial dependence plots. We have also used SHAP values which can be used to explain both global features and individual predictions."
  },
  {
    "objectID": "Literature/history_nn_papers.html#how-are-neural-networks-being-used-in-reserving",
    "href": "Literature/history_nn_papers.html#how-are-neural-networks-being-used-in-reserving",
    "title": "14  A brief history of papers looking at using neural networks in reserving",
    "section": "14.1 How are neural networks being used in reserving?",
    "text": "14.1 How are neural networks being used in reserving?\nThere are a number of steps in between, say, a process of taking your Excel template from last quarter, copying the data, and then selecting factors, to having a fully automated individual claims reserving system. For example, a process might involve organising where the data is coming from, having a robust and automated process to do the data transformations, all the way to taking the results and putting them in a report and disseminating those results. If you don’t already have those steps figured out you might better spend your time resolving these steps first before diving into machine learning.\nIn my opinion none of the existing published methodologies will be deployed into production as they currently stand. That is not to say that these are pie in the sky concepts that are not relevant in real life, it’s just that we are, as a field, pretty early into the journey of figuring out what works and what doesn’t. In fact I’m pretty confident that whatever people are building today in companies, or will be building in the next year or two, they will definitely incorporate concepts that have been introduced in these papers.\nDon’t underestimate how quickly technology moves, even in a somewhat relatively slower moving industry such as ours. It takes just a couple of companies who really care to move things forward a bit. I think we’ve hit critical mass already in terms of the talent and awareness of machine learning in industry.\nI think machine learning on claims is particularly interesting because it has applications beyond reserving. If you think about it, if you’re able to forecast future cashflows for each individual claim you can use that information as part of a claims triaging process or other claims analytics applications.\nIt is worth pointing out that there are other approaches other than neural networks to all of this, but here we are just focusing on neural networks. If you are unfamiliar with neural networks, then scroll down to the end of this article for a brief, high level introduction (or click on the the two question marks here to jump down to this section: @nn-intro)."
  },
  {
    "objectID": "Literature/history_nn_papers.html#the-papers",
    "href": "Literature/history_nn_papers.html#the-papers",
    "title": "14  A brief history of papers looking at using neural networks in reserving",
    "section": "14.2 The papers",
    "text": "14.2 The papers\n\n\n\n\n\nPaper\n\n\n\n\n2017\n\n\nWüthrich, M. V. Neural Networks Applied to Chain-Ladder Reserving\n\n\n2018\n\n\nKuo, K. DeepTriangle: A deep learning approach to loss reserving\n\n\nGabrielli, A., R. Richman, and M. V. Wüthrich. Neural Network Embedding of the Over-Dispersed Poisson Reserving Model\n\n\n2019\n\n\nGabrielli, A. A neural network boosted double overdispersed Poisson claims reserving model\n\n\n2020\n\n\nKuo, K. Individual Claims Forecasting with Bayesian Mixture Density Networks\n\n\nDelong, L., M. Lindholm, and M. V. Wüthrich. Collective Reserving using Individual Claims Data\n\n\nGabrielli, A. An individual Claims Reserving Model for Reported Claims\n\n\n\n\n\n\n\nThe first thing you will notice when looking at the list of papers is that by actuarial literature standards this is pretty recent work. We are at the intersection of insurance and machine learning, so that the speed at which this has grown is somewhere between those two fields. Another thing you will notice is that we don’t have too much diversity at this point - it is the same 4 or 5 people putting out papers every year. But as we do a better job of promoting this content and inviting people to contribute, I am hopeful we will get more perspectives in the coming years.\n\n14.2.1 2017 Wuthrich, M. V. Neural Networks Applied to Chain-Ladder Reserving\nBack in 2017 Mario Wuthrich at ETH Zurich, first proposed a Neural Network that takes some triangle characteristics to predict age to age factors. That’s all it was - a pretty straight-forward application to triangles.\n\n\n\n14.2.2 2018 - Kuo, K. DeepTriangle: A deep learning approach to loss reserving.\nIn 2018, as a response to the CAS reserving call for papers programme, I proposed an architecture that takes sequences of paid losses and case reserves. This used Schedule P data on triangles. It then feeds those sequences through a recurrent neural net to output sequences of future payments and claims outstanding amounts.\n\n\n\n14.2.3 2019 Gabrielli, A. A neural network boosted double overdispersed Poisson claims reserving model.\nAround the same time Andrea Garbielli at ETH, one of Mario Wuthrich’s students, proposed this. You start with a triangle, and then look at the ODP model predictions and then train a neural net. The neural net initialises to give the ODP predictions, but you train on the residuals, so it’s sort of a boosting process (as evidenced in the title).\n And as a follow up to that work he also incorporated claim counts in addition to the claim amounts, and then performed what we call a ‘multitask learning’ in the neural net to predict both quantities simultaneously with better accuracy.\n\n\n\n14.2.4 2020 Kuo, K. Individual Claims Forecasting with Bayesian Mixture Density Networks.\nIn 2019/20 I was looking at individual claims forecasts. This used encoder-decoder architecture, and took sequences of cashflows of individual claims, as well as the static claims characteristics, to come up with paid loss and recovery distributions. So the main contribution here was to develop a way that we can get distributions of the future cash flows, in addition to just the point estimates.\n\n\n\n14.2.5 2020 Delong, L., M. Lindholm, and M. V. Wüthrich. Collective Reserving using Individual Claims Data.\nAlso in 2020 Delong and collaborators took the claim life cycle, broken down into separate processes. For each of those processes (closer to a first principles approach) they sort of abstract away the stochastic process by parameterising them with neural nets.\n\n\n14.2.6 2020 Gabrielli, A. An individual Claims Reserving Model for Reported Claims\nMore recently Gabrielli again attacks the individual claims problem. This is on reported claims. Then from this architecture you can get the expected probability of payments and then the payment amount given there’s a payment for each time period.\n\nSo that’s a quick run through. You can see the shift in focus from aggregate triangle methods to individual claims which I think is interesting and a good direction to be moving in, because we have a lot more data with individual claims, and with the more data theoretically we should come up with better estimates and better insights."
  },
  {
    "objectID": "Literature/history_nn_papers.html#nn-intro",
    "href": "Literature/history_nn_papers.html#nn-intro",
    "title": "14  A brief history of papers looking at using neural networks in reserving",
    "section": "14.3 A high level introduction to neural networks",
    "text": "14.3 A high level introduction to neural networks\nI’ll conclude this article with a high level introduction to neural networks for those unfamiliar with them.\nAt a high level a neural network is just a function that takes in some inputs and tries to predict an output - much like a generalised linear model (GLM). In fact you can think of a neural network as a bunch of GLMs chained together.\n\nThe way it works is that we take our inputs, which we can think of as a vector, and then apply a number of matrix multiplications to it, ie neural network transformations. Between the matrix multiplications we have to add some sort of non-linearity to keep things interesting. These are known as the activations. Why do we need the non-linearity? Going back to linear algebra 101 - if we compose a series of linear transformations together we just end up with another linear transformation, which is not very interesting nor particularly flexible.\nThe picture below is another way that we can look at it. You have some inputs, and then layers, that you can think of as matrices, and then the weights are just entries in the matrix.\n\nSo we initially predict some values with our neural nets. In the beginning it’s not going to be great - if we compare them to the true targets it’s going to be way off. But then we can quantify how unhappy we are with these predictions using measures like mean square error (MSE).\nFor example, if we take the difference between the predicted and actual payment amount in some future time period, we can go back and tweak the weights a little bit.\nThe idea is that we do this over and over again until we have a model that provides reasonable predictions. This process will be analogous to say, iteratively reweighted least squares in a GLM context."
  },
  {
    "objectID": "Literature/al_mudafer.html#muhammed-taher-al-mudafers-thesis-november-2020",
    "href": "Literature/al_mudafer.html#muhammed-taher-al-mudafers-thesis-november-2020",
    "title": "15  Probabilistic Forecasting with Neural Networks Applied to Loss Reserving",
    "section": "15.1 Muhammed Taher Al-Mudafer’s thesis, November 2020",
    "text": "15.1 Muhammed Taher Al-Mudafer’s thesis, November 2020\nThis thesis explores emerging Neural Network (NN) techniques that:\n\nenable a distribution of results to be produced rather than just a central estimate\nimprove explainability and interpretability (in that you start with a distribution that you understand).\n\nIt is a very accessible read and I would recommend it to machine learning beginners as well as those following the latest developments in applying neural networks to GI reserving.\nYou may want to dip into sections rather than read the whole document from cover to cover. The very readable and comprehensive literature review takes you through the development in the literature of applying NNs to reserving. It introduces what Mixture Density Networks (MDNs) and ResNets are in a way that a (relative) layperson would understand.\nThe section on data provides good detail on SynthETIC, the simulator used to generate the data triangles (four different scenarios of claim triangles were used; from simple short tail claims, to an inflation shock, to those with high volatility). The background information on the claims simulation engine and the explanation of the derivation of the data triangles alone will likely be of use to many. We discussed SynthETIC in an earlier Chapter (?sec-synthetic).\nThe thesis also poses some interesting questions:\n\nIt notes that there is not currently a consistent framework in the literature for testing and training the data and goes on to propose a system that takes into account the time series or sequential nature of claim development triangles and discusses the Rolling Origin Validation Method.\nIt highlights the importance of also having a clear methodology for selecting hyper-parameters, as NNs are highly dependent on the parameters chosen, and sets out a proposal for a systematic hyper-parameter fine tuning algorithm.\n\nI would highly recommend this accessible thesis that is a good source of background information whilst also pushing forward thinking on using NNs in reserving. It is encouraging to see the continuing stream of papers in this area."
  },
  {
    "objectID": "Literature/al_mudafer.html#want-to-read-more",
    "href": "Literature/al_mudafer.html#want-to-read-more",
    "title": "15  Probabilistic Forecasting with Neural Networks Applied to Loss Reserving",
    "section": "Want to read more?",
    "text": "Want to read more?\n\nThe thesis: Probabilistic Forecasting with Neural Networks Applied to Loss Reserving\nThe research paper based on the thesis: Stochastic loss reserving with mixture density neural networks"
  },
  {
    "objectID": "Research/research.html",
    "href": "Research/research.html",
    "title": "Research",
    "section": "",
    "text": "The Research workstream carries out research in a number of different areas. In the past we have looked at the performance of different ML models for aggregate claims reserving. We are currently investigating individual claims reserving and the use of deep learning."
  },
  {
    "objectID": "Research/giro-2021-questions.html#introduction",
    "href": "Research/giro-2021-questions.html#introduction",
    "title": "16  Machine learning: Reserving on triangles - Q & A after GIRO 2021",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\nIn 2021 we presented the results of using machine learning on a number of triangular data sets. In this post we cover the Q&A session that followed the session."
  },
  {
    "objectID": "Research/giro-2021-questions.html#technical-questions",
    "href": "Research/giro-2021-questions.html#technical-questions",
    "title": "16  Machine learning: Reserving on triangles - Q & A after GIRO 2021",
    "section": "16.2 Technical questions",
    "text": "16.2 Technical questions\n\n16.2.0.1 Did you do any simulations on classes that had very sparse data points to see how the ML models worked against the CL?\nNo we haven’t. ML models tend to perform better on larger volume data sets. However if faced with this problem in practice, we could overcome it by, for example, fitting ML models to market data and extract elements such as claims development patterns/trends to see if they are relevant to the sparse dataset.\n\n\n16.2.0.2 Do you choose the cross validation set as a random sample or a stratified sample?\nThe cross validation is currently performed using random sampling. There are other possible approaches which the WP is planning to explore in the future such as roll-origin cross validation.\n\n\n16.2.0.3 Given tree based models are poor at extrapolating, does this not affect their suitability for reserving?\nThe performance of tree-based models will depend on the characteristic of the class of business being modelled, as with any statistical model. In general, extrapolation is always a challenge, particularly where there are time trends present in the modelled population.\n\n\n16.2.0.4 How do we identify which model to use among the Lasso basic, extra, xgboost basic and extra?\nDifferent models have different characteristics (e.g. models with extra features tend to perform better at picking up Accident/Calendar trends but require more tuning and may be more prone to overfitting). Our presentation covers 5 typical scenarios and compares the relative model performance in each scenario using a range of diagnostics. However the comparison uses only basic hyperparameter tuning (through k fold cross validation) and is based on simple synthetic datasets. More needs to be done to better understand the characteristics of each model with real life data. So lots of trial and error will be expected.\n\n\n16.2.0.5 Thanks for a great presentation. How sensitive are the results/models to the development period granularity of the triangles?\nA reasonable amount of data is generally required for ML models, so annual development triangles are unlikely to produce optimal results. Our dataset is quarterly. In practice a balance will need to be struck between the volume of data for the ML models and the granularity required to be captured.\n\n\n16.2.0.6 Using machine learning methods, would you advise that claims data be split into large and attritional losses?\nIt could be something worth experimenting with. However aggregate large claims data (e.g. in triangle form) tend to be high severity low frequency in nature. Reserving using such data may not be the most appropriate for ML methods, which are better trained on large volumes of data. The working party is undertaking separate research into individual claims reserving using ML methods, which could be something interesting for you to follow up on.\n\n\n16.2.0.7 What additional features were engineered?\nExtra features were: ramp functions (a unary real function, whose graph is shaped like a ramp) by accident origin, development and calendar periods, and interaction terms between features.\n\n\n16.2.0.8 With a calendar quarter effect like Ogden or a step change in inflation, do the models allow flexibility in the assumptions of what will happen in future? E.g. if we think inflation will return to normal, or increase further, can we adjust the model accordingly?\nWhilst ML models can pick up calendar quarter effects in historical data reasonably well, extrapolation of future trends (such as the Ogden inflation discussed here) is always a key challenge in reserving. It may therefore be more appropriate to combine ML model output with expert judgements using wider actuarial toolkits.\n\n\n16.2.0.9 Your data seemed to show constantly increasing data - in the LM we often see triangles which also shows reductions. How do the models work in this case?\nThere are a number of options to dealing with reductions. One would be to change the underlying distribution of modelled claims to allow for negative values. Modelling gross claim amounts and recoveries separately could also be an option. The WP is planning further research into this area and will publish our findings via blog posts or other events as we progress on this.\n\n\n16.2.0.10 Are there any heuristics/ rules of thumb you could recommend to initially tune the lambda hyper parameter in a reserving context?\nA lot of the heavy lifting is done by the glmnet package if you use its cross-validation functionality – it will select a series of lambda parameters and fit models to all of these. It will then return the cross validation error associated with each of these. You then need to select the particular lambda to use, but the statistics / ML community identifies two useful values and thus two useful models:\n\nThe lambda associated with the minimum CV error (“lambda.min”)\nThe largest lambda for which the CV error is within 1 standard deviation of the minimum (“lambda.1se”)\n\nThe first value gives you the model that minimises the CV error. However, if you are concerned about the possibility of over-fitting then you may prefer to use the lambda.1se model – this will be simpler than the lambda.min model.\nThese comments apply in general to any lasso problem. In a reserving context, you will also want to look at what the model is actually forecasting for your reserve and ask if it’s reasonable. Consider both the overall result as well as more detailed diagnostics. This may help you prefer one model over the other. However, it may be the case that you will want to adjust the results of a lasso model before using it. For example, if you include calendar period trends, you may want to control how these are projected into the future. This type of adjustment is separate to the tuning the lambda hyperparameter but is very much part of our core actuarial practice."
  },
  {
    "objectID": "Research/giro-2021-questions.html#application",
    "href": "Research/giro-2021-questions.html#application",
    "title": "16  Machine learning: Reserving on triangles - Q & A after GIRO 2021",
    "section": "16.3 Application",
    "text": "16.3 Application\n\n16.3.0.1 Can the models incorporate other factors which we tend to use to help our judgement such as exposure factors, rate indices, etc.?\nYes, that’s one of the advantages of ML models – additional features can be incorporated flexibly (the decision of whether to keep them is then informed by the predictions from the training process).\n\n\n16.3.0.2 Given machine learning algorithms are parameterized on historical data, do changes in risk profile, claims processes, a change in Ogden rate etc. lead to poor model output immediately after such a structural change? Is expert judgement more responsive to these changes in the short term?\nAs with any traditional actuarial model (e.g. chain ladder, BF), ML models suffer from the limitation that history does not necessarily reflect future experience. However, if engineered appropriately, they may be more capable of identifying emerging trends in the data compared to traditional methods. Where ML methods better help us identify the drivers of experience (e.g. inflation, calendar year effect, sub-components like social inflation), we may then use this intelligence to identify where best to overlay current projections with SME judgements in extrapolating into the future. In any case we do not see ML methods being used to replace expert judgements (which are always important for any reserving exercise), but as useful extensions to the traditional actuarial toolkit at this stage.\n\n\n16.3.0.3 Have you considered combining the results from multiple machine learning methods - a sort of wisdom of crowds (of models)?\nIt is definitely an interesting area to explore. So far we are focusing on individual methods as work is needed to better understand the characteristics of these methods in isolation. However once users become familiar and more comfortable with these methods, additional judgements could be made to combine results from different models as commonly done today with CL and BF methods.\n\n\n16.3.0.4 How do you fit these methods into the actuarial control cycle?\nAt this stage we do not see ML methods being used to replace any existing reserving methods but useful extensions to the traditional actuarial toolkit, e.g. to produce an alternative set of results for benchmarking purposes. Such results may be helpful to more quickly identify areas of reserving that require extra attention from the actuary during the reserving cycle. More work is required to understand the performance of ML models on real life data for different classes of business.\n\n\n16.3.0.5 John said that machine learning in reserving is not yet a push button exercise - does that mean you expect it to become one?\nUnlikely as there will be key judgements involved in a reserving exercise that require human intervention (e.g. determination of trends applicable in the future). Even in just a machine learning setting, model structure and hyperparameter selection would require judgements, preventing reserving from being a ‘push button’ exercise. However ML models may help accelerate and improve human judgements in reserving by extracting better insight from data.\n\n\n16.3.0.6 Most of the time spent in real world reserving exercises can be around identifying ‘outliers’ for points to exclude, especially if there could be data issues - could these techniques help reduce that time at all?\nWe would expect the cross validation process for ML models to help deal with outliers more effectively than traditional actuarial methods.\n\n\n16.3.0.7 Presume this model formulation would work on individual claims level data with far more features. Is this something you might want to experiment with?\nCorrect. We are focusing on aggregate triangle data in this particular presentation as it is likely to be the most accessible form of data to reserving actuaries. The working party is undertaking separate research on individual claims reserving using ML methods (e.g. NN) and will publish our findings via blog posts or other events as we progress on this.\n\n\n16.3.0.8 What is the main limitation of adopting ML to predict the first cut of reserves at the moment?\nPossible limitations of ML models may include volume of data available, time trends which may invalidate the use of ML models for extrapolation into the future, performance of model dependent on hyperparameter tuning which can involve heavy judgement.\n\n\n16.3.0.9 When comparing ML vs. CL, it’s fine to show examples of where the CL doesn’t work well. But in the real world we use other methods (e.g. BF) when the CL doesn’t work well, do you have an opinion on what would happen if you pitched ML against a fully worked up reserving exercise with all the optimisation that the actuary uses to set reserves?\nApplying ML methods to real life data and assessing their performance to fully worked up reserving results are one of the areas of further work planned for our working party. We will publish our findings via blog posts or other events as we progress on this.\nAt this stage we do not see ML methods be used to replace any existing reserving methods but useful extensions to the traditional actuarial toolkit. More work is certainly required to better understand the performance of ML models on real life data under different circumstances. However we believe there is benefit in adopting ML models for reserving actuaries in the long run such as: faster identification of emerging trends, increased flexibility to incorporate more features, higher efficiency in reserving large numbers of cohorts."
  },
  {
    "objectID": "Research/giro-2021-diagnostics.html#introduction",
    "href": "Research/giro-2021-diagnostics.html#introduction",
    "title": "17  Modelling Five Scenarios using the SynthETIC Dataset - Diagnostic from GIRO 2021",
    "section": "17.1 Introduction",
    "text": "17.1 Introduction\nIn the MLRWP’s talk at GIRO 2021, we used the SynthETIC data claims data simulator, and four previously published parameter sets to simulate 5 different claims scenarios (20 simulated datasets for each scenario) and apply the following reserving models for each scenario to compare model performance:\n\nVolume all chain ladder\nLasso using Accident and Development Quarters as factors\nLasso using the basis functions per Grainne’s post at \nXGBoost using Accident and Development Quarters as factors\nXGBoost using the same basis functions as those used in (3).\n\nThe 5 claims environments used to test different methods are:\n\nSimple, short tail claims\n30% uplift to incremental paid from calendar quarter 30 onwards\nSuperimposed inflation jumps to 20% after calendar quarter 30\nGradual increase in claims processing speed\nLonger tail, more volatile claims development.\n\nIFoA members can find full details of the modelling methodology used in this exercise can be found in the MLRWP’s 2021 GIRO presentation.\nThe aim of this blog is to showcase the result of our exercise, focusing specifically on the diagnostic plots used to compare model performance."
  },
  {
    "objectID": "Research/giro-2021-diagnostics.html#four-graphs-used-for-the-diagnosis-of-method-performance",
    "href": "Research/giro-2021-diagnostics.html#four-graphs-used-for-the-diagnosis-of-method-performance",
    "title": "17  Modelling Five Scenarios using the SynthETIC Dataset - Diagnostic from GIRO 2021",
    "section": "17.2 Four graphs used for the diagnosis of method performance",
    "text": "17.2 Four graphs used for the diagnosis of method performance\nThe following 4 diagnostic plots are used to assess trends in the data sets under different scenarios:\n\nThe first two diagnostic plots (ie Graph 1 and Graph 2) are applicable to all scenarios.\nThe 3rd diagnostic plot (ie Graph 3) mainly shows the impact of calendar period effect.\nThe last diagnostic plot (ie Graph 4) mainly shows the impact of accident period effect.\n\n\n17.2.0.1 Graph 1. Percentage of Cumulative paid to Ultimate\n\nThe graph shows cumulative payments as percentage of ultimate claims by development quarter for each accident quarter for a single simulation.\nThis graph is useful to assess:\n\nThe speed of claims development across all accident and development periods\nThe volatility of claims development.\n\n\n\n17.2.1 Graph 2. Percentage Error in Forecast Reserve\n\nThe graph shows the percentage error in forecast reserve produced by the Chain Ladder method compared to the underlying simulated reserves across all accident years and simulations, summarized in the boxplot format.\nThe percentage error in Forecast Reserve is calculated as (ie sum of the bottom triangle)\n\\[\\frac{\\sum_{AQ=2}^{40}\\sum_{DQ=41-AQ+1}^{40}X_{AQ,DQ}^{pred}}{\\sum_{AQ=2}^{40}\\sum_{DQ=41-AQ+1}^{40}X_{AQ,DQ}^{true}}\\]\nwhere:\n\nAQ: Accident Quarter\nDQ: Development Quarter\n\\(X_{AQ,DQ}^{pred}\\) :is the predicted incremental payment of each method which could be Chain Ladder, LASSO_basic, LASSO_extra, XGBoost_basic, XGBoost_extra.\n\n\n17.2.1.1 Graph 3. Average Incremental Payment by Calendar Quarter\n\nThe graph shows the average incremental payment by calendar quarters across all simulations.\nThis diagnostic plot is able to capture the calendar period effect which may not be obvious from the first 2 diagnostic plots.\n\nArea A shows that a step change in environment 2 Calendar Quarter 30 is driven by the 30% uplift payment from Calendar Quarter 30 onward.\nArea B shows that the incremental payment pattern appears to have the second peak in environment 3 Calendar Quarter 50 and which is driven by the 20% superimposed inflation from Calendar Quarter 30\n\n\n\n17.2.1.2 Graph 4. Incremental Paid as Percentage of Ultimate\n\nThe graph shows the incremental payment as percentage of ultimate claims by accident quarter for selected development quarters in a single simulation.\nThis diagnostic plot mainly captures the Accident Period effect. For the data sets with no Accident Period effect, each line should appear to be roughly horizontal across different accident quarters. Where that’s not the case, the Accident Period effect will be present. For example, in the environment 4 at development quarter 1 (row 4 column 1 in the graph), it’s clear from the increasing line that as accident quarter increases, there is an acceleration of claims development."
  },
  {
    "objectID": "Research/giro-2021-diagnostics.html#use-of-diagnostic-plots-to-evaluate-model-performance",
    "href": "Research/giro-2021-diagnostics.html#use-of-diagnostic-plots-to-evaluate-model-performance",
    "title": "17  Modelling Five Scenarios using the SynthETIC Dataset - Diagnostic from GIRO 2021",
    "section": "17.3 Use of diagnostic plots to evaluate model performance",
    "text": "17.3 Use of diagnostic plots to evaluate model performance\nEach of the sections below sets out the comparison of modelling results across all four Machine Learning methods against the underlying simulations."
  },
  {
    "objectID": "Research/giro-2021-diagnostics.html#environment-1---simple-short-tail-claims",
    "href": "Research/giro-2021-diagnostics.html#environment-1---simple-short-tail-claims",
    "title": "17  Modelling Five Scenarios using the SynthETIC Dataset - Diagnostic from GIRO 2021",
    "section": "17.4 Environment 1 - Simple, short tail claims",
    "text": "17.4 Environment 1 - Simple, short tail claims\n\n\nIn this simple scenario, claims payments appear uniform across Accident Quarters and Calendar Periods. As a result the traditional Chain Ladder method provides accurate reserve forecasts, similar to most other methods."
  },
  {
    "objectID": "Research/giro-2021-diagnostics.html#environment-2---all-payments-uplifted-by-30-from-calender-30",
    "href": "Research/giro-2021-diagnostics.html#environment-2---all-payments-uplifted-by-30-from-calender-30",
    "title": "17  Modelling Five Scenarios using the SynthETIC Dataset - Diagnostic from GIRO 2021",
    "section": "17.5 Environment 2 - All payments uplifted by 30% from calender 30",
    "text": "17.5 Environment 2 - All payments uplifted by 30% from calender 30\n\n\nIn this scenario, Calendar Period has a clear effect on claims payments, where the levels of payments increase proportionately by 30% across all Accident Periods post Calendar Quarter 30. This Calendar Period effect is well captured by the Lasso method with extra features, as demonstrated in the chart below. All other methods appear to systematically underestimate reserves across the 20 simulations."
  },
  {
    "objectID": "Research/giro-2021-diagnostics.html#environment-3---superimposed-inflation-jumps-from-0-to-20-after-calendar-quarter-30",
    "href": "Research/giro-2021-diagnostics.html#environment-3---superimposed-inflation-jumps-from-0-to-20-after-calendar-quarter-30",
    "title": "17  Modelling Five Scenarios using the SynthETIC Dataset - Diagnostic from GIRO 2021",
    "section": "17.6 Environment 3 - Superimposed inflation jumps from 0% to 20% after calendar quarter 30",
    "text": "17.6 Environment 3 - Superimposed inflation jumps from 0% to 20% after calendar quarter 30\n\n\nIn this scenario, Calendar Period has a clear effect on claims payments, where the levels of payments inflates at a rate of 20% post Calendar Quarter 30, leading to a slow down in the cumulative claims payment pattern over Accident Quarters.This Calendar Period effect is best captured by the Lasso method with extra features, as demonstrated in the chart below. All other methods appear to systematically underestimate reserves across the 20 simulations as they do not adequately reflect the underlying claims inflation trend."
  },
  {
    "objectID": "Research/giro-2021-diagnostics.html#environment-4---gradual-increase-in-claims-processing-speed",
    "href": "Research/giro-2021-diagnostics.html#environment-4---gradual-increase-in-claims-processing-speed",
    "title": "17  Modelling Five Scenarios using the SynthETIC Dataset - Diagnostic from GIRO 2021",
    "section": "17.7 Environment 4 - Gradual increase in claims processing speed",
    "text": "17.7 Environment 4 - Gradual increase in claims processing speed\n\n\nIn this scenario, Accident Period has a clear effect on claims payment patterns, where the proportions of incremental payment made in earlier Development Quarters increase over more recent Accident Quarters and those made in later Development Quarters reduce over Accident Quarters. This Accident/Development Quarter interaction is well captured by both the XG Boost and Lasso methods with extra features, as demonstrated in the chart below."
  },
  {
    "objectID": "Research/giro-2021-diagnostics.html#environment-5---long-tail-claims",
    "href": "Research/giro-2021-diagnostics.html#environment-5---long-tail-claims",
    "title": "17  Modelling Five Scenarios using the SynthETIC Dataset - Diagnostic from GIRO 2021",
    "section": "17.8 Environment 5 - Long tail claims",
    "text": "17.8 Environment 5 - Long tail claims\n\n\nIn this scenario, the long tailed and volatile nature of the claims payments led to significant distortions and errors in the reserve forecasts provided by the traditional Chain Ladder method (as expected).\nInterestingly, whilst ML methods with extra features appear to capture the volatility in the claims payment patterns, they produces less accurate reserve forecasts compared to those without extra features. The XG Boost and Lasso methods without extra features on the other hand appear to provide reasonable accurate reserve forecasts over most seeds whilst capturing less volatility in the claims payment patterns."
  },
  {
    "objectID": "Research/chainladder_to_individual_mdn.html",
    "href": "Research/chainladder_to_individual_mdn.html",
    "title": "18  From Chain Ladder to Individual Mixture Density Networks on SPLICE Data",
    "section": "",
    "text": "Refer to this blog post by Jacky Poon where he details a step-by-step process of moving from a Chain Ladder model to Individual claims models using neural networks.\nThe post was published on 18 April 2023."
  },
  {
    "objectID": "Practical/practical.html",
    "href": "Practical/practical.html",
    "title": "Practical Considerations",
    "section": "",
    "text": "Members of the Practical Considerations workstream look at practical issues facing those wishing to use ML techniques in their day-to-day reserving work."
  },
  {
    "objectID": "Practical/time_resource.html",
    "href": "Practical/time_resource.html",
    "title": "19  Practical Considerations Part 1: Time & Resource Limitations",
    "section": "",
    "text": "20 Practical Considerations Part 1: Time & Resource Limitations\nThis article was written by Isabelle Williams and originally published on 23 April 2022."
  },
  {
    "objectID": "Practical/time_resource.html#why-is-machine-learning-not-commonly-being-adopted-in-the-uk-reserving-market",
    "href": "Practical/time_resource.html#why-is-machine-learning-not-commonly-being-adopted-in-the-uk-reserving-market",
    "title": "19  Practical Considerations Part 1: Time & Resource Limitations",
    "section": "20.1 Why is Machine Learning not commonly being adopted in the UK Reserving Market?",
    "text": "20.1 Why is Machine Learning not commonly being adopted in the UK Reserving Market?\nThere are many reasons why Machine Learning (“ML”) techniques are currently not being adopted in the UK reserving market. Previously, the issues faced were more theoretical than practical. However, in recent times, interest in this area has grown, and many companies are now considering investing in ML techniques for their reserving teams.\nAs stated in the MLR Working Party UK Survey, the three main reasons why companies are not currently investing into these methods are:\n\nResource and Time Limitations\nAccessibility of Knowledge\nExplainability (or rather, the perceived lack thereof) of models & results\n\nThe first of these will be discussed in this blog post, with the second and third points to follow as separate discussions."
  },
  {
    "objectID": "Practical/time_resource.html#time-limitations",
    "href": "Practical/time_resource.html#time-limitations",
    "title": "19  Practical Considerations Part 1: Time & Resource Limitations",
    "section": "20.2 Time Limitations",
    "text": "20.2 Time Limitations\nWhy are limits on reserving actuaries’ time such a barrier to developing ML techniques?\nThis author believes that the answer lies in the quality of data and processes currently available to, and followed by, reserving teams. This can either be due to:\n\nInternal issues, for example:\n\nExtensive processes built up over time without review\nInsufficient checks on data at critical stages of the process\n\nExternal issues, for example:\n\nPoor quality data from third party sources\nPoor data capture at point of claim\n\n\nWhether these issues are internal or external, poor quality data and processes will affect the time it takes to run a process, and therefore the reserving actuaries’ capacity to invest time into learning ML techniques. The diagram below describes the knock-on effects that poor quality data & processes have on reserving actuaries’ time.\n\n\n\nThe Time Catch-22\n\n\nThe Catch-22, or “vicious cycle” above begins with the highlighted issue, which is poor data and process quality. This fundamental problem then goes on to affect the entire reserving process, and to fuel the propagation of this cycle.\nThe actions that actuaries can take regarding data issues varies widely between companies.\n\nSome actuarial teams are unable to fix poor data, as the processing of this data lies outside the remit of their department…\n\n…or the data is provided via a third party…\n…often in a format that may be difficult to work with, particularly for ML purposes.\n\nConversely, many teams can play too great a role in fixing data issues themselves…\n\n…rather than tackling them at source via a third party…\n…or notifying other departments, whose role it is to fix the underlying data.\n\n\nProcess errors, however, frequently lie within the remit of the actuarial team, and as such can be addressed by the team themselves. For this reason, these will be the main focus of discussion in this longer-form article.\n\n20.2.1 Process Issues\nProcess issues are common in any industry that deals with large amounts of data, and the insurance industry is no exception.\nWith many changes in the adoption of that technology and software across the market in recent years, it is not uncommon to find data processes that have one or more of the following issues:\n\nThe issue regarding manual steps is often one of the most pressing, as one manual step or “quick fix” can spiral into the creation of further “quick fixes” down the line, making processes more difficult and time-consuming to run correctly.\nThese issues with data processes can be true for many reasons, including:\n\nLack of process documentation, often caused or exacerbated by:\n\nLack of attention from senior management…\n…leading to a lack of staff training to develop sustainable, robust processes…\n…leading to a lack of attention from staff\n\nStaff or resourcing problems, such as:\n\nFrequent changes of staff within the department\n\nChanges in the level of technical expertise in the actuarial team\nChanges in the software preferences of those staff\n\nKey person risk - reliance on a single staff member to run a process…\n\n…and frustrations with inefficient processes leading to resignations\n\nFrequent moving of the process to different departments in the business, where expertise in different coding languages and/or software is an issue\n\n\nOnce again, it should be noted that whatever the reason for process issues or inefficiencies, the outcome is the same when it comes to the pressure on staff time, and the resultant inability of the staff to invest in ML techniques.\n\n20.2.1.1 Complexity of Processes\nThe diagram below illustrates where such inefficiencies might occur within a typical reserving department data process:\n\n\n\nA possible typical reserving process\n\n\nIn some organisations, who have begun to adopt ML in their reserving processes, the process could potentially be even more complicated, and may involve the running of an R or Python script at any stage between the generation of the raw data and the production of the report.\nA valid question at this point may be this – why would having a process designed in this way take any longer to run than a process that relies on, for example, a single script?\nThe answer to this question is complex. In theory, if such a process is:\n\nWell-documented\nLargely free of the aforementioned manual steps and “quick fixes”\nFixed in a sustainable way when errors occur\n\nThere should not be any issues. However, in practice, it has been the author’s experience that this is rarely the case, and in fact, the following sequence of steps is generally true:\n\n\n\nThe impact of a complex multi-stage process\n\n\nIn general, the longer the process and the more stages the process goes through, the longer it will take to run in practice.\nIt is important to note that:\n\nThe issues with the process do not typically appear at the stage in which actuarial judgement must be applied…\n…but at a processing stage prior to exercising this judgement.\nDespite this occurring at a processing stage…\n…the knock-on effects from longer required processing not only impacts the time of analysts, but also the time of senior reviewers.\n\n\n\n20.2.1.2 Software Considerations\nIn assessing inefficiencies within a reserving process, a major factor that we must consider is the type of software used, particularly regarding the following practical considerations:\n\nMany reserving processes involve at least one stage where the data must be processed in Excel. It is worth noting that this thinking can be applied to any other software, but as Excel is so commonly used for reserving in the UK market at this time, it makes sense to assess this software with respect to the considerations outlined above.\nAs Excel is the arguably still the most popular piece of software to use for constructing data processes and reserving models within actuarial departments, let us assess the features of Excel against some of the practical considerations above:\n\nConsidering the qualities of Excel relating to the five criteria above, it is easy to see where inefficiencies and errors in the process can creep in – if the spreadsheet is not well-designed and maintained, the process can quickly fall over, particularly as new requirements develop over time.\nExcel is not the only software in which this can occur, and software such as R and Python that is frequently used for ML is not, for example, immune to difficulties when linking in other pieces of software, or an inability to sense-check results.\nHowever, it is worth noting that the choice of software for one particular stage can impact the efficiency of the entire process, and therefore the time it takes to run.\nIn addition, the choice of software can impact the ability of the process to easily link in to ML software. This is something that needs to be considered when adding on additional ML modelling stages to a reserving process, or replacing modelling stages in an existing process with ML techniques.\n\n\n20.2.1.3 Short-Term Thinking\nA final point on process design is the point of prioritising “quick fixes” and “quick wins” over spending the time on building a process that has a longer build time, but is more robust in the long term.\nAt the end of running many regular processes, it is common for time to be ring-fenced for “process development”. However, this time is often:\n\nDe-prioritised at short notice…\n…or not spent in developing the process in a strategic or sustainable way…\n…or in a way that employs the best practices of process and software developments recognised by the industry.\n\nThis thinking leads to the following two chains of events, which both have the same outcome:\n\n\n\nThe impact of short-term thinking\n\n\nWith the time constraints frequently imposed on reserving teams, often from senior management, it is understandable that teams will prefer to implement quick short-term fixes in software that they are more familiar with than longer-term transformation projects. However, short-term thinking often causes further issues down the line.\nThis short-term thinking particularly impacts the adoption of new software, which as has already been mentioned, is often time-consuming to learn and test. In addition, a lack of recognition of the long-term benefits of adopting new software contributes to this hesitancy, leading teams to cling to the familiar and “stick with what they know”.\n\n\n\n20.2.2 What Can Be Done?\n\n20.2.2.1 Step 1: Recognise Required Resources\nRecognising the resources required to fix the problem is the first step in the process. Once the extent of the issues in the current process are recognised, management can make decisions as to where the resources will come from. For example:\n\nResources can be diverted away from regular processes\nContractors and/or other expert resource can be bought in from other companies\nExpert resources (e.g. IT staff, data analysts, data scientists) from elsewhere in the business can be brought in to bolster actuarial resource\nFurther analysts can be recruited, if required.\n\n\n\n20.2.2.2 Step 2: Clean Up Processes\nOnce resources are made available, the next action that can be taken is to clean up issues in the process. A good first step in this regard is to set up automated reconciliation and sense-checking processes on external data, or alternative internal sources. For example:\n\nGo through with staff what checks are currently implemented for common data issues\nConsider how current checks that are implemented in a more manual process can be automated\nConsider software that could potentially be used to implement these automated checks, and how it can be made to tie in to the current process.\n\n\n\n20.2.2.3 Step 3: Automate, automate, automate!\nThe next action that can be taken is to implement automated reserving tools, such as the Chain Ladder Package (available in R and Python) to automate reserving processes.\nManagers should bear in mind the following:\n\nThese are not always appropriate.\nWhen they are appropriate, they are sometimes best used as a “guide” or “benchmark” against which a more manual process can be assessed.\n\nThe appropriateness of automated reserving tools, and some considerations surrounding these, are better placed as the subject of a future blog post.\nAn often overlooked piece is that visualisations and reports can also be automated in a similar way to the checks and models above.\n\nThe generation of an automated report can save a department valuable time…\n…and although there is certainly value to generating custom visuals and ad-hoc sections to a report…\n…an automatically generated framework can be exceptionally helpful, particularly in reporting periods where there may be very little movement to report.\n\n\n\n20.2.2.4 Step 4: Make it Sustainable\nIt is important that once an automated process is created, the process is made as sustainable as it possibly can be. Attention should always be paid to how bug fixes are implemented, and how the process is maintained over time as the needs of the business change.\nIntegration into the rest of the processes used within the department is also paramount. Managers can research into how software that could produce ML models can be linked into other software currently used within the department. For example:\n\nConnecting current data storage solutions to ML software\nReading in data from Excel sheets\nLinking R/Python to proprietary software"
  },
  {
    "objectID": "Practical/time_resource.html#resource-limitations",
    "href": "Practical/time_resource.html#resource-limitations",
    "title": "19  Practical Considerations Part 1: Time & Resource Limitations",
    "section": "20.3 Resource Limitations",
    "text": "20.3 Resource Limitations\nWhy are limits on actuarial resources a barrier to implementing ML techniques?\nThe author believes that the issue is three-fold:\n\nFirstly, there is insufficient time for reserving actuaries to develop ML skills\nSecondly, there is insufficient emphasis on the development of ML skills in the first place, potentially due to a lack of enthusiasm on either the part of staff or managers\nLastly, there are few resources available for reserving actuaries to learn ML skills\n\nThe diagram below illustrates these issues, and the impact thereof on the issue of resourcing:\n\n\n\nThe Resources Catch-22\n\n\nThe last of these points is already being addressed by the working party in the provision of resources, but the causes behind the first two have not yet been addressed.\nThe first of these points can be addressed by the first half of the cycle, up until the point where few applicable resources are available. The time available for reserving actuaries to learn ML techniques is constrained by many things, not exclusive to those included in the cycle. For example:\n\nLack of opportunities in the actuarial curriculum to pursue ML research in a hands-on setting\nLack of reserving resources to complete regular work, which leads to the Time Constraints Catch-22 previously discussed\n\nThe lack of opportunities in the actuarial curriculum to pursue ML research will also lead to the second point, as these skills may not be seen as a priority for many actuaries while the IFoA does not emphasise the development of these skills via the exam curriculum. It is worth noting that the Data Science learning path provided gives a good theoretical understanding of many ML techniques, but does not yet provide a practical pathway for learning ML-relevant techniques.\n\n20.3.1 Data Scientists in Reserving\nAn apparently simple solution to the lack of ML expertise and resources, as well as the lack of time to develop these resources, in reserving teams is to hire, or second, data science resource into the team. However, this is not currently common practice for a number of reasons, which are listed below:\n\nAll of these are issues that can be resolved, but many companies would prefer to train their own resources. This is also a valid option for many companies, particularly those without specific data science resource available. However, referring back to the Resources Catch-22, the issue among management when it comes to developing this resource is often one of expertise.\n\n\n20.3.2 What Can Be Done?\nAs previously stated, one of the goals of this working party is to improve the availability of machine learning resources, and to create applicable machine learning resources for reserving actuaries to make use of. Therefore, some of the issues present within the Catch-22 will be addressed, at least with regards to the availability of applicable ML resources.\nOnce these resources are available, there is the issue of what can be done to ensure that methods pioneered by the working party are adopted by reserving actuaries.\n\nTime will need to be made available for actuaries to do this, whether this is through:\n\nThe introduction of modelling projects on company time…\n…or within the actuaries’ own time\n\nFor example:\n\nThrough regular research meetings or presentations…\n…or setting aside time for research and team development in times where resources are less pressured.\n\n\nThere is also the possibility of recruiting resource into the team via external means.\nIt is worthwhile stating at this point that not all companies have data science resource available, or the wherewithal to recruit specific data science resource. However, if this resource is available, requests can often be made for this resource to be shared for certain purposes – for example:\n\nResource can often be negotiated for a specific project, or specific purpose\nIf not, there is potential for resource to be made available on secondment, or for external consulting resource to be brought in.\n\nRecruiting data science resource specifically into reserving teams can be difficult. There are many barriers to recruiting this resource directly into reserving teams. While there are certainly firms in the market that are doing this, it can often be difficult to:\n\nNavigate management barriers to recruiting data scientists into actuarial teams\nTailor job descriptions so that skills are sufficiently relevant to data scientists and will not disincentivise them from applying for these roles\nAid management to make the best use of these resources.\n\nThe wider topic of recruiting data scientists into reserving teams is likely better to be the subject of a longer article, and so will not be fully discussed here. However, it is important to note that this is often an issue, and one that should be solved if teams wish to employ hybrid resource in the future."
  },
  {
    "objectID": "Survey/survey.html",
    "href": "Survey/survey.html",
    "title": "Surveys",
    "section": "",
    "text": "In 2020, we carried out surveys on the use (or not) of ML among UK and Canadian actuaries.\nWe share the results of this in this section."
  },
  {
    "objectID": "Survey/surveys2020.html#uk",
    "href": "Survey/surveys2020.html#uk",
    "title": "20  2020 Surveys of the use of ML in reserving",
    "section": "20.1 UK",
    "text": "20.1 UK\n\n\n\n\n\n\nEnthusiasm from reserving actuaries but stakeholder engagement low\n\n\n\n\n\n\nIn 2020 undertook a survey to find out to what extent machine learning is currently being used in reserving in the UK.\nWe found that there was near universal enthusiasm for developing techniques amongst reserving actuaries. This contrasts starkly with the GIROC 2014 reserving survey which found that “triangles and chain ladder and Bornhuetter-Ferguson type techniques are still the methods of choice and there is very little appetite for new methodologies to be found.” There certainly appears to have been quite a sea change in attitudes towards new reserving techniques since then.\nDespite this enthusiasm only a very small number of companies have actually applied machine learning to reserving so far. It seems a gap is opening up in the motor insurance industry - will these companies gain an advantage over their competitors?\nOne of the key differentials seems to be stakeholder engagement: with a key barrier for reserving teams being time and resource limitations, investment and support from management is vital. Developing the necessary knowledge is not something that can be learned in an afternoon. To quote one respondent “it is complicated and is a lot of work”.\nIt is interesting to note that many of the companies already use machine learning for pricing, so will have a lot of the skills within their organisation, but they are not necessarily turning their attention to applying these to reserving.\nFor the fuller picture, and to see why some companies are choosing to invest in this, please see the UK write-up here.\nPlease note, the UK survey comprised personal lines companies only."
  },
  {
    "objectID": "Survey/surveys2020.html#canada",
    "href": "Survey/surveys2020.html#canada",
    "title": "20  2020 Surveys of the use of ML in reserving",
    "section": "20.2 Canada",
    "text": "20.2 Canada\nOn behalf of the working party, Jacqueline Friedland conducted a survey among Canadian actuaries on the use of machine learning (ML) in reserving. The survey respondents consisted of:\n\nNine insurers including\n\nCanadian and global companies\nOne reinsurer\n\nSix consulting firms including\n\nCanadian and global firms\nThree of the big 4 accounting firms\n\nThree telephone interviews and twelve email responses\n\nOverall there was enthusiasm for the potential of ML to assist with setting reserves, with four insurers currently using ML in the reserving process. For three of these insurers, ML was used for additional insights, but, significantly, the final insurer used ML to book reserves.\nThe full findings of the survey can be found here."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "21  Summary",
    "section": "",
    "text": "the content in this book was created using the following R environment\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_Ireland.utf8  LC_CTYPE=English_Ireland.utf8   \n[3] LC_MONETARY=English_Ireland.utf8 LC_NUMERIC=C                    \n[5] LC_TIME=English_Ireland.utf8    \n\ntime zone: Europe/Dublin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.1    fastmap_1.1.1     cli_3.6.1        \n [5] htmltools_0.5.6.1 tools_4.3.1       rstudioapi_0.15.0 rmarkdown_2.25   \n [9] knitr_1.44        jsonlite_1.8.7    xfun_0.40         digest_0.6.33    \n[13] rlang_1.1.1       renv_1.0.3        evaluate_0.22"
  }
]